[
{
	"uri": "https://aiot-ist.github.io/mcm-204/",
	"title": "MCM-204",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory MCM-204 Discover how to use the MCM-204 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/u300/",
	"title": "PCIe-U300 Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s PCIe-U300 Series is a PCI Express x4 Gen2 USB3 Vision frame grabber supporting 4/8/12 USB 3.1 Gen 1 ports for multiple USB3 Vision device connections with data transfer up to 5 Gb/s per port.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/",
	"title": "USB3 Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " Frequently asked questions (FAQ) about NEON-Series - Model and Sensor Specifications - Hardware Specifications - Software Environment - Software Version check - Trigger input and Strobe output\nGeneral  NEON-Series Model and Sensor Specifications    Model Name Image Sensor Specification Image Sensor Module     NEON-201B-JT2 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JT2 1\u0026frasl;1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JT2 1\u0026frasl;3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JT2 1\u0026frasl;2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201B-JNX 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JNX 1\u0026frasl;1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JNX 1\u0026frasl;3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JNX 1\u0026frasl;2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201A-JNX 1\u0026frasl;2.6\u0026rdquo;, 2 M, Global Shutter , 1920x1080, 60fps, COLOR Appropho, On Semi AR0234   NEON-202A-JNX 1\u0026frasl;1.8\u0026rdquo;, 8 M, Rolling Shutter, 3840x2160, 30fps, COLOR Appropho, Sony IMX334    Hardware Specification    Neon-iSeries Digital Input Digital Output UART Dimensions Weight     Neon-Series 4x DI, includes 1x sensor trigger 4x DO, includes 1x strobe out TXD, RXD, GND 123.3 x 66.81 x 77.5 mm 700g    Software environment    Neon-Series JetPack L4T Ubuntu CUDA cuDNN TensorRT Python2 Python3 Pylon     Version 4.2.1 32.2 18.04.2LTS 10.0 7.5.0.56 5.1.6.1 2.7.15 3.6.9 5.0.9   Version 4.3 32.3.1 18.04.2LTS 10.0.326 7.6.3.28 6.0.1.10 2.7.17 3.6.9 5.2.0   Version 4.4 32.4.3 18.04.2LTS 10.2 8.0.0.180 7.1.3.0 2.7.17 3.6.9 5.2.0   Version 4.6.1 32.7.1 18.04.2LTS 10.2 8.2.1 8.2.1 2.7.17 3.6.9 5.2.0   Version 5.0.2 35.4.1 20.04 11.4 8.4.1 8.4.1 2.7.18 3.8.10 5.2.0   Version 5.1.2 35.4.1 20.04 11.4 8.6.0.166 8.5.2 2.7.18 3.8.10 5.2.0    Version check  Jetpack version sudo apt-cache show nvidia-jetpack  L4Tversion head -n1 /etc/nv_tegra_release  TensorRT dpkg -l|grep nvinfer  CUDA nvcc --version  cuDNN dpkg -l|grep cudnn  Python python --version   Trigger in and Strobe out     NEON-201B-JNX NEON-202B-JNX NEON-203B-JNX NEON-204B-JNX NEON-201A-JNX NEON-202A-JNX     Support V V V V V -     How to set the NEON camera to trigger mode.  NEON-20xA  Enable the trigger mode. v4l2-ctl --set-ctrl=trigger_mode=1  v4l2-ctl --set-ctrl=low_latency_mode=1  Reference sample code  NEON-20xB  ##### Open Pylon viewer. ##### \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Mode\u0026rdquo;-\u0026gt; \u0026ldquo;On\u0026rdquo; ##### \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Source\u0026rdquo;-\u0026gt; \u0026ldquo;Line1\u0026rdquo; ##### Reference sample code  How to wire.   "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/wd-fov/",
	"title": "Neon &amp; Lens selector",
	"tags": [],
	"description": "",
	"content": " Neon Model and Sensor Specifications    Model Name NEON-201B NEON-202B NEON-203B NEON-204B NEON-201A NEON-202A     Image Sensor spec. USB3 USB3 USB3 USB3 MIPI CSI MIPI CSI   Resolution (HxV) 1280 x 960 1600 x 1200 1920 x 1080 2592 x 1944 1920 x 1200 3840 x 2160   Resolution 1.2M 1.9M 2M 5M 2M 8M   Frame Rate (fps) 54 60 30 14 60 30   Color/Mono Color Color Color Color Color Color   Shutter Global Global Rolling Rolling Global Rolling   Sensor Size 1\u0026frasl;3\u0026rdquo; 1/(1.8\u0026rdquo;) 1/(3.7\u0026rdquo;) 1/(2.5\u0026rdquo;) 1/(2.6\u0026rdquo;) 1/(1.8\u0026rdquo;)   Pixel Size (µm) 3.75 x 3.75 4.5 x 4.5 2.2 x 2.2 2.2 x 2.2 3 x 3 2.0 x 2.0   Sensor Vendor ON Semiconductor e2v ON Semiconductor ON Semiconductor ON Semiconductor SONY   Sensor Model AR0134 EV76C570 MT9P031 MT9P031 AR0234 IMX334   Image sensor SDK Basler pylon Basler pylon Basler pylon Basler pylon V4L2 \u0026amp; Gstreamer V4L2 \u0026amp; Gstreamer    Getting the FOV and Resolution Please select the Model Name: None NEON-201B NEON-202B NEON-203B NEON-204B NEON-201A NEON-202A    Please select the Lens:\nNone 3.5mm 8mm 12mm 16mm   \nPlease enter WD (mm):  VFOV (mm):  \nHFOV (mm):  \nHFOV(mm/pixel):  \nVFOV(mm/pixel):   \nImage(H):   Image(V):   H_Angle:   V_Angle:   Pixel Size (µm):   Get Your NEON AI Smart Camera Now!\n .fancy-link { color: red; text-decoration: none; position: relative; } .fancy-link:after { content: ''; position: absolute; width: 0; height: 2px; display: block; margin-top: 5px; right: 0; background: red; transition: width .2s ease; -webkit-transition: width .2s ease; } .fancy-link:hover:after { width: 100%; left: 0; background: red; }   // 获取下拉菜单和输入框的引用 const selectModel = document.getElementById(\"selectModel\"); const selectLens = document.getElementById(\"selectLens\"); const horizontalResolutionSpan = document.getElementById(\"horizontalResolution\"); const verticalResolutionSpan = document.getElementById(\"verticalResolution\"); const lensHSpan = document.getElementById(\"lensH\"); const lensVSpan = document.getElementById(\"lensV\"); const pixelSizeSpan = document.getElementById(\"pixelSize\"); const wdInput = document.getElementById(\"wdInput\"); const hfovInput = document.getElementById(\"hfovInput\"); const vfovInput = document.getElementById(\"vfovInput\"); // 创建对照表 // 创建对照表 const fovTable = { \"NEON-201B\": { \"3.5mm\": { H: 68.8, V: 54.3 }, \"8mm\": { H: 33.3, V: 25.3 }, \"12mm\": { H: 22.5, V: 17 }, \"16mm\": { H: 17.1, V: 12.8 } }, \"NEON-202B\": { \"3.5mm\": { H: 91.5, V: 75.2 }, \"8mm\": { H: 48.4, V: 37.2 }, \"12mm\": { H: 33.3, V: 25.3 }, \"16mm\": { H: 25.4, V: 19.2 } }, \"NEON-203B\": { \"3.5mm\": { H: 62.1, V: 37.5 }, \"8mm\": { H: 29.5, V: 16.8 }, \"12mm\": { H: 19.8, V: 11.2 }, \"16mm\": { H: 15, V: 8.5 } }, \"NEON-204B\": { \"3.5mm\": { H: 78.2, V: 62.8 }, \"8mm\": { H: 39.1, V: 29.9 }, \"12mm\": { H: 26.6, V: 20.1 }, \"16mm\": { H: 20.2, V: 15.2 } }, \"NEON-201A\": { \"3.5mm\": { H: 76.27, V: 60.37 }, \"8mm\": { H: 37.927, V: 29.003 }, \"12mm\": { H: 25.802, V: 19.497 }, \"16mm\": { H: 19.594, V: 14.744 } }, \"NEON-202A\": { \"3.5mm\": { H: 93.5, V: 76.27 }, \"8mm\": { H: 48.4, V: 37.2 }, \"12mm\": { H: 33.3, V: 25.3 }, \"16mm\": { H: 25.4, V: 19.2 } }, }; // 监听下拉菜单的更改事件 selectModel.addEventListener(\"change\", calculate); selectLens.addEventListener(\"change\", calculate); wdInput.addEventListener(\"change\", calculate); function updateSlider() { var wdInput = document.getElementById('wdInput'); var wdSlider = document.getElementById('wdSlider'); wdSlider.value = wdInput.value; } function updateInput() { var wdInput = document.getElementById('wdInput'); var wdSlider = document.getElementById('wdSlider'); wdInput.value = wdSlider.value; // 在这里调用 calculate() 函数，或在需要的任何其他地方 calculate(); } // 计算函数 function calculate() { const selectedModel = selectModel.value; const selectedLens = selectLens.value; const wd = parseFloat(wdInput.value); const horizontalResolution = parseFloat(horizontalResolutionSpan.textContent); const verticalResolution = parseFloat(verticalResolutionSpan.textContent); const pixelSize = parseFloat(pixelSizeSpan.textContent); const resolutions = { \"NEON-201B\": { horizontal: 1280, vertical: 960 , pixel: 3.75 }, \"NEON-202B\": { horizontal: 1600, vertical: 1200,pixel: 4.5 }, \"NEON-203B\": { horizontal: 1920, vertical: 1080,pixel: 2.2 }, \"NEON-204B\": { horizontal: 2592, vertical: 1944,pixel: 2.2 }, \"NEON-201A\": { horizontal: 1920, vertical: 1200,pixel: 3.3 }, \"NEON-202A\": { horizontal: 3840, vertical: 2160,pixel: 2.0 } }; // 根据选择的模型填充输入框(Resolution pixelSize ) if (resolutions[selectedModel]) { horizontalResolutionSpan.textContent = resolutions[selectedModel].horizontal; verticalResolutionSpan.textContent = resolutions[selectedModel].vertical; pixelSizeSpan.textContent = resolutions[selectedModel].pixel; } else { horizontalResolutionSpan.textContent = \"\"; verticalResolutionSpan.textContent = \"\"; pixelSizeSpan.textContent = \"\"; } // 根据选择的模型填充输入框(lensHSpan lensVSpan ) if (fovTable[selectedModel] \u0026\u0026 fovTable[selectedModel][selectedLens]) { lensHSpan.textContent = fovTable[selectedModel][selectedLens].H; lensVSpan.textContent = fovTable[selectedModel][selectedLens].V; } else { lensHSpan.textContent = \"\"; lensVSpan.textContent = \"\"; } // 根据输入框的值进行计算 if (wd \u0026\u0026 !isNaN(wd) ) { // 根据WD计算HFOV和VFOV const lensHSpanValue = parseFloat(lensHSpan.textContent); const lensVSpanValue = parseFloat(lensVSpan.textContent); const hfovFromWD = wd * Math.tan(lensHSpanValue / 2 * Math.PI / 180) * 2; const vfovFromWD = wd * Math.tan(lensVSpanValue / 2 * Math.PI / 180) * 2; const vfov_resolution = vfovFromWD / resolutions[selectedModel].vertical; const hfov_resolution = hfovFromWD / resolutions[selectedModel].horizontal; // 更新输入框的值 document.getElementById('hfovInput').textContent = hfovFromWD.toFixed(2); document.getElementById('vfovInput').textContent = vfovFromWD.toFixed(2); document.getElementById('vfov_r').textContent = vfov_resolution.toFixed(2); document.getElementById('hfov_r').textContent = hfov_resolution.toFixed(2); } else { // 清空输入框的值 hfovInput.value = \"\"; vfovInput.value = \"\"; } }   table { border-collapse: collapse; } table td { border: 1px solid #ccc; padding: 8px; } table tr:hover { background-color: #f2f2f2; } .condition { display: inline-block; margin-right: 10px; } .disabled-input { display: inline-block; padding: 5px; border: 1px solid #ccc; background-color: #f2f2f2; color: #999; width: 100px; }  "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtochooselenses/",
	"title": "How to choose suitable lens for Neon?",
	"tags": [],
	"description": "",
	"content": " Here\u0026rsquo;s the lens selector tool from Basler website. Please enter your criteria and we\u0026rsquo;ll show you suitable lens models.\nStep 1: Select camera series and model Takes Neon-203B for example\nStep 2: Please enter as many values as possible. Missing values will be calculated automatically. Take example for real case: 1. At 75ft(22860 mm) needs to support a field of view that is 406 inches (10312 mm) X 406 inches (10312 mm) 2. And supports a 10ft depth of field\nStep 3: Display suitable lenses If you want to use the lens with a CS-mount camera, a distance ring (5 mm) must be attached to the lens.\nStep 4: You can contact with your vendor and find the similar spec of lenses. Kindly remind you that the smaller focal length may cause “fisheye” effect.\nThe document from Basler website explains what you should know when selecting a lens for your camera.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtouseneontocontroldio/",
	"title": "How to use NEON to control DI/O?",
	"tags": [],
	"description": "",
	"content": " Here is the list for our demo devices. Different devices might have different default settings. Please adjust the procedures based on your corresponding devices.\n Easy Intermediate  Easy Devices List:  NEON-2000-JNX Series Starter Kit ADLINK DIN-37D-01 PATLITE MES-_02A 24V Power Supply  Architecture diagram Overview Wiring  Step 1. Connect the Neon with the din board through a connector. Step 2. Connect the Red LED\u0026rsquo;s wire in pin 4 for digital output (we take DO1 as an example). Step 3. Plug the 24V power supply\u0026rsquo;s negative wire in Din board\u0026rsquo;s pin 2 for grounding. Step 4. Connect the positive of power supply with the positive of LED. It shorts the two wires at pin20 which is reserved as terminal. Step 5. Use Neon to control the LED. Use the command cd /usr/src/Neon/Sample/Neon_Setting to adjust Neon setting. Use sudo ./NeonSet DO 1 1 to turn on the LED, use sudo ./NeonSet DO 1 0 to turn off the LED conversely.  Note Due to dual function of DO0 and DI0, the default function of DO0 is strobe out and DI0 is trigger in. If you want to set them as general DO and DI. Please configure it as following:\n DIO0Config 1: Set DIO0 as GPDI and GPDO0 DIO0Config 0: Set DIO0 as Trigger in and Strobe out (default)  Set DIO0 as GPDI and GPDO0 sudo /usr/src/Neon/Sample/Neon_Setting/NeonSet DIO0Config 1 Check the information of DIO. sudo /usr/src/Neon/Sample/Neon_Information/NeonInformation Intermediate Devices List:  ADLINK NEON-1000-MDX ADLINK DIN-37D-01 High Bright Tech PC-24V24W-2-S 5V Power Supply  Step 1. Connect the light with the light controller at channel 1. Step 2. Connect the light controller with din board at strobe channel 1. Plug the positive wire in pin 3 for device output, and plug the negative wire in the pin 10 for grounding.\nStep 3. Connect the Neon with the din board through a connector. Step 4. Connect a trigger device on the din board. Insert the positive wire at pin 11 for trigger in, insert the negative wire at pin 10 for grounding. Here we used a 5V power supply for demonstration, you could connect your own device for your own purposes.\nStep 5. Adjust the default settings from Neon. Please enter the specific file path to adjust the settings. We modified the strobe-out polarity to set turning off the light as default, this step might be different due to your controller devices. Once the Neon reboots, the setting would be restored.\n Use command sudo -i to get in root mode.   Use the command cd /sys/class/neon_camctrl to change directory.   Use cat command to check current status. To change the default setting of strobe-out polarity, use the command echo 1 \u0026gt;StrobeOutPolarity.  Step 6. To extend or narrow the device strobe out time, please follow the steps\n Use the command cd /usr/src/Neon/Sample/Neon_Setting to change the current working directory.   To adjust the strobe out time, use the command sudo ./NeonSet StrobeOutPulseWidth N. The parameter N is the time of the width, the unit is us(10^-6 second).   For checking current infos of Neon, please change the directory with the command cd /usr/src/Neon/Sample/Neon_Information. Use sudo ./NeonInformation to get current status.  "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtosetupneon/",
	"title": "How to set up Neon camera?",
	"tags": [],
	"description": "",
	"content": " This section describes the mounting accessories provided with the Neon camera and includes actual installation images for user reference during on-site setup.\n Accessories Mount in the factory  Accessories VESA mount The Neon camera provides standard VESA mount installation(10x10cm) and can be referenced with accessory 91-95321-0010. Tripod You can also refer to our designed tripod adapter plate to mount the Neon camera on a photography tripod, for example Velbon EX-Macro. Mount in the factory "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtobackupandextendsdcard/",
	"title": "How to backup and extend microSD card?",
	"tags": [],
	"description": "",
	"content": " According to application requirements, when the micro SD card installed in the Neon camera or EOS-JNX doesn\u0026rsquo;t meet the capacity needs, you can follow the steps below to back up and restore the card\u0026rsquo;s contents to a larger storage device.\n Back up micro SD card Restore image Clone micro SD card by rsync command Extend the partition of SD card  Neon camera and EOS-JNX supports addressing larger capacity SD 3.0 or SD-XC cards up to 2 TB. We have tested the following cards:\n SanDisk ExtremePRO microSDXC 512GB SanDisk ExtremePRO microSDXC 128GB SanDisk ExtremePRO microSDXC 64GB Transcend TS128GUSD430T 128GB Transcend TS64GUSD430T 64GB  Back up micro SD card Please prepare the Linux x86 machine and use the \u0026ldquo;disks\u0026rdquo; tool preinstalled in ubuntu.\n   Restore image Restore the backup image made from previous section into your another SD card.\n Clone image file to microSD card using one of the following methods  Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card      Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher   Clone micro SD card by rsync command  format your target micro SD card as ext4 format   \n rsync [OPTION] SOURCE [SOURCE]... DEST\n For example sudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/test \u0026amp;\u0026amp; sync    Extend the partition of SD card If your memory card space is larger than the backup image, you need to extend your memory card partition.\n   "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtoupdatecamfw/",
	"title": "How to update Camera Firmware of Neon-20XA-Series?",
	"tags": [],
	"description": "",
	"content": " Keeping your Neon Camera updated with the latest firmware ensures more stable performance for your application.\n  Neon-Series Neon-201A Neon-202A   Latest FW Version 23414 33413   Step 1: Check camera FW version v4l2-ctl -C version\nStep 2: Update camera FW //Here is an example where the FW file has been pre-downloaded and extracted to /home/adlink. cd FW-24314 sudo dd if=RS_M12MO_AR0234C70E.bin of=/sys/kernel/debug/m12mo_a/fwupdate obs=2093056 //The default password of Neon is adlink  Step 3: Unplug Neon power and power on after flash procedure Step 4: Check the FW version again v4l2-ctl -C version\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/",
	"title": "EOS-iSeries",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory EOS-iSeries Discover how to use the EOS-iSeries and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/",
	"title": "GigE Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/",
	"title": "PCIe-GIE7x Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s ADLINK\u0026rsquo;s PCIe-GIE7x series PCI Express® PoE+ frame grabber supports 2\u0026frasl;4-CH independent Gigabit Ethernet ports for multiple GigE Vision connections transferring up to 1 Gb/s per port. PoE+ provides up to 30W power and automatic detection for stable, reliable connections, reducing costs, simplifying installation, and easing maintenance burdens.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/gettingstart/",
	"title": "Getting start with Yolov3",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start with EOS-iSeries from un-boxing.\nStep 1: Hardware wiring  Connect the peripherals, keyboard, mouse, monitor and cameras. Connect 110V AC power source to the terminal block Power on   Step 2: Run inference with different source The path of inference sample locates on the C:\\Users\\user\\Desktop\\ADLINK. You can select image, video, webcam or Basler GigE camera as inference source.\nOpen Terminal and paste commands below: - Image\n cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights C:\\Users\\user\\Desktop\\ADLINK\\darknet-For_basler_camera\\data\\dog.jpg    Terminate program by close the Terminal or ctrl+c.\n  Video\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4     Webcam\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe web_camera     Basler GigE camera\n Open pylon Viewer and make sure GigE camera works.\n Key in commands\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe basler_camera  or Click Run_basler_camera_with_Object_Detection.bat in folder\n    "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-1000-mdx/",
	"title": "Neon-1000-MDX",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision Neon-1000-MDX Discover how to use the Neon-1000-MDX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " Frequently asked questions (FAQ) about Pylon\nHow to grab the Image from the GigE Basler.  #### Step1 : Set the Lan port IP address ? Assign the TCP/IP4 IP to be 192.168.11.1 #### Step2: How to set the Camera IP by Pylon? Set the Ip 192.168.11.4 by Pylon IP configurator. \u0026mdash;  Video   "
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/powerbudget/",
	"title": "What is PCIe-GIE74P PoE Budget Mode？",
	"tags": [],
	"description": "",
	"content": " The PoE budget mode is to ensure product power supply security, PoE automatically cuts off when PoE power is exceeded.  PCIe-GIE74P PoE Power budget 1、Max 20.0W w/ PCIe slot only. 2、Max 61.6W w/ PCIe slot and 4-pin Molex connector.  How does PCIe-GIE74P reserve PoE Power Budget？ ADLINK provides two power budget modes, customers can configure settings according to user scenarios.  PoE Power Consumption Mode for variable power budget PCIe-GIE74P will detect the camera’s real power consumption to reserve PoE power budget. PoE Class Mode for fixed power budget PCIe-GIE74P will detect the camera’s PoE class to reserve PoE power budget.  How to setting Power Budget Mode？ 1、Using API of AVS_PoESetPowConsumCalcModel. Customer can configure settings according to user scenarios. 2、Execute POE Budget Mode Setting Tool. This tool can set multi-card to the power consumption mode at one time. Step1、Download SDK from WebLink Step2、 Install AVS SDK Step3、 Open Folder \u0026ldquo;POE_tool\u0026rdquo;. Step4、 Execute “Sample.exe”.  Example、User’s Situation. If I want to connect 3 GIgE cameras, and I didn’t connect PCIe-GIE74P’s 4-pin Molex connector\u0026hellip; By PoE Power Consumption Mode If this camera’s real power consumption is 5 watts, PCIe-GIE74P will reserve 6 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 5 (watts) = 15 (watts)\nPCIe-GIE74P can power 3 cameras. By PoE Class Mode If this camera’s PoE class is “2”, according to the PoE class of the PoE standard, PCIe-GIE74P will reserve 7 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 7 (watts) = 21 (watts)\nPCIe-GIE74P can only power 2 cameras, cannot power the third camera.   If you want to provide more than 20 watts of power, you need to connect the 4-pin Molex connector of PCIe-GIE74P.\n  ##### Table PoE Classes     Class No. Type Maximum power available at the Power Sourcing Equipment (PSE) Power required by PoE class at the Powered Device (PD)     0 802.3af 15.4 W 0.44 – 12.95 W   1 802.3af 4.0 W 0.44 – 3.84 W   2 802.3af 7.0 W 3.84 – 6.49 W   3 802.3af 15.4 W 6.49 – 12.95 W   4 802.3at(PoE+) 30 W 12.95 – 25.5 W    "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/datasheet/",
	"title": "00-Datasheet",
	"tags": [],
	"description": "",
	"content": " All accessory datasheets for KVM local and remote packages kit are available. Accessory list  HDMI to VGA adapter VGA to HDMI Converter VGA Splitter 1 to 2 KVM DX-131RX VX-131TX  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/newloadproject/",
	"title": "00-New project &amp; Run Script &amp; combine Script",
	"tags": [],
	"description": "",
	"content": "  How to create a new project Reload a project(blf) when remodifying  How to combine a script before running Run the script (.SPT) What is the difference between with SPT VS BLF  How to create a new project Step:  1 Click New 2 Click Select 3 Choice the folder for save the script data 4 Click Ok  Reload a project when remodifying Step  1 Click load 2 Choice the .blf list file  How to combine a script before running Step  1 Click mouse button of right in any one getting.bmp 2 Combine Script -\u0026gt; General (It\u0026rsquo;s ok for choicing any one bmp) 3 Inupt the name and click ok  Run the script Step  1 Click load 2 Choice the .spt list file 3 select the file 4 Click open   Click Run Scripts  What is the difference between with SPT VS BLF Pleas see below list\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/quicklystart/",
	"title": "00-Quckly start (Video)",
	"tags": [],
	"description": "",
	"content": " DEX-100 Quckly start (video) Follow those videos you can quickly finish one project by yourself.  You can check other links for getting more tips, samples, and notes.\n -製作圖層篇,How to make the layer\n-設定Page ID篇,How to set the PageID\n-編輯腳本篇,How to edit the instruction\n-OCR使用REST上拋資料篇,How to get the OCR by restful\n-設定OP Screen篇,How to set the OP Screen\n-自動執行腳本篇,How to set for autorun the script\n-OCR設定篇,How to add the OCR\n-OCR 字庫訓練工具篇,How to train OCR to improve accuracy\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/goto/",
	"title": "01-Jump",
	"tags": [],
	"description": "",
	"content": "  Use GoTo fulfill JUMP function Use both element of GoTo with Lebel fulfill JUMP function  Use PageID fulfill JUMP function Use both element of PageID with Lebel fulfill JUMP function Use Table Compare fulfill JUMP function  How to design \u0026ldquo;Jump function\u0026rdquo; by GOTO \u0026amp; PageIdentify \u0026amp; label \u0026amp; Table compare. There are three function can fulfill JUMP\n GOTO : Directly jump to other command line. PageID : It is like the pattern match and jump to other command line. Compare Table : Compare the OCR Table and jump to other command line.  Use GoTo fulfill JUMP function Discription Use both element of GoTo with Lebel fulfill JUMP function Discription Use PageID fulfill JUMP function Discription Use both element of PageID with Lebel fulfill JUMP function Discription Use Table Compare fulfill JUMP function Discription "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/layer/",
	"title": "01-Layer",
	"tags": [],
	"description": "",
	"content": " How to add the bmp(layer) and select the folder for project You can flow the below steps or watch the video link. - Quickly guide Video\nSOP  Select one folder to save the script data.   Click Connect   Select the browser of Remote Control   Click Control   Move mouse or key the keyboard to you want page(bmp)\n Click snapshot\n  "
},
{
	"uri": "https://aiot-ist.github.io/dex/unboxing/",
	"title": "01-Unboxing",
	"tags": [],
	"description": "",
	"content": " DEX-100 Unboxing  Equipment wiring map  DEX-100 wiring for VGA  DEX-100 wiring for DVI DEX-100 wiring for USB DEX-100 wiring for PS2  DEX-100 for editing environment What is the different between (DP to VAG) and (VGA out) ?  Equipment wiring map The basic wiring is below the figure. DEX-100 wiring for VGA  This is PC connected to screen by VGA cable.  Separate the VGA cable that the VGA-INPUT connects to dex-100 then VGA-OUTPUT connects to screen.   DEX-100 wiring for DVI  This is PC connected to screen by DVI cable.  Separate the DVI cable that the DVI -INPUT connects to dex-100 then DVI -OUTPUT connects to screen. 1.DVI-D is dual-link type please referring.\n 2.DVI Output shall display when DVI input and DVI output are connected then restart the dex-100.\n   DEX-100-wiring-usb  These are USB of keyboard and mouse how to wire with DEX.   DEX-100-wiring-ps2  These are PS2 of keyboard and mouse how to wire with DEX.   DEX-100 for editing environment  The dex-100 has one GUI that is dex-pro, the dex-pro can design a script for auto-running and show the exclusive screen by DP. Due to editing the script that requires ones pair keyboard and mouse then show in the exclusive screen.   What is the different between (DP to VAG) and (VGA out) ?  Describe the different functions between (PD to VAG) and (VGA OUT), Please referring the below figure. e  "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/script/",
	"title": "01-Unboxing the KVM",
	"tags": [],
	"description": "",
	"content": " How to unbox and wire for both KVM local and KVM remote solutions. Overview  Full view of KVM local accessories Full view of KVM remote accessories  Unboxing  KVM Local accessories KVM remote accessories   Full view of KVM local accessories Full view of KVM remote accessories KVM Local accessories  VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100   Machine VGA wiring to KVM   Keyboard \u0026amp; Mouse wiring from KVM to Machine  KVM remote accessories  VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100   Wiring both TX and RX   Wiring the VGA from Machine to TX  "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/unboxing/",
	"title": "01-Unboxing the KVM",
	"tags": [],
	"description": "",
	"content": " How to unbox and wire for both KVM local and KVM remote solutions. Overview  Full view of KVM local accessories Full view of KVM remote accessories  Unboxing  KVM Local accessories KVM remote accessories   Full view of KVM local accessories Full view of KVM remote accessories KVM Local accessories  VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100   Machine VGA wiring to KVM   Keyboard \u0026amp; Mouse wiring from KVM to Machine  KVM remote accessories  VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100   Wiring both TX and RX   Wiring the VGA from Machine to TX  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/addscript/",
	"title": "02-Add script",
	"tags": [],
	"description": "",
	"content": "  Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction   How to add script Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/mouse/",
	"title": "02-Mouse",
	"tags": [],
	"description": "",
	"content": "  Click Dclick Up Down Coordinate refer from OCR Table  How to Click、Dclick the mouse in the script Instruction can simulat those command for mouse button of left,middle and right.\nFollow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse click the bmp in right side 4. Show the coordinate X and Y 5. Can add bye hotkey or add-Script 6. You can see the click/Dclick instruction in the script How to shift the popup form in instruction       Follow the steps:\n1. Click the Starting poistion 2. Recorder the coordinate X \u0026amp; Y. 3. Choice the instruction of \u0026ldquo;left click down\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 4. Click the second poistion 5. Recorder the coordinate X \u0026amp; Y. 6. Choice the instruction of \u0026ldquo;move\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 7. Click the ended poistion 8. Recorder the coordinate X \u0026amp; Y. 9. hoice the instruction of \u0026ldquo;left click up\u0026rdquo; and keyin the coordinate X \u0026amp; Y. How the position coordinate refer the dynamic OCR Table   sample data 1. Script.7z 2. restful.py 3. SOP.zip\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/keyin/",
	"title": "03-Keyin",
	"tags": [],
	"description": "",
	"content": "  From Text From OCR Table From external txt Choice Number key  From Text The instrction can key in the statice string.\nFollow the steps:\n Choice the getting_xxx.bmp Screen capture mode Add script and select instruction of key-in  Key-in from Text x = 1 from text Text = \u0026ldquo;ADLINK GOGO\u0026rdquo;   From OCR Table  Choice the getting_xxx.bmp Screen capture mode Add script and select instruction of key-in  Key-in from Text\n x = 3 from text\n Y = 1 OCR Table\n PS: Key-in value from the \u0026ldquo;OCR Table 1 = ABCD1234 \u0026rdquo;\n  From external txt  Choice the getting_xxx.bmp Screen capture mode Add script and select instruction of key-in  Key-in from external text   Choice Number key Keyboard\u0026rsquo;s 0 to 9 there are two types for sent the different ASCII, so one parameter can switch it. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/pageid/",
	"title": "03-PageID",
	"tags": [],
	"description": "",
	"content": " How to identify the feature for each bmp(layer) You can follow the below steps or watch the link below Video.\n Quickly guide Video  SOP  Open your design\u0026rsquo;s [.blf] file.   Select the page of Page Identify Define and draw the region(ROI)  Example For the ROI, all parameter values of pixel size must be multiples 4.\n  Save config the region of the feature(ROI) after adjusting the threshold for each BMP(layer).  A larger threshold can tolerate more noise, but it will lose accuracy.\n  Click ok   Load config for checking PageIdentify enabled   Click the button to connect again then seeing the light blue bar. It is successful.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/",
	"title": "04-Instruction",
	"tags": [],
	"description": "",
	"content": " Overview Video Video Funtions list How to design \u0026ldquo;Jump function\u0026rdquo;  jump to flag or next step by goto\n Jump to flag when this page is right. Jump to next or previous step when comparing the OCR table.  How to design \u0026ldquo;click function\u0026rdquo;  Left-button,right-button and midel-button all can single-click and double-click. Left-button,right-button and midel-button all can click-down or click-up. The mouse can click multi-point by dynamically update the OCR table.  How to design \u0026ldquo;key-in function\u0026rdquo;  key-in string is fixed. key-in string is from OCR Table. key-in string is from text.  Add OCR and save the log  How add OCR in script How to add the Exist ROI setting How to save the OCR table in CSV  How to design a for loop  Use the single loop. use the dual loop.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ocrandlog/",
	"title": "04-OCR and Log",
	"tags": [],
	"description": "",
	"content": "  How to add the ROI for OCR in script How to set the \u0026ldquo;Exsit Setting\u0026rdquo;  Gaussian blur Threashold Super resolution Interpolation Confidence  How to save the ROI result in csv  Enable the sharing OCR Table items Add instruction for recoding the organizations in CSV  Save all OCR or part of OCR    How to add the ROI for OCR in script Follow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse drawing the ROI in the layer before click add  Draw the ROI which you want to monitor. (1~4) Click the Add for increasing the OCR instruciton in the script.(5)   ### 4. Select the \u0026ldquo;Exist Settings\u0026rdquo; with each ROI\nBase on the amount of your designed ROI and select the \u0026ldquo;Exist ROI Setting\u0026rdquo; until lastly ROI. There are 4 pair instructions in the script after adding OCR. (For this demo )\nIf your \u0026ldquo;Exist Setting\u0026rdquo; is such as the left side of the below figure , pls view the link\n How to set the Exist Setting Introduc the parameters in Exist Setting.\nAdd new one \u0026ldquo;Exist Settings\u0026rdquo; recipe  Gaussian blur  Functional difference for real case\n### Threashold ### Super resolution Interpolation ### White and Black List Traineddata and Configuration What is the \u0026ldquo;Traineddata\u0026rdquo;?\nDue to character have many font,size and color, We usually separate the same feature in the same trainedata folder. If you don\u0026rsquo;t have any ideal,Please referring the [Link]()\nWhat is the \u0026ldquo;Configuration\u0026rdquo;?\nChoose the configuration (single selection) selection). Press the Advanced Setting button for Tesseract advanced settings if required as follow. For the more parameters information in detail , you have to refer the tesseract from google. config.cfg is the golden and config1.cfg is up to user defined)\nConfidence How to save the ROI result in csv The script can log that you want to monitor OCR in CSV after enable sharing the OCR Table.\nEnable the sharing OCR Table items:  1. Open the configuration managment  Click the DDS \u0026amp; REST (OCR configuration)    Select Reest   Modify 0 to 20 and check yes then click \u0026ldquo;Save to items\u0026rdquo; (for this sample)    5. Check thes OCR item to be Ture that you want to monitor.   6. Stop REST 7. Run REST (Reset the REST module)   8. It is successful when you resee the restful module popup again.   9. Exit   Add instruction for recoding the organizations in CSV  1. Open you desing\u0026rsquo;s [.blf] file.   2. Choice the bmp which you want to design   3. Click mouse button of left side then Add the Script  Save all OCR or part of OCR  Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (save all OCR)   Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (part of OCR)  where is the CSV ? There is a file per day.\n  Path C:\\Users\\USER\\Documents\\RCM_TABLE_LOG The system default keeps the files for 30 days however you can modify the parameter.\n "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/backup/",
	"title": "05-Backup/Restore",
	"tags": [],
	"description": "",
	"content": "  DEX Pro Backup DEX Pro Restore Backup Config Replace Config Factory Restore OCR Restore  When you backup/restore the machine ? You want to exchange to another DEX-100 and going to run that you design script and parameter. DEX Pro Backup  Content OCR data base Schedule file\n Program  DEX Pro Restore  Content OCR data base Schedule file Program\n  Backup Config  Content DDS/LOG Config Modbus Config Machineini Page Definition  The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config.\n Replace Config  Content DDS/LOG Config Modbus Config Machineini Page Definition  The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config\n Factory Restore  Content Machineini  OCR Restore SOP  Download the OCRClean.bat Run the OCRClean.bat Download the OCRDB.zip and unzip it. Using the DEX Pro Restore and choice the OCRDB folder.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/loop/",
	"title": "05-LOOP",
	"tags": [],
	"description": "",
	"content": "  How to use the loop  How to use both loop2 with loop  How to the loop X: Loop in execution.\n\u0026gt;0: go to previous steps; =0: go to the last instruction \u0026lt;0: go to next steps\nY: Loop is ended. \u0026gt;0: go to next steps =0: go to the last instruction \u0026lt;0: go to previous steps;\nW: This value denotes the iteration times of the loop\n Pleas referring the sample:   How to use both loop2 with loop They are a pair both loop2 and loop when you use the double loop.\n Pleas referring the sample: The loop2\u0026rsquo;s start instruction must be the first loop.   "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/pageidetify/",
	"title": "06-PageIdentify",
	"tags": [],
	"description": "",
	"content": "  How to use the PageIdentify  How to the use the PAgeIdentify Introduction the parameter X: number of instructions to jump  X\u0026gt;0: go to previous steps X=0: go to the last instruction X\u0026lt;0: go to the next steps  Y: jump mode.  0: jump by instruction number 1: jump by label  W: target page id  Referring the number of bmp   Pleas referring the sample 1: The pattern matches(Yes) then going to the \u0026ldquo;Jump\u0026rdquo; label.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\nPleas referring the sample 2: The pattern matches(Yes), then going to the following four steps.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/autorun/",
	"title": "06-Script Setting &amp; AutoRun",
	"tags": [],
	"description": "",
	"content": "  Script Setting Scripts Management AutoRun  Script Setting  Time Stop: The waiting time after executing each instruction. Spt Step Speed: The minimum waiting time between instructions.  Example: Time Stop Use both settings.\nObserve the figure below, where the second instruction takes an additional 3s compared to the first instruction.\n How to get the 3s? - \u0026ldquo;Time stop\u0026rdquo; is 1000ms - \u0026ldquo;Sleep\u0026rdquo; is 3 - 3s = Time stop * Sleep\n Example: Spt Step Speed Set \u0026ldquo;Spt Step Speed\u0026rdquo; to 1000ms.\nNotice that the second instruction runs with a 1s delay after the first instruction.\nScripts Management Provides Multi-Script Management to define the trigger command table.\nIn other words, you can use RESTFUL or MSMQ to trigger the required script.\nOpen the MSMQ Execute Scripts Management Path: Tools \u0026gt; MSMQ Execute Scripts Management 1. Choose the MSMQ Cmd 2. Select the Scripts file 3. Click Change. Attachments MSMQ \u0026amp; RESTFUL sample  MSMQ C# Sample (Download) Python Sample\n Download and install python from python.org Run pip3 install requests Sample code\nimport requests import json #trigger script url3 = \u0026quot;http://127.0.0.1:5555/executescript/2\u0026quot; headers = {'Content-Type': 'application/json'} response3 = requests.post(url3, headers=headers) print(response3.text) #stop url4 = \u0026quot;http://127.0.0.1:5555/stopscript\u0026quot; headers = {'Content-Type': 'application/json'} response4 = requests.post(url4,headers=headers) print(response4.text)    AutoRun Set up the Background Schedule.\n Path: Tool \u0026gt; Config \u0026gt; Background Schedule Setting\n Select the \u0026ldquo;Script File\u0026rdquo; you want to run.\n Note: Enable the \u0026ldquo;Run script if HID keeps idling over timeout.\u0026rdquo;\n You can also click Start(Auto) without selecting the \u0026ldquo;.spt\u0026rdquo; file again once you have selected the script file.\nHID idle time: The script will AutoRun when both keyboard \u0026amp; mouse have been idle for the set time.\nAutorun screensaver timeout: A 10-second countdown will begin before automatic execution.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/restful/",
	"title": "07-Restful",
	"tags": [],
	"description": "",
	"content": "  How to add the instruction for RESTFUL API Upload all items once Uplaod one by one RESTFUL function list RESTFUL sample code and Postman application  How to add the instruction for RESTFUL API Sop 0. Please enable the OCR Table you can refer the Enable the sharing OCR Table items,if you don\u0026rsquo;t how to set. 1. Add the script 2. Select the MSG_SAVE_TABLE_TO_LOG Upload all items once  Q : 0,Upload all once through RESTFUL(Submachinestatus) X : 0,Save all items \u0026gt; - Text : Bypass the parameter when X is 0 and recode the ROI Table referring to the items of True status  Get the all items by Submachinestatus of RESTFUL API.\n  The items auto upload once based on your design OCR amount when executing this instruciton.   There are 5 ROI case.\nThere are 9 ROI case\nUpload one by one  Q : 1,Upload one bye one through RESTFUL X : 1,Save selected items Text : 1~4;7 \u0026gt; - (This case is only allowed those 1 to 4 and 7 of OCR table Set or Get by RESTFUL API.)  You can see the relationship in the below image.\nWhat is one by one The RESTFUL\u0026rsquo;s Get function can get one data once for you select OCR table items and without parsing anything metadata.\nThe RESTFUL\u0026rsquo;s Set function can set one data to OCR table for handshaking with other app.\n1.Set the data to OCR table by anytime.\n2.Get the current data from OCR table after executing this command.\n3.It wait more time by more you select OCR items.\n RESTFUL function list They support two format both XML and Json. Doc\n The port is 5555 for RESTFUL. (Exapmle for local : 127.0.0.1:5555)\n 1. SubmachineStatuse  You can get all the OCR tables that you design for sharing,but mabye you shall analyze it by yourself.  2. Get  You can get one metadata from OCR Table that you design for sharing. Those OCR TABLE are update after executing this MSG_SAVE_TABLE_TO_LOG instrution.  The 0 in the OCR table cannot be obtained\n 3. Set  You can set any data to OCR Table.  4. Executescript  It can trigg the script that you want to autorun.  5. Stopscript  It can stop the current script.  RESTFUL Sample code and Postman applictation  C# sample Python sample Postman Application\n3.1 SubmachineStatuse  For Command content: http://x.x.x.x:5555/submachinestatus (exchange the ip by yourself)\nFor Body content: {\u0026ldquo;DES\u0026rdquo;:{\u0026ldquo;MachineStatus\u0026rdquo;: {\u0026ldquo;SubDataType\u0026rdquo;: \u0026ldquo;RAW_DATA\u0026rdquo;}}}\n 3.2 Set value  For Command content: http://x.x.x.x:5555/setocrdata (exchange the ip by yourself )\nFor Body content : OCRID : 22 (OCR Table 22),OCRValue : 7878\n 3.3 Get value  For Command content: http://x.x.x.x:5555/getocrdata/3 (exchange the ip and 3 ,it is ocr table address, by yourself )\n 3.4 Executescript  For Command content: http://x.x.x.x:5555/executescript/1 (exchange the ip and 1 ,MSMQ cmd number, by yourself )\n 3.5 Stopscript  For Command content: http://x.x.x.x:5555/stopscript (exchange the ip by yourself )\n  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ledcontrol/",
	"title": "08-LED control",
	"tags": [],
	"description": "",
	"content": " How to control the LED You can control the LED ON/OFF by instruction.\nExample : There two LED turn on \u0026amp; ture off by instruction. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/printscreen/",
	"title": "09-Printscreen",
	"tags": [],
	"description": "",
	"content": "  How to Print Screen  Print Screen for tracing with OCR How to choose PNG or JPEG for storing image  How to Print Screen Sometimes you need to store the current screen for analysis by another tool without using the OCR.\n1. Add one OCR of instruction Please feel free to add a position of OCR (this OCR is meaningless and only for creating instruction) in .blf mode and combine it after saving the Script.\n2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo;  Load the .spt of your design before modifying the parameter. Select the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path.  The storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\n Print Screen for tracing with OCR To verify the results of OCR with the current screen, you can refer to the below steps.\n1. Add the OCR of instruction This sample has 4 OCR and one instruction of “MSG_SAVE_TABLE_LOG” in .blf mode and combines it after saving the Script.\n Note: To log the results of OCR in CSV, that needs the “MSG_SAVE_TABLE_LOG.”\n ### 2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo;\nSelect the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path.\nThe storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\n Sample sharing How to choose PNG or JPEG for storing image Mabey, you have different considerations about your tools to choose the saving type of images.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-source/",
	"title": "1 - Choice the Dex-100 Video Source",
	"tags": [],
	"description": "",
	"content": " Choice the Dex-100 Video Source Click the Frame Grabber Setting and choice the Source input.  Dex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting  Use the Diag.exe for checking the current video information.  Path -\u0026gt; -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  Clear horizontal and vertical offset values  Click the button of \u0026quot;Frame Adjust\u0026quot;   Please set 0 for all the offset\n  Select source and resolution  Select source from VGA or DVI   Select resolution \u0026ldquo;Auto\u0026rdquo;   Get the Video informations  Check the both result for Resolution and Signal, if the connection is successful.  If doesn\u0026rsquo;t detected the signal, follow the steps for troubleshooting.   Check the Local machine setting for display resolution.   Reconnect the local machine VGA cable to monitor.   Reboot the local machine.   Check the resolution from the monitor.   Reconnect the VGA to Dex-100.   Check Frame Grabber Setting whether the auto select for unexpected resolution.   "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/opscreen/",
	"title": "10-OPScreen",
	"tags": [],
	"description": "",
	"content": "  How to use the OPSCreen  Print Screen for tracing with OCR  When to use the OPscreen The operator or engineer wants to view the specified page when DEX-100 is running. At this moment this function will be very suitable.  The OPScreen supports a max of 5 pages of all.\n Those 5 pages are not the current frame. However, dex-100 can autosave the image when the script is running to the required page.\n The Operator or Engineer only can select any pages of 5, and keyboard \u0026amp; mouse are disabled to control the machine. (This moment Dex-100 is working)\n  Need to know OPScreen is a special mode that screen is from VGA out and the mouse only can control by Mouse Input. How to use the OPSCreen -VIDEO,How to set the OP Screen\nSOP 1. Open the OP Screen setting 2. Select the Host view 3. Click the Done when you finish the setting. If you saw \u0026ldquo;Fail to load image: page X is undefined.\u0026ldquo;, Please recheck the Pageidentify setting.\n How to stop by the trigger of external or internal? 1. Trigger by internal (software) Use the mouse click the button of Full Operation. 2. Trigger by external (hardware) The DI2 ON can alse stop the OPScreen mode. \u0026gt; kill switch is enable.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-quality/",
	"title": "2 - Video quality adjustment",
	"tags": [],
	"description": "",
	"content": " How to adjust quality when inputting the bad video  3. Using the Diag.exe 4. Select source \u0026amp; resolution 5. Draw the image by clicking the \u0026ldquo;Start\u0026rdquo; 6. Confirm the current image that is right 7. Using the \u0026ldquo;Video Quality Analysis\u0026rdquo; 8. Choose the test interval and start the testing 9. Cancel the testing log or save it in storage 10. System auto full in the result in the parameter of the sampling phas 11. Trobleshooting  Steps 1 Boot up the dex-100 Power on the DEX-100\n2 Input the Video source (VGA or DVI) Plug-in the video source cable.\n3 Using the Diag  Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  4 Select source and resolution  Select source  Select resolution   5 Draw the image by clicking the button of Start You shall see the result when grabbing is successful.\n- - 1. It is the current source - 2. It is the current resolution - 3. It is succeful or filed\n If the resolution can\u0026rsquo;t match your choose resolution or the signal can\u0026rsquo;t be detected, please follow the below steps.\n - Dex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting - 6 Confirm the current image that is right  It is not right, please retrying Step 2~5 again.  Dex-100 can\u0026rsquo;t process some Intel chips that can\u0026rsquo;t output your required resolution because the resolution always keeps in one resolution.\n 7 Using the Video Quality Analysis  Test VGA video quality for a long time and give suggested parameters\n (Ensure that the video quality of the machine is almost the same throughout the day.)\n   8 Choose the test interval and start the testing Choose the test interval There are several parameters for your requirement. - Click the \u0026ldquo;start\u0026rdquo; button. - 9 Cancel the testing log or save it in storage You shall see the report after automatically test the video quality. - Click the ok button.\n10 System auto full in the result in the parameter of the sampling phas It is successful when you see the value be set in the \u0026ldquo;Sampling Phase\u0026rdquo; from the testing result.\nThe system only brings into the testing result of the sampling phase in the correct parameter.\n    11 Trobleshooting  If you get a large value from \u0026ldquo;Average Dynamic Noise,\u0026rdquo; we suggest replacing the VGA cable with anti-noise (EMI CORES) when getting the unideal value after using Video Quality Analysis.\n   "
},
{
	"uri": "https://aiot-ist.github.io/dex/video/resolution/",
	"title": "3 - Create new resolution",
	"tags": [],
	"description": "",
	"content": " How to increase a new resolution  Frame Grabber Setting  Attachments   Frame Grabber Setting  Path -\u0026gt; DEX-Pro -\u0026gt; Tool -\u0026gt; Config -\u0026gt; Frame Grabber Setting   Select an Auto mode in \u0026ldquo;Frame Grabbe Configuration.\u0026rdquo;\n   Steps () 1 Get the new resolution by Screen.    2 Run the ADLINK_SFDT Run the ADLINK_SFDT.exe follow the below path.\n Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\ADLINK_SFDT   3 Detect the parameters from the current video You can get the parameters from the current resolution after clicking the “Sense ”button.\n   If no parameters are displayed, please refer to the below link (troubleshooting)\n video-source\n4 Backup the dex100_rgb.txt Clone the dex100.rgb to desktop and copy one more to be a backup file.\n Path -\u0026gt; C:\\Windows\\dex100_rgb.txt     5 Increase a new resolution table  Please search for a set of resolutions closest to your desire and copy it to the top of the \u0026ldquo;dex100_rgb.txt\u0026rdquo;.\n    6 Edit the resolution name [Width Height fps Interlace]  Name the new title and modify the parameters (Width、Height 、FPS) to quickly selected the resolution list.\n    7 Fill in the parameters [HF VF VTotal Vsw]  Fill in the parameters from the ADLINK_SFDT tool in “dex100_rgb.txt.”\n    8 Get the unknown parameters  Get the unknown parameters by filling in the new resolution((Width、Height 、FPS) in the first sheet “CVT” of VesaCVT.xlsx\n    9 Fill in the parameters to Way_2 Fill in the parameters from the ADLINK_SFD in the sheet of Way_2.\n   10 Fill in the parameters [Hf HTotal Hsv Hbp Vf VTotal Vsw Vbp PClock Cr] Please key-in each color-bound box parameter to dex100_rgb.\n   11 Fill in the parameters in content of the new resolution set Please key-in each color-bound box parameter to dex100_rgb.\n   12 Save the txt 13 Replace the dex100_rgb.txt Replace the new dex100_rgb.txt to this C:\\Windows\\dex100_rgb.txt.\n14 Reboot the Dex-100 15 Reuse the Diag.exe  Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  15 Checking the resolution table can be choiced.    16 Choice the new list and click the button on start    17 Align the screen edge by button of Frame Adjust    Attachments VesaCVT.xlsx\ndex100_rgb.txt\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/coaxlink-quad/",
	"title": "Coaxlink Quad",
	"tags": [],
	"description": "",
	"content": "  How to install the firmware for one camera or multi camera  How to show the video by eGrabber Studio  How to install the firmware for one camera or multi camera You can use 1~4 cameras by installing the different firmware from Euresys Coaxlink Firmware management.\nSOP steps :  Open this Coaxlink Firmware management  Choice the avaliable card.  Select your required variant.  Click the button of proceed  Click the ok.  Wait to download finished.  Shart down and replug-in the power, then power on again.  How to show the video by eGrabber Studio SOP steps:  Open this eGrabber Studio  Select the Coaxlink and the system will auto show how many cameras can be used.  Choose each CCD and click the button to open it.  You shall see a new browser and click the play.    "
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/ps2metadata/",
	"title": "Detect the PS2 metadata",
	"tags": [],
	"description": "",
	"content": " How to acquit the ps2 metadata for KB or Mouse.  Step1: Power off both Dex-100 \u0026amp; host-machine Step2: Boot up the Dex-100 and run the Dbgview as administrator  Step3: Enable the function to follow the below image Step4: Setting the diffrent filter-string Step5: Power on the host-machine Step6: Record the logs by Dbgview  Step7: Save the log  Troubleshooting for DebugView DebugView download  Power off both of Dex-100 and host-machine The Dex-100 choices the USB or PS2 protocol by both keyboard and mouse successful connection with host-machine, but the handshake is beginning when power on for PS2 mode.\nBoot up the Dex-100 and run the Dbgview as administrator Enable the function to follow the below image DEX-100-wiring-usb Setting the diffrent filter-string Key-in the different filter-string for collecting the data of the mouse or keyboard and click ok.\n For log keyboard matamata : KBD  - For log Mouse matamata : KVM2\nYou can click the button and choose each string for collecting the data of the mouse or keyboard.\n Power on the host-machine Dex-100 choices the PS2 mode when hand-shake is successful both with the PS2 mouse and host-machine after power on the host-machine.\nDex-100 has only one rule for choosing PS2 or USB protocol by detecting the ps2 mouse.\n Record the logs by Dbgview You can see some metadata to show on the Debug Print , if it is successful.\nIt recordes logs when moving mouse or key-in any keyboard.\nPlease record both of Bypass mode \u0026amp; control mode.\n Save the log Save the log and sent back for Adlink contact window.\nTroubleshooting for DebugView Please follow the troubleshooting when you see the warning message\n Remove the Dbgv.sys as administrator or renmae it. Path : C:\\Windows\\System32\\drivers\n  Retry step1 to step7.  Attachment DebugView(save as to your computer)\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/disablemouseaccelerator/",
	"title": "Disable Mouse accelerator",
	"tags": [],
	"description": "",
	"content": " How to turn off the Mouse accelerator?  Step0: Disable Mouse accelerator by UI  Step1: Run the Registry Editor Step2: Change the Mouse speed setting  How to turn off the Mouse accelerator for Linux ?  Disable the mouse acceleartor in Terminal  Disable Mouse accelerator by UI in local machine Wake up the Mouse Properties and disable the \u0026ldquo;Enhance pointer precision\u0026rdquo; in the local machine\nPath:Control Panel\\All Control Panel Items\\Mouse\n Please follow the next step if you can\u0026rsquo;t find the UI of Mouse properties.\nRun the registry editor in local machine Using the hot-ky \u0026ldquo;Win+R\u0026rdquo;\nWake up the Registry Editor\nChange the mouse speed setting in local machine Edite the Mouse speed setting from 1 to 0. Define 1 = Enable 0 = Disabl\nPath:HKEY_CURRENT_USER\\Control Panel\\Mouse\\MouseSpeed\n Disable the mouse acceleration by command line in terminal for Linux  Step 1 : Call the Terminal by the hotkey \u0026ldquo;Alt + Ctrl + T\u0026rdquo;.\n Step 2 : Key-in the command line \u0026ldquo;xset m 1 1\u0026rdquo;  Sometimes your terminal is not the admin, maybe you can change the command line, \u0026ldquo;Sudo xset m 1 1\u0026rdquo; , if you know the admin\u0026rsquo;s password.\n "
},
{
	"uri": "https://aiot-ist.github.io/euresys/",
	"title": "Euresys",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory Euresys Tutorials about Easy Tool series\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/",
	"title": "Euresys Grab card",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory Euresys Grab Card Tutorials about the Grablink for Cameralink\nTutorials about the Coaxlink for CoaXPress\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/cxp-for-camera-triggering-with-encoder/",
	"title": "Using Euresys Coaxlink with Terminal Board  for Camera Triggering",
	"tags": [],
	"description": "",
	"content": " \n Terminal Board Signal Wiring and Frame Grabber System Architecture Overview Wiring External Input Signals to the Terminal Board Wiring Strob Output from the Terminal Board  Trigger Camera Control Method  Configuration in the Device Tab (Step 1) Setting the TriggerSource (Step 2) Setting the InputTool (Step 3) Verifying Operation with EventCount (Step 4) Setting the Strobe Output (Step 5) Setting the Cycle Trigger Source(Step 6)  Setting Divider with External Signals  Terminal Board Signal Wiring and Frame Grabber System Architecture Overview This section will explain how to connect the Terminal Board to the Coaxlink\u0026rsquo;s I/O board and verify signal reception. CXP Series \u0026amp; Grablink Duo I/O Connector (Pin Define) | Terminal Board (Pin Define)|Terminal Board Datasheet\nYou can find more information in the official Euresys documentation.\nCXP Series \u0026amp; Grablink Duo I/O Connector    Pin Signal Usage     1 GND Ground   2 DIN12+ High-speed differential input #12 – Positive pole   3 IIN11+ Isolated input #11 – Positive pole   4 IIN13- Isolated input #13 – Negative pole   5 IIN14- Isolated input #14 – Negative pole   6 IOUT12- Isolated contact output #12 – Negative pole   7 GND Ground   8  Not connected   9 GND Ground   10 GND Ground   11 DIN12- High-speed differential input #12 – Negative pole   12 IIN11- Isolated input #11 – Negative pole   13 IIN12+ Isolated input #12 – Positive pole   14 IIN13+ Isolated input #13 – Positive pole   15 IIN14+ Isolated input #14 – Positive pole   16 IOUT12+ Isolated contact output #12 – Positive pole   17 TTLIO12 TTL input/output #12   18 GND Ground   19 DIN11- High-speed differential input #11 – Negative pole   20 DIN11+ High-speed differential input #11 – Positive pole   21 IIN12- Isolated input #12 – Negative pole   22 IOUT11- Isolated contact output #11 – Negative pole   23 IOUT11+ Isolated contact output #11 – Positive pole   24 GND Ground   25 TTLIO11 TTL input/output #11   26 +12V +12 V Power output    Back to Wiring Map \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Back to Top\nTerminal Board Pin Define ( CXP Series \u0026amp; Grablink Duo)    Pin Signal Usage Pin Signal Usage     1 +12V +12 V Power output 14 GND Ground   2 DIN11+ High-speed differential input #11 – Positive pole 15 DIN11- High-speed differential input #11 – Negative pole   3 DIN12+ High-speed differential input #12 – Positive pole 16 DIN12- High-speed differential input #12 – Negative pole   4 GND Ground 17 GND Ground   5 IIN11+ Isolated input #11 – Positive pole 18 IIN11- Isolated input #11 – Negative pole   6 IIN12+ Isolated input #12 – Positive pole 19 IIN12- Isolated input #12 – Negative pole   7 IIN13+ Isolated input #13 – Positive pole 20 IIN13- Isolated input #13 – Negative pole   8 IIN14+ Isolated input #14 – Positive pole 21 IIN14- Isolated input #14– Negative pole   9  Not connected 22 GND Ground   10 IOUT11+ Isolated contact output #11 – Positive pole 23 IOUT11- Isolated contact output #11 – Negative pole   11 IOUT12+ Isolated contact output #12 – Positive pole 24 IOUT12- Isolated contact output #12 – Negative pole   12 TTLIO11 TTL input/output #11 25 GND Ground   13 TTLIO12 TTL input/output #12 26 GND Ground    Back to Wiring Map\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Back to Top\nWiring External Input Signals to the Terminal Board In this case, as encoder signals are being used, the High-speed differential input #11 has been selected as the connection point for its capability to handle high-speed signals. For signals that are not high-speed, standard input ports can be utilized. It\u0026rsquo;s important to select an appropriate input port based on the signal speed to ensure optimal performance and accuracy of the system.\nBack to Top\nWiring Strob Output from the Terminal Board Trigger Camera Control Method The Coaxlink card supports two modes for camera triggering:  NC Mode (Freerun): The camera captures images continuously at its own pace, independent of external signals. RG Mode (Asynchronous): The frame grabber dictates the camera's cycle rate and exposure, synchronizing it with external triggers like an encoder.   RG Mode is ideal for situations requiring precise timing control, such as high-speed imaging or synchronized event capture. Select the mode that best fits your application\u0026rsquo;s requirements.\nConfiguration in the Device Tab (Step 1) Adjust the following parameters in the Device Tab to configure the RG mode:   Set the Camera Control Method to RG. Enable Exposure Readout Overlap to allow the exposure and readout processes to overlap, optimizing the camera cycle time. Set the Cycle Trigger Source to Immediate, which triggers the camera cycle without waiting for an external trigger. The Cycle Trigger Source is a critical setting. Choose LIN1 (see Step 3 for detailed settings) or Divider as the source. It's essential to set this parameter correctly for proper system operation.   Calculating the Cycle Minimum Period To ensure the proper camera cycle rate, calculate the Cycle Minimum Period with this formula:  For example, if your camera's frame rate (FPS) is 40, the calculation would be:    Cycle Minimum Period = 1 / 40 FPS = 0.025 seconds = 25 milliseconds (ms)   Given the 25 ms Cycle Minimum Period, set both the Exposure Time and the Strobe Duration to 20 milliseconds (20000 microseconds) to ensure they remain within this operational limit. This keeps the camera's exposure and illumination well-coordinated and prevents exceeding the maximum allowed cycle time.    Exposure Time = 20000 microseconds (20 ms), which is less than the 25 ms limit.  Strobe Duration = 20000 microseconds (20 ms), which also does not exceed the 25 ms limit.   Back to Top\nSetting the TriggerSource(Step 2)  Different camera manufactures might use different names for their parameters.Always check with the camera manufacturer on parameters to set.\n Back to Top\nSetting the InputTool (Step 3) When configuring the InputTool in your system, it\u0026rsquo;s important to understand the function of each parameter:\n  LineInputToolSelector:  This parameter serves as a central reference for the InputTool configuration, encompassing the selected source and activation mode for the input. Once set, LineInputToolSelector ensures consistency across the system; any changes made to this parameter will automatically update all related settings that reference it. This feature streamlines the process of modifying the input source or trigger method, as adjustments need only be made once at this central point.  LineInputToolSource:   This parameter defines the source of the input signal. It can be used to specify where the signal is coming from, such as a specific hardware port or an internal source. LineInputToolActivation:  This parameter sets the activation mode of the input tool. It determines how the tool reacts to the input signal, such as triggering on a rising edge, falling edge, or other condition.   Properly configuring these parameters ensures that your system accurately responds to external signals and triggers. Back to Top\nVerifying Operation with EventCount (Step 4)  EventSelector: This parameter allows you to select the source for event monitoring. You can set it to LIN1 (as described in Step 3) or choose Divider as the source. Selecting the correct EventSelector enables you to track specific events relevant to your input configuration. EventCount: This parameter displays the count of the events detected. A value in EventCount indicates successful operation. For consistent movements over fixed distances, you will observe an equivalent accumulation in the EventCount value. EventCountReset: This parameter can be used to reset the EventCount value. It is useful for clearing the count to facilitate easier tracking and calculations of new events.   Click the button shown in the figure below to retrieve the current counter value.\n Back to Top\nSetting the Strobe Output (Step 5)  LineSelector [Selector]: This parameter is set to TTLIO11 to select the corresponding line for strobe output configuration. LineMode: The line mode is set to Output, which configures the selected line to act as an output, driving the strobe signal. LineSource: The source for the line is set to Device0Strobe   Setting Divider with External Signals Most encoders emit high-frequency signals, which are often too rapid for direct camera triggering. Using a divider, these signals can be adjusted to lower frequencies, suitable for stable camera activation, ensuring accurate and consistent image capturing.\n DividerToolSelector: This parameter acts as the central identifier for the Divider Tool configuration, integrating the selected source (specified by DividerToolSource), the activation control (determined by DividerToolEnableControl), and the division factor (set by DividerToolDivisionfactor) for the input. Once DividerToolSelector is configured, it guarantees consistency across the system; any subsequent modifications to the source, activation, or division factor are centralized through this single parameter. This consolidation simplifies the adjustment process, enabling changes to be made in one place that then propagate to all related configurations. DividerToolSource: Select LIN1 for the source, as previously described in Step 3. DividerToolEnableControl: Set this to 'Enable' to activate the divider tool. DividerToolDivisionfactor: Enter the division factor value you wish to use for frequency division.   Once you have configured the Divider Tool, remember to revisit Step 1 to adjust the \u0026lsquo;Cycle Trigger Source\u0026rsquo; accordingly. Additionally, you can verify the functionality by modifying the \u0026lsquo;EventSelector\u0026rsquo; in Step 4. These steps ensure that your settings are consistent and functional across the system.\n Back to Top\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/deep-learning-studio/",
	"title": "Deep Learning Studio",
	"tags": [],
	"description": "",
	"content": " Software version  WHITE PAPER STUDIO DATASHEET Deep Learning Studio  AT A GLANCE  Ease the evaluation of Open eVision\u0026rsquo;s Deep Learning tools Dataset creation and image annotation for classification, segmentation, and object localization Create and configure dataset splits to decide how your images are used Manage the data augmentation transformations Train your tools in succession thanks to the training queue Validation and analysis of the results of the trained tools Available on Windows and Linux Free of charge  Deep Learning Studio is a Visualization Tool Deep Learning Studio has a complete visualization training tool that can be used for proof of concept (POC) without requiring a license. The general workflow is as follows:\n Project Labeling Splitting Augmentation Training Tool Validation  Project In the project management of Deep Learning Studio, it is possible to create an AI model from five different models and individually set project names based on requirements. Labeling You can import your collected dataset and freely create the number of labels in Deep Learning Studio. Additionally, the software automatically compresses the images without the need for third-party tools to perform resizing operations. Splitting Deep Learning Studio allows you to split your dataset into training, validation, and test sets. You can create multiple dataset splits to experiment and check the performance of tools trained with different set of images. Augmentation Deep Learning Studio integrates annotation tools adapted to each library. For classification and unsupervised segmentation, you can quickly assign label to each image. For supervised segmentation, the segmentation editor allows you to draw the ground truth segmentation. For localisation, the object editor allows you to quickly draw the bounding box around each of your objects. Training Tool The Tools tab allows you to configure and train your tools. Operating on CPU or GPU, the training can be stopped and restarted at any time. You can launch as many training as you want thanks to the processing queue. The training and inference operations will be queued and processed one after the other. Validation The validation process is customized for each library to allow you to get the most out of your data. A comprehensive set of metrics, tables, and/or graphs is available to analyze and explore the results of the training process.\nTables and confusion matrixes allow you to filter your results to understand the strengths and weaknesses of the trained models. Score histograms and ROI curves are useful to select a threshold and adapt the trained models to your needs. "
},
{
	"uri": "https://aiot-ist.github.io/euresys/dongle/",
	"title": "Dongle &amp; License ID",
	"tags": [],
	"description": "",
	"content": "  Dongle rule Alternative solutions for PAR EOL License ID  Neo Licensing System  Neo is the new Licensing System of Euresys. It is reliable, state-of-the-art, and is now available to store Open eVision and eGrabber licenses. Neo allows you to choose where to activate your licenses, either on a Neo Dongle or in a Neo Software Container. You buy a license, you decide later. Neo Dongles offer a sturdy hardware and provide the flexibility to be transferred from a computer to another. Neo Software Containers do not need any dedicated hardware, and instead are linked to the computer on which they have been activated. Neo ships with its own, dedicated, Neo License Manager, which comes in two flavours: an intuitive, easy to use, Graphical User Interface and a Command Line Interface that allows for easy automation of Neo licensing procedures.  Dongle-rule License ID (NEO、Open eVision 、eVision)  Dongle \u0026amp; License ID\n thead th { position: sticky; top: 0; background-color: #eee; z-index: 1; } .scrollable-table { display: block; overflow-y: auto; height: 800px; /* Adjust based on your needs */ } table { width: 100%; border-collapse: collapse; } th, td { padding: 14px; border: 1px solid #ddd; text-align: center; } .licenses { background-color: #FFC0CB; /* Pink */ } .evision { background-color: #FFFFB3; /* Yellow */ } .eol, .eol-only-background { /*background-color: #FFD1D1; Light red */ background-color: #e0f2f1; } .eol { text-decoration: line-through; /* Strikethrough */ } th.licenses, th.evision,th.library { font-size: 24px; /* 或你希望的任何更大的字体大小 */ } tbody td { font-size: 20px; /* 或根据需要设置其他大小 */ } tbody td:hover { background-color: #e0f7fa; /*鼠标悬停时的背景颜色 */ border-color: #ff0000; /* 鼠标悬停时的边框颜色 */ border: 2px solid #ff0000; /* 增加边框宽度并设置为红色 */ } table { border-collapse: separate; border-spacing: 0; /* 调整为所需的间距 */ } .bg-licenses { background-color: #FFC0CB; /* 粉色，与 .licenses 相同 */ } .bg-evision { background-color: #FFFFB3; /* 浅黄色，与 .evision 相同 */ }                Library Open eVision Licenses eVision Licenses    Neo Dongle\n6514 USB Dongle\n6512 PAR Dongle\n6513 Host(Software)\n(EOL) USB Dongle\n6512 PAR Dongle\n6513 Board     Inspection Bundle (EasyImage,EasyGauge,EasyMatch,EasyObject,EasyColor) 4314 4164 4214 4264 4014 4064 4114   EasyMatrixCode 4307 4157 4207 4257 4007 4057 4107   Legacy Mark Inspection Bundle  4165 4215 4265 4015 4065 4115   EasyFind 4308 4158 4208 4258 4008 4058 4108   EasyMatch 4303 4153 4203 4253 4003 4053 4103   EasyGauge 4309 4159 4209 4259 4009 4059 4109   EasyImage 4301 4151 4201 4251 4001 4051 4101   EasyOCR 4305 4155 4205 4255 4005 4055 4105   Legacy ID Bundle 1.2  4176 4226 4276      EasyQRCode 4325 4175 4225 4275      Full Bundle 4327 4177 4227 4277 4017 4067 4117   Easy3D 4331 4181 4231 4281      Easy3DObject 4333 4183 4233 4283      Easy3DMatch 4334 4184 4234 4284      Easy3DLaserLine 4336 4186 4236 4286      EasyObject 4302 4152 4202 4252 4002 4052 4102   EasyColor 4304 4154 4204 4254 4004 4054 4104   EasyOCR2 4329 4179 4229 4279      EasyBarCode 4310 4160 4210 4260 4010 4060 4110   EasyOCV 4306 4156 4206 4256 4006 4056 4106   Deep Learning Bundle 4332 4182 4232 4282      3D Bundle 4335 4185 4235 4285      ID Bundle 4330 4180 4230 4280      Legacy Inspection Bundle     4016 4066 4116   Legacy EasyMeasure     4011 4061 4111   Legacy SDK  4178        eGrabber Gigelink 4400         eGrabber Recorder 4401               Alternative solutions for PAR EOL Due to the official announcement from Euresys that PAR Dongle is being phased out, USB Dongle will be recommended as a replacement in the future. Below are two suggested solutions:\n1. Please use USB Dongle with the dedicated part number for Open eVision Licenses. For example: Original solution: Inspection Bundle + PAR Dongle, 4214 + 6513\nAlternative solution: Inspection Bundle + USB Dongle, 4164 + 6512\n2. Please use USB Dongle with the dedicated part number for eVision Licenses. For example: Original solution: Inspection Bundle + PAR Dongle, 4064 + 6513\nAlternative solution: Inspection Bundle + USB Dongle, 4014 + 6512\nBoundle detail list    Number ID Function Items     4332 Deep Learning Bundle EasyClassify, EasySegment(u/s), EasyLocate   4337 Full Bundle EasyImage, EasyGauge, EasyFind, EasyMatch, EasyObject, EasyColor, EasyOCR, EasyOCR2, EasyBarCode, EasyMatrixCode, EasyQRCode   4335 3D Bundle Easy3D, Easy3DLaserLine, Easy3DObject, Easy3DMatch   4314 Inspection-Bundle EasyImage, EasyGauge, EasyMatch, EasyObject, EasyColor   4330 ID Bundle EasyImage, EasyOCR, EasyOCR2, EasyBarCode, EasyMatrixCode, EasyQRCode    "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easycolor/",
	"title": "EasyColor",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyColor by Euresys\n Performing Thresholding on Color Images Performing Color Segmentation\n Performing Thresholding on Color Images Following this tutorial, you will learn how to use EasyColor to segment a color source image, by setting a threshold value for each color component of the current color system. For example, to retrieve the solder pads on a PCB, you\u0026rsquo;ll perform a color segmentation based on the golden pixels (H), with a loose discrimination on the brightness (L) and saturation (S), to eliminate surface and lighting effects.\n Load the source image Create a destination image Create a color lookup table Perform the color segmentation Link on line Doc      Performing Color Segmentation Following this tutorial, you will learn how to use EasyImage to perform color segmentation.\n Load the source image Create a color lookup table Perform the color segmentation Link on line Doc       "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/",
	"title": "EasyDeepLearning",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start DeepLearning by Euresys\n EasyClassify EasySegment EasyLocate\n There are 3 functions for deep learning studio  EasyClassifier EasySegmentation  2.1 Unsupervised 2.2 Supervised  EasyLocate  3.1 Bounding Box 3.2 Interest Point     EasyClassify Link on line Doc\n  Easysegment Link on line Doc\nSupervised   Unsupervised   EasyLocate   "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/",
	"title": "EasyFind",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyFind by Euresys\n Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files Improving the Score of Found Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files Following this tutorial, you will learn how to use EasyFind to detect in multiple images highly-degraded occurrences of a reference model. The degradation can be due to noise, blur, occlusion, missing parts or unstable illumination conditions.\n Load the reference image Create an ROI to define the reference model on the reference image Learn the reference model Set rotation and scaling tolerances Select multiple images Browse multiple images Link on line Doc       ### Improving the Score of Found Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyFind to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; in geometric pattern matching. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image, such as text and numbers. Loading the reference image Creating an ROI to define the reference model on the reference image Learning the reference model Setting a rotation tolerance Detecting instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Defining the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detecting instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc       "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/qa/",
	"title": "EasyFind (Q&amp;A)",
	"tags": [],
	"description": "",
	"content": " Q\u0026amp;A list  How to reduce the working time for EPatternFinder.Find processing time\n How to reduce the working time for EPatternFinder.Find processing time Description the scenario: Users would like to reduce the working time or give up the execution when having a high processing time.\n Fundamental for EasyFinder EasyFind works internally in 2 stages.\n It selects reasonable candidates for the pattern It makes a finer analysis/positioning of the candidates.  Suggestion way There is a new parameter that may help by using the number of selected candidates has a direct impact on the processing time.   Link on line Doc\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easygauge/",
	"title": "EasyGauge",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyFind by Euresys\n Measuring the Rotation Angle of an Object Measuring the Diameter of a Circle Measuring a Distorted Rectangle Locating Points Regarding to a Coordinate System Unwarping a Distorted Image\n Measuring the Rotation Angle of an Object Following this tutorial, you will learn how to use EasyGauge to measure the rotation angle of a CCD sensor package. As we only need to retrieve an angle value, it\u0026rsquo;s not required to work in a calibrated field of view. All geometrical parameters and results will be express as numbers of pixels.\n Load the source image Attach a line gauge to the image Perform the inspection Link on line Doc       ### Measuring the Diameter of a Circle Following this tutorial, you will learn how to use EasyGauge to measure the diameter of a circle in an image. Load the calibration image Calibrate the field of view Load the source image Attach a circle gauge to the image Perform the inspection Link on line Doc     Measuring a Distorted Rectangle Following this tutorial, you will learn how to use EasyGauge to perform measurements on a distorted rectangle component..\n Load the calibration image\n Calibrate the field of view\n Load the distorted image\n Attach a rectangle gauge to the image\n Perform the inspection\n Link on line Doc\n     Locating Points Regarding to a Coordinate System Following this tutorial, you will learn how to use EasyGauge to perform lead frames inspection. This operation determines the dimension, position, curvature, size, angle or diameter of the lead frames with an excellent accuracy. Robustness is ensured by powerful edge-point selection mechanisms that are intuitive and easy to tune, allowing measurement in cluttered images.\n Load the calibration image\n Calibrate the field of view\n Loading a lead frame image\n Setting a coordinate system\n Attaching a point gauge to the frame shape\n Attaching other point gauges to the frame shape\n Loading another lead frame image\n Tuning the coordinate system\n Performing the inspection\n Link on line Doc\n     Unwarping a Distorted Image Following this tutorial, you will learn how to use EasyGauge to perform grid calibration, and unwarp a distorted image.\n Load the calibration image\n Calibrate the field of view\n Load the distorted image\n Unwarp the distorted image\n Link on line Doc\n       "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyimage/",
	"title": "EasyImage",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyImage by Euresys\n Transforming a Gray-Level image into its Black and White Edges Extracting an Object Contour Detecting the Corners of an Object Using Harris Corner Detector Detecting a Horizontal or Vertical Line Using Projection Creating a Flexible Mask \u0026amp; Computing Gray-Level Statistics Using a Flexible Mask Detecting the Corners of an Object Using Hit-and-Miss Transform Extracting a Vector Using Profile Function Enhancing an X-ray image Correcting Non-Uniform Illumination Correcting Shear Effect Correcting Skew Effect  Video  Function_1 Thresholding Following this tutorial, you will learn how to use EasyImage to convert a gray-level source image into a binary destination image. Thresholding an image transforms all the gray pixels into black or white pixels, depending on whether they are below or above a specified threshold. Thresholding an image makes further analysis easier.\n How to use the singal threshold How to use the dual-threshold How to use the Adaptive Threshold Link on line Doc      Function_2 Extracting an Object Contour Following this tutorial, you will learn how to use EasyImage to trace an object outline in a gray-level image. The contour extraction allows you to get in a path vector all the points that constitute an object contour, just by clicking an edge of this object.\n Load the source image Set the destination vector Extract the contour Link on line Doc      Function_3 Detecting the Corners by Using Harris Corner Detector Following this tutorial, you will learn how to use EasyImage to detect the corners of an object. The detection uses the Harris corner detector algorithm.\n Load the source image\n Set the hit-and-miss kernel\n Apply the hit-and-miss transform Link on line Doc\n     Function_4 Detecting a Horizontal or Vertical Line Using Projection Following this tutorial, you will learn how to use EasyImage to detect defects (horizontal/vertical line) in a gray-level image.\n Load the source image\n Set the destination vector\n Detect the defects Link on line Doc\n     Function_5 Computing Gray-Level Statistics Using a Flexible Mask Following this tutorial, you will learn how to compute gray-level statistics on an arbitrary-shaped area only.\n Load the source image\n Invert the image\n Threshold the image\n Save the flexible maskLink on line Doc\n Load the flexible mask image\n Apply the flexible mask on the source image\n Compute the gray-level statisticsLink on line Doc\n     Function_6 Detecting the Corners of an Object Using Hit-and-Miss Transform Following this tutorial, you will learn how to use EasyImage to detect top corners in an image, using the hit-and-miss transform.\n Load the source image\n Set the hit-and-miss kernel\n Apply the hit-and-miss transformLink on line Doc\n     Function_7 Extracting a Vector Using Profile Function Following this tutorial, you will learn how to use EasyImage to detect scratches.\n Load the source image\n Set the destination vector and detecting the scratchesLink on line Doc\n     Enhancing an X-ray image Following this tutorial, you will learn how to use EasyImage to enhance an X-ray image.\n Load the source image\n Set the convolution parameterLink on line Doc\n     Correcting Non-Uniform Illumination Following this tutorial, you will learn how to use EasyImage to correct non-uniform illumination in an image.\n Load the source image\n Load the reference image\n Perform the correctionLink on line Doc\n     Correcting Shear Effect Following this tutorial, you will learn how to use EasyImage to correct a shear effect in an image. The following image is taken by a line-scan camera. The camera sensor was misaligned, resulting in a so-called shear effect.\n Load the source image\n Create a destination image\n Set the pivots parameters Link on line Doc\n     Correcting Skew Effect Following this tutorial, you will learn how to use EasyImage to correct skew effect in an image.\n Load the source image\n Creating a destination image\n Setting the correction angle Link on line Doc\n      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easymatch/",
	"title": "EasyMatch",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyMatch by Euresys\n Learning a Pattern According to an ROI Improving the Score of Matching Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n Learning a Pattern According to an ROI Following this tutorial, you will learn how to use EasyMatch to learn a model from an ROI in a source image, and to perform pattern matching on the same image.\n Load the source image Define an ROI Learn a model from the ROI Match the pattern Link on line Doc      Improving the Score of Matching Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyMatch to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo;. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image.\n Load the source image Learn the reference model Detect instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Define the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detect instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc       "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyobject/",
	"title": "EasyObject",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start EasyObject by Euresys\n Removing Non-Significant Objects After Image Segmentation Detecting Differences Between Images Using Min-Max References Detecting Printing Errors Using a Flexible Mask\n Removing Non-Significant Objects After Image Segmentation Following this tutorial, you will learn how to use EasyObject to detect bad rice grains (largely dark) among many normal rice grains (largely light).\n Load the source image Perform image segmentation Remove the smallest objects Remove the smallest objects Link on line Doc      Detecting Differences Between Images Using Min-Max References Following this tutorial, you will learn how to use EasyObject to compare images. In this example, we will check the quality of a PCB film.\n Load the source image Build min and max reference images Load an image to be inspected Compare the image with the reference images Link on line Doc      Detecting Printing Errors Using a Flexible Mask Following this tutorial, you will learn how to use a flexible mask to target and search specific areas in the image.\n Load the source image Load the flexible mask image Inspect the image Link on line Doc       "
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/bayer2rgb/",
	"title": "Euresys Bayer to RGB by FPGA",
	"tags": [],
	"description": "",
	"content": " Euresys Bayer to RGB Conversion by FPGA Peak Pixel Processing Rate    Firmware Version Applies to Frame-Grabber Cards Peak Pixel Processing Rate     1 i. 3620 Coaxlink Quad CXP-12 JPEG (4-camera) ii. 3620-4 Coaxlink Quad CXP-12 JPEG (4-camera) 500,000,000 pixels/sec   2 i. 1633 Coaxlink Quad G3(2-camera), (2-camera, bayer)\nii. 1633-LH Coaxlink Quad G3 LH(2-camera), (2-camera, bayer) 1,000,000,000 pixels/sec   3 i. 1633 Coaxlink Quad G3(1-camera) ii. 1633-LH Coaxlink Quad G3 LH (1-camera) iii. 1635 Coaxlink Quad G3 DF(1-camera), (1-df-camera) iv. 3602 Coaxlink Octo (2-camera) v. 3622 Coaxlink Duo CXP-12(1-camera) vi. 3622-LH Coaxlink Duo CXP-12 LH (1-camera) 1,108,000,000 pixels/sec   4 i. 3602 Coaxlink Octo (1-camera) ii. 3603 Coaxlink Quad CXP-12 (1-camera) . 3603-4 Coaxlink Quad CXP-12 (1-camera) iii. 3625 Coaxlink QSFP+ (1-camera) 2,216,000,000 pixels/sec    Calculate Maximum Frame Rate Please select the Firmware Version:  .condition { margin-bottom: 20px; display: flex; align-items: center; }\n.condition label { margin-right: 10px; }\n#maximumFrameRateValue { font-weight: bold; }\n.highlight-row { background-color: yellow; } \nFirmware Version:   Please select 1 2 3 4  Camera Resolution (MPixel): MB  Maximum Frame Rate: FPS   function calculateFrameRate() { const firmwareVersion = document.getElementById(\"firmwareVersion\").value; const cameraResolution = document.getElementById(\"cameraResolution\").value; if (firmwareVersion \u0026\u0026 cameraResolution) { const pixelProcessingRate = getPixelProcessingRate(firmwareVersion); const maximumFrameRate = pixelProcessingRate / (cameraResolution*1000000); document.getElementById(\"maximumFrameRateValue\").value = maximumFrameRate.toFixed(2); } else { document.getElementById(\"maximumFrameRateValue\").value = \"\"; } } function getPixelProcessingRate(firmwareVersion) { // Define the pixel processing rates for each firmware version const pixelProcessingRates = { \"1\": 500000000, \"2\": 1000000000, \"3\": 1108000000, \"4\": 2216000000 }; return pixelProcessingRates[firmwareVersion] || 0; } function highlightRow() { // Implement highlighting logic here }  "
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/ffc/",
	"title": "Euresys Grabber Card with FFC Support (FFC)",
	"tags": [],
	"description": "",
	"content": " FPS Calculation with FFC Enabled\nIntroduction to Flat Field Correction (FFC) The Flat-field correction (Wikipedia: FFC) is a method used to correct:\n The differences of light sensitivity between the pixel sensors of a camera some artifacts related to the optical system (e.g., non-uniform lighting and vignetting)  The goal is to correct the pixels of the captured (raw) images in such a way that when a uniform background is captured by the system (camera \u0026amp; lens), the resulting output image is uniform.\nBelow link shows the cards and variants that supports FFC Link\nMaximum image size for all supported pixel foramts when FFC enabled Please verify if the camera output, which is either monochrome or RGB interface, is within the specified limits as shown in the diagram.\nSustainable Relative Data Rate A 4-lane CXP-6 maximum data rate is 2500MB/s\nCalculate For a Quad CXP-12, the sustainable relative data rate 123.2% which is 2500MB x 1.232 = 3080 MB/s If your resolution is 7K x 9K, Then the max fps will be about 3080MB/7K x 9k = 48 fps\n  Frame grabber card None CXP-6 CXP-12   Bit depth None 8位元 10位元 12位元 14位元 16位元   Pixel format (Camera to Grabber)  None Bayer RGB   Image(V):  Image(H):   2500MB  *  Ratio  =  sustainable data rate (MB/s)    Sustainable data rate (MB/s):  /  Image(V):  /  Image(H):  =  FPS    /* CSS樣式部分 */ .condition { display: inline-block; margin-right: 10px; }   // JavaScript部分 function calculate() { var condition1 = document.getElementById(\"condition1\").value; var condition2 = document.getElementById(\"condition2\").value; var dataRate = \"\"; if (condition1 === \"cxp-6\") { if (condition2 === \"8-bit\") { dataRate = \"70%\"; } else if (condition2 === \"10-bit\") { dataRate = \"77.8%\"; } else if (condition2 === \"12-bit\") { dataRate = \"84%\"; } else if (condition2 === \"14-bit\") { dataRate = \"89.1%\"; } else if (condition2 === \"16-bit\") { dataRate = \"93.3%\"; } } else if (condition1 === \"cxp-12\") { if (condition2 === \"8-bit\") { dataRate = \"123.2%\"; } else if (condition2 === \"10-bit\") { dataRate = \"136.9%\"; } else if (condition2 === \"12-bit\") { dataRate = \"147.8%\"; } else if (condition2 === \"14-bit\") { dataRate = \"156.8%\"; } else if (condition2 === \"16-bit\") { dataRate = \"164.3%\"; } } updateDisplayDataRate(dataRate); calculateVariable2(); document.getElementById('variable2_clone').value = document.getElementById('variable2').value; document.getElementById('condition3_clone').value = document.getElementById('condition3').value; document.getElementById('condition4_clone').value = document.getElementById('condition4').value; calculateFPS(); } function updateDisplayDataRate(dataRate) { var displayDataRate = document.getElementById(\"displayDataRate\"); displayDataRate.value = dataRate; } function calculateVariable2() { var displayDataRateStr = document.getElementById(\"displayDataRate\").value; var displayDataRate = parseFloat(displayDataRateStr) / 100; // Convert percentage to decimal var variable2 = 2500 * displayDataRate; document.getElementById(\"variable2\").value = variable2; } function calculateFPS() { var variable2 = parseFloat(document.getElementById('variable2_clone').value); var condition3 = parseFloat(document.getElementById('condition3_clone').value); var condition4 = parseFloat(document.getElementById('condition4_clone').value); var condition5 = document.getElementById('condition5').value; if(condition3 \u0026\u0026 condition4){ // To make sure both condition3 and condition4 are not zero var fps = variable2 * 1000000/ (condition3 * condition4); if (condition5 === \"RGB\") { fps = fps / 3; } document.getElementById('variable3_fps').value = fps; } }  FFC Wizard Sample Program (Quickly start) Euresys provides the source code of a sample application, called ffc-wizard, that computes the coefficients and packs them in a binary file targeting the frame grabber.\nThe purpose of this sample code is threefold:\n guide the user through the calibration procedure; provide a technical and practical translation of what\u0026rsquo;s described in this document; provide building blocks for developing custom applications.  Link\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/dongle/how2activate/",
	"title": "How to activate the license",
	"tags": [],
	"description": "",
	"content": "  How to activate the Neo-license in NEO Dongle How to activate the Open-Evision-license in USB dongle How to activate the Evision-license in USB dongle  How to activate the Neo-license in Neo Dongle Your browser does not support the video tag.  How to activate the Open-Evision-license in USB dongle Your browser does not support the video tag.  How to activate the Evision-license in USB dongle  Evision (Download)  "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/",
	"title": "Neon-2000-JT2",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision NEON-2000-JT2 Discover how to use the NEON-2000-JT2 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/parameters/",
	"title": "Parameters",
	"tags": [],
	"description": "",
	"content": "  Color Lookup  Color Lookup ### IndexBits, the number of table entries and the corresponding table size are given below:\nHere are grouped images of those full RGB palettes by 4 bits \u0026amp; 5 bits \u0026amp; 6 bits.\n 4 bits = 12-bit RGB   5 bits = 15-bit RGB   6 bits = 18-bit RGB  Reference https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats#4-bit_grayscale_2\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/gettingstart/",
	"title": "Getting start",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start with Neon-iSeries from un-boxing.\n Hardware wiring Inference by NEON Run DI/O Sample  Hardware wiring The NEON-2000-JT2 DC power source can be either from a USB Type-C adaptor or DC jack. The USB Type-C connector also supports a DisplayPort video signal and USB3, which can be used to connect a keyboard and mouse. The following figures show examples of possible power and peripheral connection configurations. - Separate Power and Peripheral Connections - This configuration requires an ADLINK AC/DC power adapter (P/N 31-62156-1000-A0). - Combined Power and Peripheral Connections - This configuration requires an ADLINK USB Type-C hub/adapter (P/N 92-99090-1010). Inference by NEON The inference sample, Capture_and_inference_Sample, is built on the Desktop of NEON-2000-JT2.\n inference by image\n ./imagenet-console [input_image][output_image]\ncd ~/Desktop/Neon_Samples/Neon-20xB/C++/Capture_and_Inference/build/aarch64/bin ./imagenet-console ./aarch64/bin/jellyfish.jpg ./aarch64/bin/output.jpg   inference by camera\n ./imagenet-camera [input_width] [input_height]\ncd ~/Desktop/Neon_Samples/Neon-20xB/C++/Capture_and_Inference/build/aarch64/bin ./imagenet-camera 1920 1080    Run DIO Sample  Get hardware and DI/O information\ncd /usr/src/Neon/Sample/Neon_Information sudo ./Neon_Information     Get hardware and DI/O information by python\ncd /usr/src/Neon/Sample/Neon_Python sudo python3 Neon.py     Note Due to dual function of DO0 and DI0, the default function of DO0 is strobe out and DI0 is trigger in. If you want to set them as general DO and DI. Please configure it as following:\nDIO0Config 1: Set DIO0 as GPDI and GPDO0 DIO0Config 0: Set DIO0 as Trigger in and Strobe out (default) sudo /usr/src/Neon/Sample/Neon_Setting/NeonSet DIO0Config 1\n  Check the information of DIO.\nsudo /usr/src/Neon/Sample/Neon_Information/NeonInformation\n- The way that using the DIO functionality of the Neon camera without sudo privileges.\n cd /sys/class/neon_dio sudo chown adlink:adlink DO_0 sudo chown adlink:adlink DO_1 sudo chown adlink:adlink DO_2 sudo chown adlink:adlink DO_3  "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " This page cover the process to flash a new operating system onto a NEON device.\nThere are 2 methods to flash the NEON-2000-JT2 and NEON-2000-JT2-X:\n Flash over USB Clone image to microSD card  Other useful information about flashing a Jetson based device can be found here.\nUSB Method This method uses a host machine to flash the internal eMMC storage over USB.\nTo perform this method the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  Step 1: Download image to your host pc with Ubuntu Neon-2000-JT2  Jetpack 4.2.1 emmc image Jetpack 4.3 v1.0.0 emmc image Jetpack 4.3 v1.0.2 emmc image MD5:e70d52d564e09b11b76fa74314c96c79 Jetpack 4.4 v1.0.3 emmc image MD5:19ee6e9bed2247d5894c3e9066d20b2b Jetpack 4.6.1 v1.0.6 emmc image MD5:31a7ddc595ac588633f1ea64275c33b9 Jetpack 4.6.1 v1.0.7 emmc image(with EVA3.8.5 + EVA IDE) MD5:b3485b4cdcb1aa1de57b27be8efc6146  Neon-2000-JT2-X  Jetpack 4.3 v1.0.2 MD5:ddf504b2c600175eacd61777f528b435 Jetpack 4.4 v1.0.2 MD5:00126a7170cbb82a38707da5ca3e25e3 Jetpack 4.4 v1.0.2 with EVA MD5:18e818021339e3204065d50b54e91dbd Jetpack 4.6.1 v1.0.6(with EVA3.8.5 + EVA IDE) MD5:1b3f971e278371ee3d8daa9b6fccd339  Step 2: Checksum Check md5 checksum to make sure image file is correct\n Linux\n $ md5sum [file]  $ md5sum JT2X_JP461_emmc_v1.0.6.tar.gz   Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile JT2X_JP461_emmc_v1.0.6.tar.gz MD5    Step 3: Flash the image to NEON This step involves connecting the NEON camera to the Host machine and flashing the image.\nThere is also a video and PDF showing the process\n On the Host PC unzip the file downloaded in Step 1\n$ tar -zxvf JT2X_JP461_emmc_v1.0.6.tar.gz  or\n$ tar -jxvf A4_Linux_for_Tegra.20200528_JT2_JP43_v1.0.2.tbz2  Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again  Connect the microUSB cable to the NEON and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd JT2X_JP461_emmc_v1.0.6  Flash the Neon-2000-JT2/Neon-JT2X\nsudo ./flash.sh -r jetson-tx2 mmcblk0p1   To flash the Neon-JT2(jetpack 4.6.1), Neon-JT2X(jetpack 4.6.1), Neon-2000-JNX\nsudo ./flash.sh    Below is a video of the process to flash a NEON-2000-JT2   microSD Card Method This method involves cloning the operating system image for the NEON camera to a microSD, booting the NEON camera from this image and then optionally copying the image from the microSD card to the internal emmc storage on the NEON camera.\nRequired tools:\n microSD of at least 32GB microSD card reader  If you plan on running the operating system from the microSD permanently, make sure to use a high quality microSD card to prevent corruption\n Download image file:  Jetpack 4.4 v1.0.3 microSD image MD5:9ccc55b9dec65b15eefee866e6a1fc85 Jetpack 4.4 v1.0.3 microSD image with EVA_IDE sample  Check the md5 checksum to make sure image file is correct\n Linux\n $ md5sum [file]  $ md5sum JT2_JP44_microSD_v1.0.3.tar.gz   Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile JT2_JP44_microSD_v1.0.3.tar.gz    Unzip the file downloaded in Step 1 to get a .img file\n$ tar -zxvf JT2_JP44_microSD_v1.0.3.tar.gz  Clone image file to microSD card using one of the following methods\n Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card   Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher  Once cloned, check the boot config file is set to boot from the SD card\n Insert SD card into a machine Open file /\u0026lt;path to sd card\u0026gt;/boot/extlinux/extlinux.conf Make sure the boot line is set to /dev/mmcblk2p1\nAPPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait   Insert SD card into the NEON camera\n Boot NEON\n  Optional - Clone image from microSD card to internal eMMC in the NEON:  Mount the internal eMMC  lsblk sudo mkfs -t ext4 /dev/mmcblk0p1 lsblk -f sudo mkdir /media/adlink/ssd sudo mount /dev/mmcblk0p1 /media/adlink/ssd  Clone image from microSD card to internal eMMC using rsync\nsudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd  Change boot config on the internal eMMC to boot from the internal eMMC mmcblk0p1\n Open file /media/adlink/ssd/boot/extlinux/extlinux.conf Replace the following\n# Replace this line # APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait # With this line APPEND ${cbootargs} quiet   Remove the SD card and reboot the NEON\n  "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/howtobackupneonibymicrosd/",
	"title": "How to backup Neon by microSD card?",
	"tags": [],
	"description": "",
	"content": "  Clone Neon data to microSD card Backup microSD card as image Recovery microSD card by image  According to the official document, the boot sequence is decided by U-boot. U-Boot functionality includes a default booting scan sequence. It scans bootable devices in the following order: - External SD card - Internal eMMC (Jetson TX2 series devices only) - USB device (Jetson TX2 series devices only) - NFS device\nU-Boot boots up the kernel by /boot/extlinux/extlinux.conf in the sequence of bootable device. That is to say, if NEON-2000-JT2 equits microSD card with OS, U-Boot boots up accord to /boot/extlinux/extlinux.conf in microSD card instead of extlinux.conf in eMMC.\nRequirements:  32G up microSD card NEON-2000-JT2  Clone Neon data to microSD card Step 1: Format your microSD card as ext4 The total size of image with Jetpack4.4 is 16G. It suggests cloning the image using a 32G microSD card.\n Insert microSD card in Neon  Right click on microSD card folder and click Format\u0026hellip;  Fill Volume Name:JP44 for example. Choose Type as Internal disk for use with Linux systems only (Ext4)  Confirm details and click Format  Search applications: Disks  Select SD Card Reader -\u0026gt; Mount selected partition   Step 2: Clone eMMC data to microSD  Make sure NEON-2000-JT2 mounts your microSD card /dev/mmcblk2p1.\ndf -h  Clone eMMC data to microSD\nsudo cp -ax / '/media/adlink/yourSDcard' \u0026amp;\u0026amp; sync   Step 3: Modify extlinux.conf in microSD card for bootup sequence  Edit /boot/extlinux/extlinux.conf in microSD card\n APPEND ${cbootargs} quiet\nAPPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait\n  Step 4: Reboot NEON-2000-JT2  It will boot from microSD card.  Reference\nOptional Backup microSD card as image ### Step 1. Use command and check the disk of microSD in PC\n df -h  ### Step 2. Image a card with dd\nif=path_of_your_image.img of=/dev/disk\n sudo dd bs=4M if=/dev/sdd status=progress | zip Neon-2000-JT2-JP43.zip -  Recovery microSD card by image  image for microSD, Jetpack 4.3 v1.0.2 MD5: ae005ea9624999fb0922c487322680d1s  ### Step 1. Unzip the image\n unzip Neon-2000-JT2-JP43.zip  ### Step 2. Rename it\n mv - unzip Neon-2000-JT2-JP43.image  ### Step 3. Insert microSD and flash it by Etcher\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/",
	"title": "02-Video Source",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory Video Source There are some know how for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/",
	"title": "03-Mouse",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory Mouse There are several note for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/",
	"title": "04-Dex-pro",
	"tags": [],
	"description": "",
	"content": " How to add the bmp(layer) and select the folder for project. How to use classic functions in the script.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/faq/",
	"title": "10-FAQ",
	"tags": [],
	"description": "",
	"content": " FAQ List  Specifications of DI1 DI2 LED Batch kill dex-pro The mouse follows offset How to restart the MSMQ How to check why the dexpro can\u0026rsquo;t grab the video How to check the exception-event How to backup or restore the traineddata to other machines How to quckly select TesseractDB for mutil-ROI  Specifications of DI1 DI2 Hardware Figure DI connector pin assignment DI1/2 function LED There are 3 LEDs in the hardware that can light by script command. kill the dex-pro by batch  Please run the cmd as administrator\n input the command taskkill /f /im DEX-Pro.exe\n The mouse follows offset  Check the machine PC’s mouse setting, Keep middle speed and disable “Enhance pointer precision”. If the offset distance is not fixed, adjust mouse speed of the machine PC   How to restart the MSMQ If the Dex-por can\u0026rsquo;t work by MSMQ, Please follow the below steps for Troubleshooting\n1. Go to \u0026ldquo;control panel\u0026rdquo; 2. Go to \u0026ldquo;Program\u0026rdquo; 3. Go to \u0026ldquo;Turn Windows Features on or off\u0026rdquo; 4. Disable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo; 5. Reboot the DEX-100 6. Enable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo; How to check the setting when the dexpro can not grab the video If the Dex-pro can\u0026rsquo;t grab the video, We have several suggestions please follow checking the below steps. 1. Please checking the setting is Auto by the dex-pro tool of Fram Grabber Configuration. 2. Please open the Diagnostic tool.  Tool-Path \u0026quot;C:\\Program Files\\ADLINK\\DEX-100\\utility\u0026quot;   You shall see both the result for Resolution and Signal if the connection is successful.  If is no signal , please use below steps to help you troubleshooting. A. Check the Local machine setting for display resolution. B. Reconnect the local machine VGA cable to monitor. C. Reboot the local machine. D. Check the resolution from the monitor. E. Reconnect the VGA to Dex-100 F. Check Frame Grabber Setting whether the auto mode is selected for any resolution.  How to check the exception-event You can track the Event Viewer if dex-pro encounters the Unexpected crashing.\n 1. Enter CMD (Command Prompt) 2. Key-in \u0026quot;eventvwr\u0026quot;   Path : \u0026quot;Event Viewer ==\u0026gt; Windows Logs ==\u0026gt; System\u0026quot;  How to backup or restore the traineddata to other machines  A. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management   B. Bakcup \u0026gt; - Select all \u0026gt; - Export    Click Yes Select the folder ==\u0026gt; OK    C. Restore \u0026gt; - Select all \u0026gt; - import (select the Folder name)  \u0026gt; - Select the folder ==\u0026gt; Ok\nHow to quckly select TesseractDB for mutil-ROI  A. In the blf mode   B. Select an image layer that has many ROI for OCR. C. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management   D. Can be selected in batches and edit the ROI setting.   E. Click the Replace   F. Exit without change   G. Save Script   H. Recombine the script  "
},
{
	"uri": "https://aiot-ist.github.io/dex/sdkfwversion/",
	"title": "11-SDK &amp; FW Version &amp; Manual",
	"tags": [],
	"description": "",
	"content": " SDK Internal version : 1.2.7 2022/03/28\nOffical version : V7\n SDK V7 (Download)\n Release note\n Autoinstall Download\n   Pls unzip and double-click this .bat file.\n ### How to install SOP - #### 1. Uninstall - #### 2. Reboot - #### 3. Install SDK - #### 4. Reboot\nFW  FW-C5 (Download) FW-C7 (Download)\n  Vision_history  ## How to intsll SOP\n Use this tool C:\\Program Files\\ADLINK\\DEX-100\\utility\\BurnDex100_x64   Check the version that is C5 or C7\n Upload the right bit\n  Note: The FPGA shall be null when you install the error bit.\n  Cold boot  Utility_Manual Utility_Manual_20210426 (Download)\nModules\u0026amp;EdgeServe DEX-100_SW_User_Manual (Download)\nHardware Hardware-Manual(Download)\nCE DEX-100 CE EMC TEST REPORT(Download)\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/",
	"title": "Vision Card",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory ADLINK Vision Card Find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to getting start with MCM-204 from un-boxing\nStep 1: Connect Power. Connect the positive and negative wires from a 9V to 30V DC power source to the terminal block. Step 2: Connect MCM-204 to PC or laptop. Connect MCM-204 to the PC or laptop by ethernet cable. Please make sure the PC/laptop network mode is under DHCP or static IP set at 169.254.x.x network segment.\n Step 3: Open the web browser to access the build-in web console. Open the web browser (chrome is recommanded) to access with the default IP http://169.254.1.1\nOnce acess MCM-204 web console sucessfully, the page will displayed like below. The default username is administrator and password is Adlink6166, after fill in these required information then click LOGIN to login to the web console. Step 4: Device Setting Click menu Device Setting to enter the device setting page. You could control the analog input setting and customize what kind of data you want to acquire. You could scroll the page to the bottom and simply click Apply to apply the default setting at this tutorial.\nAfter done, there will pop up a successful message to notify.\nStep 5: Data Capture Click menu Data Capture to enter the data display page and all the data set at last step will be displayed at this page.\nIf the data include the Voltage data type, you could click Draw to draw the voltage chart. "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/",
	"title": "Neon-2000-JNX",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision Neon-2000-JNX Discover how to use the NEON-2000-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithm/",
	"title": "Custom Algorithm Deploy Sample",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to build your own algorithm and deploy to MCM-204 You could deploy your own algorithm at MCM-204.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date  Step 1: Download ARM gcc tool chain and examples. Please click below link to download zip file compressed of ARM gcc tool chain and examples at Ubuntu.\nDownload link: ARM tool chain with examples\nAfter downloaded, please extract it, then there will be a ARM tool chain folder and a sample folder called CalRMS.\nStep 2: Compile sample. Extract the downloaded file at Step 1.\n$ unzip CustomAlgorithm.zip  Intall make, it\u0026rsquo;s not installed by default.\n$ sudo apt install make  Change directory to the CalRMS\n$ cd CalRMS  Build .so file from sample.\n$ make  The customAlgo.so is build at current directory, you could use this file to upload to MCM-204.\n$ ls customAlgo.c customAlgo.h customAlgo.o customAlgo.so Makefile  Step 3: Deploy the .so file to MCM-204 Once you completed above steps, you will get customAlgo.so. What we need to do next is deploy it to the MCM-204. Log in to MCM-204 web portal, if you don\u0026rsquo;t know how to login to web portal, you could click thie link to getting start page.\nAfter you login to web portal, then click System Setting menu. Scroll down to the bottom of page, then find the Customization library Upload section then click Choose File.\nAfter choose the customAlgo.so, simply click UPLOAD to upload it to the MCM-204 The upload process will finished at about 30 seconds, there will pop up a re login dialog until done. Simply click re-login link to re login to web portal. Step 4 Apply custom algorithm to device setting. Click Device Setting at MCM-204 web portal, then scroll down to Channel Config section At this tutorial we use the default setting of AI0, simply click ADD DATATYPE, then choose Data Type to Customization and input rms at Customization Parameter. Scroll down to the bottom of page then click Apply\n)\nStep 5 Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. You could enter to next tutorial to learn how to develope your own algorithm.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithmdev/",
	"title": "Custom Algorithm Development",
	"tags": [],
	"description": "",
	"content": " This tutorial will guide you how to develop your own algorithm In the previous tutorial, we can get the RMS or Mean value through the sample code CustomAlgorithm.zip. This tutorial will modify the sample code to get the RMS and Mean value at the same time.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date  Step 1: Change the output data from 1 to 2. Find the following code from customAlgo.c.\ndouble* CustomAlgo(uint16_t chIndex, void *rawData, DeviceInfo devInfo, char* customParams, uint32_t* outCount) { uint32_t* raw = (uint32_t*)rawData; double inputRange = devInfo.inputRange == B10 ? 10.0 : 1.25; double scalingFactor = inputRange/8388607.0*1000.0/devInfo.sensor.sensitivity;//for convert rawData to g const uint32_t COUNT = 1; double* data = (double*)malloc(sizeof(double) * COUNT); double* gArray = (double*)malloc(sizeof(double) * devInfo.dataCount); raw = raw + chIndex*devInfo.dataCount; *outCount = COUNT; for(int i = 0; i \u0026lt; devInfo.dataCount; i++) gArray[i] = scalingFactor*(((raw[i] \u0026amp; 0x00800000) == 0x00800000) ? (int32_t)(raw[i] | 0xFF000000) : (int32_t)raw[i]); if(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) data[0] = GetRms(gArray, devInfo.dataCount); else data[0] = GetMean(gArray, devInfo.dataCount); free(gArray); return data; }  Modify COUNT to 2 in line 34.\nconst uint32_t COUNT = 2;  Step 2: Change the output data. Modify conditions, then save and deploy the code.\nif(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) { data[0] = GetRms(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;mean\u0026quot;)==0) { data[0] = GetMean(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;both\u0026quot;)==0) { data[0]=GetRms(gArray,devInfo.dataCount); data[1]=GetMean(gArray,devInfo.dataCount); }  Step 3: Apply custom algorithm to device setting. Please refer to the previous tutorial.\nStep 4: Modify Customization Parameter. Enter both in customization parameter. Step 5: Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. In this way you can get the RMS and the Mean value at the same time.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " Frequently asked questions (FAQ) about MCM-204\nGeneral  #### What is the default IP address? The default IP is the fixed link local IP address 169.254.1.1 #### How to connect by hostname? The connection via hostname relies on DNS service, please make sure the DNS service is functional.  Programming  #### What kinds of API supported?  ##### RESTFul API : You could get most everything via this. ##### C/C++ API : Used for getting rawdata more effiency.   "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " This page cover the process to flash a new operating system onto a NEON-2000-JNX and EOS-JNX device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a JNX device you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu If upgrading the Jetpack version make sure to also download the eMMC image. This is because the internal eMMC and microSD card must be flashed with same version of Jetpack. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nNeon-2000-JNX  microSD image\n Jetpack 5.1.2 v1.0.9 microSD image (with pylon5.2 + without EVA) MD5:81992a6ada7f01ee30206680bdc4e878 Jetpack 5.0.2 v1.0.8 microSD image (with pylon7.2.1 + EVA4.0.2) MD5:79d87d4dc0ef2440845dccb1d0a33d4b Jetpack 5.0.2 v1.0.7 microSD image (with pylon5.2) MD5:c60395f01cd76131c25f3bf96ed53543 Jetpack 4.6.1 v1.0.5 microSD image MD5:564951bdd6074db6e853037b1338e905 Jetpack 4.6.1 v1.0.6 microSD image (with EVA 3.8.3) MD5:8b4e3d4ce7bf7e69bc90b04e98e14851 Jetpack 4.5 v1.0.3 microSD image MD5:243b98f10a873f2830e4f635eab7c80d Jetpack 4.5 v1.0.4 microSD image (with EVA 3.5) MD5:43b5931b625e3b598423e0bb4a131d6e   eMMC image - required if changing Jetpack version\n Jetpack 5.1.2 emmc image MD5:dc01fad05c3ca7df2b5096bc27877b95 Jetpack 5.0.2 emmc image MD5:73758afcdf0119a98b27c9d1630add6a Jetpack 4.6.1 emmc image MD5:f08d89a8aa4927b7df7af449c2f8edd9 Jetpack 4.5 emmc image MD5:e603db76e8ab1bbe5596760d40adb90c   Checksum  Check the md5 checksum to make sure image file is correct\n Linux $ md5sum [file]  $ md5sum NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz  Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz MD5    Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\n microSD of at least 32GB microSD card reader  Make sure to use a high quality microSD card to prevent corruption\n Unzip the microSD zip file downloaded in Step 1 to get a .img file\ntar -zxvf NeonJNX_A3_JP512_microSD_v1.0.9_woEVA.img.tar.gz  Clone image file to microSD card using one of the following methods\n Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card   Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher  Insert SD card into the NEON camera\n  Step 3: Flash eMMC image The internal eMMC must be flashed if the Jetpack version on the microSD card has changed, so the Jetpack version on the eMMC and microSD card match.\nThis step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  There is also a video and PDF showing the process\n On the Host PC unzip the file downloaded in Step 1\ntar -zxvf NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz  Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again  Connect the microUSB cable to the NEON and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd NeonJNX_A3_JP512_emmc_v1.0.9  Flash the Neon-2000-JNX\n For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass  Jetpack before 5.0.2\nsudo ./flash.sh  For Jetpack 5.0.2 ~ 5.1.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 1   Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\n  Below is a video of the process to flash a NEON-2000-JNX   Appendix: How to restore eMMC image  method 1\n step1: download environment on Linux x86 ubuntu PC  https://sftp.adlinktech.com/image/Neon-JNX/deploy.neonjnx.tar.gz md5: d9e7e4ff4030f7d81f72c353aad6d272  step2: untar the file  tar -zxvf deploy.neonjnx.tar.gz  step3: execute command  sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-neonjnx-emmc mmcblk0p1 Note: Booard name of different hardware version  HW A3: jetson-neonjnx-a3-emmc HW A2: jetson-xavier-nx-neonjnx-emmc    method 2\n Backup emmc   \ncd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp cd mntTemp sudo tar -jcf ../customerEMMC.tbz2 * sync cd ../ sudo umount mntTemp rmdir mntTemp - Restore emmc \u0026lt;div\u0026gt; \u0026lt;iframe src=\u0026quot;https://sftp.adlinktech.com/image/EOS-JNX/sop/Restore_emmc.mp4\u0026quot; width=\u0026quot;640\u0026quot; height=\u0026quot;385\u0026quot; scrolling=\u0026quot;no\u0026quot; framespacing=\u0026quot;0\u0026quot; webkitallowfullscreen mozallowfullscreen allowfullscreen\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt;  cd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp sudo tar jxf customerEMMC.tbz2 -C mntTemp sync sudo umount mntTemp rmdir mntTemp\n   "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/demo/",
	"title": "People count application With Deepstream SDK and Transfer Learning Toolkit",
	"tags": [],
	"description": "",
	"content": " People count application With Deepstream SDK and Transfer Learning Toolkit\nGit repo: https://github.com/AIoT-IST/deepstream-occupancy-analytics/tree/master\n Description Prerequisites Getting Started Build Run Output References   Description This is a sample application for counting people entering/leaving in a building using NVIDIA Deepstream SDK, Transfer Learning Toolkit (TLT) and pre-trained models. This application can be used to build real-time occupancy analytics application for smart buildings, hospitals, retail, etc. The application is based on deepstream-test5 sample application.\nIt can take streaming video or Neon camera as input, counts the number of people crossing a tripwire. In this application, you will learn:\n How to use PeopleNet model from NGC How to use NvDsAnalytics plugin to draw line and count people crossing the line  You can extend this application to change region of interest, use cloud-to-edge messaging to trigger record in the DeepStream application or build analytic dashboard or database to store the metadata.\nTo learn how to build this demo step-by-step, check out the on-demand webinar on Creating Intelligent places using DeepStream SDK.\nPrerequisites  Neon-201A-JNX or Neon-202A-JNX with jetpack 5.1.2\n Download PeopleNet model: [https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet/files]\n This application is based on deepstream-test5 application. More about test5 application: [https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_ref_app_test5.html]\n  Getting Started  Preferably clone the repo in /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y sudo apt-get install libjson-glib-dev libgstrtspserver-1.0-dev -y sudo chmod 777 -R /opt/nvidia/deepstream/deepstream-6.3/ git clone https://github.com/AIoT-IST/deepstream-occupancy-analytics.git /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/  Download peoplnet model:\ncd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/config \u0026amp;\u0026amp; ./model.sh  For Jetson use: bin/jetson/libnvds_msgconv.so\n  Build and Configure  Set CUDA_VER in the MakeFile as per platform.  For Jetson, CUDA_VER=11.4\n cd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics \u0026amp;\u0026amp; make  Run  cd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/ ./deepstream-test5-analytics -c config/dstest_occupancy_analytics.txt  Modify the boarder  cd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/tool python3 preview.py  Output The output will look like this:\nWhere you can see the kafka messages for entry and exit count.\nReferences  CREATE INTELLIGENT PLACES USING NVIDIA PRE-TRAINED VISION MODELS AND DEEPSTREAM SDK: [https://info.nvidia.com/iva-occupancy-webinar-reg-page.html?ondemandrgt=yes] Deepstream SDK: [https://developer.nvidia.com/deepstream-sdk] Deepstream Quick Start Guide: [https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html#page/DeepStream_Development_Guide/deepstream_quick_start.html#] Transfer Learning Toolkit: [https://developer.nvidia.com/transfer-learning-toolkit] forked from https://github.com/NVIDIA-AI-IOT/deepstream-occupancy-analytics  "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-ono/",
	"title": "Neon-2000-ONO",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision Neon-2000-ONO Discover how to use the NEON-2000-ONO and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-ono/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " Step 1: Download image to your host pc with Ubuntu Neon-2000-ONO  eMMC image - required if changing Jetpack version  Jetpack 5.1.3 emmc image MD5:1863fa4d85d1624f9f2901886ee19d60   Checksum  Check the md5 checksum to make sure image file is correct\n Linux $ md5sum [file]  $ md5sum mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2  Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2 MD5    Step 2: Flash eMMC image This step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  There is also a video and PDF showing the process\n On the Host PC unzip the file downloaded in Step 1\nsudo tar -I lbzip2 -xf mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2  Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again  Connect the microUSB cable to the NEON and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0  Flash the Neon-2000-ONO\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 15 --network usb0  Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\n  Below is a video of the process to flash a NEON   "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-onx/",
	"title": "Neon-2000-ONX",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision Neon-2000-ONX Discover how to use the NEON-2000-ONX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-onx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " Step 1: Download image to your host pc with Ubuntu Neon-2000-ONX  eMMC image - required if changing Jetpack version  Jetpack 5.1.3 emmc image MD5:8af092371359db985ce30da934a9eed7   Checksum  Check the md5 checksum to make sure image file is correct\n Linux $ md5sum [file]  $ md5sum mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2  Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2 MD5    Step 2: Flash eMMC image This step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  There is also a video and PDF showing the process\n On the Host PC unzip the file downloaded in Step 1\nsudo tar -I lbzip2 -xf mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2  Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again  Connect the microUSB cable to the NEON and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0  Flash the Neon-2000-ONO\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 15 --network usb0  Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\n  Below is a video of the process to flash a NEON   "
},
{
	"uri": "https://aiot-ist.github.io/dex/",
	"title": "DEX-100",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory DEX-100 Discover how to use the DEX series and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/",
	"title": "DEX-100 with KVM",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory DEX-100 with KVM package solution Tutor you on how to use KVM with DEX-100.\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/1_quick_start/",
	"title": "1 Quick start",
	"tags": [],
	"description": "",
	"content": " EVA SDK is edge computing visual analysis software supporting single source AI machine vision project deployment across different hardware platforms. EVA SDK IDE provides a visual AI machine vision deployment interface to quickly verify AI ​​Inferences. Watch the video to learn more about the intuitive GUI for fast AI inference validation. EVA IDE is built into ADLINK\u0026rsquo;s NEON series AI smart camera and EOS series AI vision system to help with AI machine vision project development.\nEVA IDE introduction   Chinese EVA Hands-on webinal   Chinese EVA manual Download link\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/2_showroom/",
	"title": "2 EVA Showroom",
	"tags": [],
	"description": "",
	"content": " Apply Now to experience the EVA Online Showroom Apply Link Having trouble choosing a topic for your factory AI application? EVA Online Showroom helps you with quick topic selection, online evaluation, and the ability to immediately launch AI vision applications.tely launch AI vision applications EVA Online Showroom is a utility that allows you to experience how EVA SDK simplifies AI Vision deployment at the edge, enabling you to deliver a successful proof of concept within two weeks. Through this digital Online Showroom, you will experience the performance and effects of AI Inference on different ADLINK AI machine vision hardware devices in a variety of selected application scenarios in order to help you choose the most appropriate AI application topics.\nEVA Online Showroom Experience steps: "
},
{
	"uri": "https://aiot-ist.github.io/eva/3_showcase/",
	"title": "3 EVA Cases",
	"tags": [],
	"description": "",
	"content": " Welcome to ADLINK EVA Showcase! In this git repository, you can download the source code of the plug-ins we designed for the showcases. Each showcase demonstrates the pipeline elements you will need. EVASDK can help you to quickly set up and build the applications for your specific implementation.\nRepo Link Showcases currently available:    Case Description Categories     Case 1 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area. Factories commonly have restricted areas to keep workers safe from moving automation or robots. In this showcase you will see how a corridor is designated as a restricted area to prevent people from entering it. If someone enters the restricted area, the region will turn red if the display is set to true, and an alert event will be recorded as metadata for a downstream element to activate. Geofencing   Case 2 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area and checking if the person is wearing a clean room suit. For certain manufacturing processes, a clean room suit is required when entering an environmentally controlled space. In this showcase, geofencing is also used to detect when someone crosses the entrance. The downstream element for clean room suit detection will then be activated to detect if the person is wearing the appropriate clothes before entering the FAB. Geofencing/Wear Detection   Case 3 In this showcase you will see how EVA is used to prepare the pipeline for detecting abnormal alignment of cookies on the production line. We can use Custom Vision to test the recognition effect of AI. In this demonstration, Custom Vision can be exported into a universal ONNX model, and the EVA can be used to quickly verify the video or the actual shooting results. Product Inspection   Case 4 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the parts preparation and its order in the parts container. This monitoring is one of the assemble standard operation to prevent missing to assemble parts like screws in the next assemble steps. In additional to this purpose, the preparation time is also the critical fact for the analyst for formulating the optimal assembly standard. Collecting this information extremely help to reflect the true preparation time instead of coming from few simulating samples. Product Inspection/Preparation   Case 5 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the product assembly based on the standard operation procedure. Each parts is required assembled orderly in time. Otherwise the whole procedure will exceed the standard assemble time designed. Each part will calculate its own consuming time and then summed for the analyst to dynamically adjust or designed the whole product assembly standard operation procedure. Ultimately, the convenient combination with showcase 4 for illustrating the two SOP algorithm plug and play in EVA. Product Inspection/Assembly    "
},
{
	"uri": "https://aiot-ist.github.io/eva/4_template/",
	"title": "4 Template",
	"tags": [],
	"description": "",
	"content": " Here\u0026rsquo;s common command line for each case.\nGet camera/video Source  video\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! xvimagesink sync=true Basler\ngst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! videoconvert ! xvimagesink sync=false Appropho gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false\nPre-processing / Post-process  AI inferece  Classification  googlenet gst-launch-1.0 pylonsrc camera=0 fps=20 ! videoconvert ! adrt model=/home/adlink/Downloads/model/googlenet.engine batch=1 ! adtrans_classifier label=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/googlenet-v2_RT_labels.txt\u0026quot; ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true  Object detection\n yolov3-tiny\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! adrt rgbconv=true model=yolov3-tiny-288-1012.engine device=0 scale=0.0039 mean=\u0026quot;0 0 0\u0026quot; batch=1 ! adtrans_yolotrt label-file=label.txt blob-size=\u0026quot;9,18\u0026quot; mask=\u0026quot;(3,4,5),(0,1,2)\u0026quot; anchor=\u0026quot;(10,14),(23,27),(37,58),(81,82),(135,169),(344,319)\u0026quot; input_width=288 class-num=80 input_height=288 threshold=0.4 ! admetadrawer ! videoconvert ! xvimagesink sync=true yolov3\ngst-launch-1.0 pylonsrc camera=0 fps=6 ! videoconvert ! adrt model=yolov3-416_test.engine scale=0.004 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_yolo label=label.txt class-num=2 ! admetadrawer ! videoconvert ! xvimagesink sync=false ssd inception\ngst-launch-1.0 pylonsrc camera=0 fps=15 ! videoscale ! video/x-raw, width=800, height=600 ! videoconvert ! adrt model=/home/adlink/Downloads/model/ssdv2.engine batch=1 device=0 scale=0.0078 mean=\u0026quot;0 0 0\u0026quot; norm=false ! adtrans_ssd label=/home/adlink/Downloads/model/ssd_coco_labels.txt ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true  Pose detection\ngst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! bayer2rgb ! videoconvert ! adrt model=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/pose-b1.engine\u0026quot; scale=0.0039 rgbconv=true ! adtrans_openpose_py ! admetadrawer ! videoconvert ! xvimagesink sync=false Segmentation\n fcn\ngst-launch-1.0 videotestsrc ! adrt model=road.engine scale=1.0 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_segment class-num=4 blob-height=512 blob-width=896 ! xvimagesink sync=false   RTSP Launch RTSP server before every operation  $ /opt/adlink/eva/bin/rtsp-simple-server   Video file streaming out\ngst-launch-1.0 videotestsrc ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test Appropho camera streaming out\ngst-launch-1.0 v4l2src io-mode=4 ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test gst-launch-1.0 v4l2src io-mode=0 ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h265enc bitrate=4000000 ! rtspclientsink location=rtsp://localhost:8554/test Receive RTSP streaming\ngst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test ! rtph265depay ! h264parse ! avdec_h264 ! xvimagesink gst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test user-id=admin user-pw=admin ! rtph264depay ! h264parse ! nvv4l2decoder ! nvvideoconvert ! \u0026quot;video/x-raw(memory:NVMM),format=RGBA\u0026quot; ! nvegltransform ! nveglglessink sync=false\nSave file Save as mkv file\ngst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv\n Preview\ngst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false\n Preview \u0026amp; save\ngst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! videoconvert ! tee name=t ! queue ! 'video/x-raw, format=(string)UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv sync=false t. ! queue ! xvimagesink sync=false\n Inference and save video gst-launch-1.0 filesrc location=pallet.mp4 ! decodebin ! nvvidconv ! videoconvert ! adrt model=pallet_yolov4_416.engine scale=0.004 mean=\u0026quot;0 0 0\u0026quot; rgbconv=True ! adtrans_yolo class-num=1 anchor=anchor=\u0026quot;(12,16),(19,36),(40,28),(36,75),(76,55),(72,146),(142,110),(192,243),(459,401)\u0026quot; threshold=0.3 label=pallet_yolov4_v1.names use-sigmoid=True ! admetadrawer ! videoconvert ! omxh264enc bitrate=4000000 ! h264parse ! qtmux ! filesink location=filename.h264\n  "
},
{
	"uri": "https://aiot-ist.github.io/eva/5_qa/",
	"title": "5 Q&amp;A",
	"tags": [],
	"description": "",
	"content": " Installation path of EVA SDK  /opt/adlink/eva/  How to get inference result from pipeline  through plugin refer to /opt/adlink/eva/samples/plugin_sample.py through pipeline refer to /opt/adlink/eva/samples/pipeline_app.py  How to clean gstreamer cache  rm -rf /home/adlink/.cache/gstreamer-1.0   "
},
{
	"uri": "https://aiot-ist.github.io/eva/6_performance/",
	"title": "6 Measure performance",
	"tags": [],
	"description": "",
	"content": " Ways to calculate execute time of each element in EVA pipeline.\nUsing debug mode in EVA IDE and see the exec time of each element. Third party tool gst-shark\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/",
	"title": "EVA",
	"tags": [],
	"description": "",
	"content": " EVA (Edge Vision Analytics) EVA SDK is a unified edge vision analytics service-ready software platform that enables ADLINK AI hardware, making it easier for users to develop optimized edge AI vision applications by simplifying integration and focusing on essential functionality. Users can leverage readyto- use open-source plugins to facilitate each stage the AI vision project lifecycle, including image capture and processing, AI inference, postprocessing, and analytics. This \u0026ldquo;One API\u0026rdquo; framework allows users successfully to build a proof-of-concept in two weeks and speed up mass deployment time.\nIntuitive GUI for Fast and Easy AI Inference Pipeline Development Key functions Support hardwares  AI Smart Camera\n NEON-2000-JNX   AI Vision System\n EOS-i6000-P Series  EOS-JNX-I / EOS-JNX-G    Contact us IST_EVA_Support@adlinktech.com\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/",
	"title": "Neon-3000-RB5",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision Neon-3000-RB5 Discover how to use the Neon-3000-RB5 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": "  Introduction User Interface UnBoxing  Introduction ADLINK Launches Image Sensor Integrated NVIDIA Jetson Nano AI Camera Dev Kit for Easy, Rapid AI Vision Prototyping Summary:  A vision development kit with the NVIDIA® Jetson Nano module and image sensor, industrial DI/O, issue-free integration with affordable price Get started quickly with add-on AI vision applications and sample code for rapid AI vision prototyping Easy leverage to vast NVIDIA, OpenCV and G-streamer resources through V4L2 video interface ADLINK exclusive EVA provides selected AI models, tools of training \u0026amp; labeling and SOP, easy to build a PoC without issues. High extensibility: seamlessly transfer the AI Camera Dev Kit to ADLINK industrial vision devices and systems  ADLINK Technology Inc., a global leader in edge computing, launched its NVIDIA Jetson Nano based vision developer kit — AI Camera Dev Kit. ADLINK is applying its 25-year machine vision design experience in an effort to meet the needs of AI vision developers for rapid concept validation and viability testing and has launched a pocket-sized developer kit with image sensor, lens, vertical I/O, varied peripherals, and exclusive ADLINK edge vision analytics software – EVA. Accordingly, the AI Camera Dev Kit delivers low cost, simple integration, fast development, and easy access to open resources and high extensibility to accelerate AI vision prototyping while seamlessly enabling users to convert the kit to ADLINK edge AI vision devices.\nADLINK’s AI Camera Dev Kit includes an 8MP color MIPI camera module; NVIDIA® Jetson Nano™ SOC; LAN; industrial DI/O interface; USB Type-C hub for easy powering, display, keyboard/mouse connection and data transmission; SD memory card with Linux OS; NVIDIA JetPack SDK; OpenCV and ADLINK’s exclusive EVA software—everything you need to start AI Vision prototyping.\nTo help streamline software development, the kit includes two pre-installed ready-to-use AI vision applications, Pose Detection and Object Classification, with open-source training, a labeling tool and SOP that are easy to learn and modify. These two applications can be applied on the scenarios like the Robot Hazard Prevention and Fab Clear SOP or similar applications such as virtual fences or other SOP compliance. ADLINK EVA is our exclusive no-code/low-code GUI-based edge vision analytics software. The intuitive GUI pipeline development environment provides fast and flexible image capturing and AI inference to logical judgement configuration. Follow the EVA SOP, users can train their own AI model on the powerful computers and migrate it to the AI Camera Dev Kit. The ADLINK EVA Portal also provides tutorials, application templates and resources for effortless PoC development.\nToday’s market trend is to start project development from a developer kit that can quickly help developers test and demonstrate their concept. Targeted for the booming AI vision market, ADLINK provides an image sensor integrated AI camera developer kit based on our popular NEON series AI smart camera. The AI Camera Dev Kit eliminates tedious integration effort with its built-in no-code GUI software (EVA) and two AI applications with sample code. This kit enables users to start their AI vision PoC in just five minutes,” said Kevin Hsu, ADLINK´s senior product manager for edge vision products. “Based on the V4L2 interface, users can easily access all NVIDIA Tao and NGC catalog resources that they are familiar with. The AI camera dev kit leverage the experience and advantage of our most popular product NEON series AI Smart Camera with affordable price, is ideal for development of a wide range of edge vision and AI.\nThe ADLINK AI Camera Dev Kit can accelerate AI vision prototyping and seamlessly enable users to convert the kit to ADLINK edge AI vision devices. The AI Camera Dev Kit is available now. Follow the links for further information about ADLINK’s AI Camera Dev Kit and EVA software.\nUser Interface Front view Back view Purchase optional accessories for your custom use case: To extend monitor, keyboard/mouse and power supply capabilities To connect to digital I/O devices: How to wiring D-sub I/O Connector Unboxing Following the video to unbox your AI camera dev kit. Learn to impletement AI application: Geo-fence and SOP compliance. Note: Open English Caption if needed.   Please refer to What is Adlink AI Suite? to know more info about the applications.\nOptional Accessories:\n① USB Type-C hub/adapter/30cm USB Type-C cable (92-99090-1010)\n② 1.8m USB Type-C cable with screw lock (30-01284-0030-A0)\n③ I/O cable with DB-15 connector (30-21621-0000-A0)\n④ 3m DB-15 to DB-37 I/O extension cable (30-01332-0010-A0)\n⑤ DIN-37D-01 IO extension board (91-14025-1020)\n⑥ AI camera kit tripod bracket (91-95340-000E)\n⑦ AI camera kit acrylic plate (91-95341-000E) This page cover the process to flash a new operating system onto a NEON-2000-JNX and EOS-JNX device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a JNX device you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu If upgrading the Jetpack version make sure to also download the eMMC image. This is because the internal eMMC and microSD card must be flashed with same version of Jetpack. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nNeon-2000-JNX  microSD image\n Jetpack 5.0.2 v1.0.8 microSD image (with pylon6.2.1 + EVA4.0.2) MD5:79d87d4dc0ef2440845dccb1d0a33d4b Jetpack 5.0.2 v1.0.7 microSD image (with pylon5.2) MD5:c60395f01cd76131c25f3bf96ed53543 Jetpack 4.6.1 v1.0.5 microSD image Jetpack 4.6.1 v1.0.6 microSD image (with EVA 3.8.3) MD5:8b4e3d4ce7bf7e69bc90b04e98e14851 Jetpack 4.5 v1.0.3 microSD image MD5:243b98f10a873f2830e4f635eab7c80d Jetpack 4.5 v1.0.4 microSD image (with EVA 3.5) MD5:43b5931b625e3b598423e0bb4a131d6e   eMMC image - required if changing Jetpack version\n Jetpack 5.0.2 emmc image MD5:73758afcdf0119a98b27c9d1630add6a Jetpack 4.6.1 emmc image MD5:1442d6597b6c93e8fff2543a808652ce Jetpack 4.5 emmc image MD5:e603db76e8ab1bbe5596760d40adb90c   Checksum  Check the md5 checksum to make sure image file is correct\n Linux $ md5sum [file]  $ md5sum NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz  Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz MD5    Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\n microSD of at least 32GB microSD card reader  Make sure to use a high quality microSD card to prevent corruption\n Unzip the microSD zip file downloaded in Step 1 to get a .img file\ntar -zxvf NeonJNX_A3_JP502_microSD_v1.0.7_woEVA.img.tar.gz  Clone image file to microSD card using one of the following methods\n Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card   Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher  Insert SD card into the NEON camera\n  Step 3: Flash eMMC image The internal eMMC must be flashed if the Jetpack version on the microSD card has changed, so the Jetpack version on the eMMC and microSD card match.\nThis step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  There is also a video and PDF showing the process\n On the Host PC unzip the file downloaded in Step 1\ntar -zxvf NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz  Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again  Connect the microUSB cable to the NEON and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd NeonJNX_A3_JP502_emmc_v1.0.7  Flash the Neon-2000-JNX\n For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass  Jetpack before 5.0.2\nsudo ./flash.sh  For Jetpack 5.0.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 1   Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\n  Below is a video of the process to flash a NEON-2000-JNX   Appendix: How to restore eMMC image  method 1\n step1: download environment on Linux x86 ubuntu PC  https://sftp.adlinktech.com/image/Neon-JNX/deploy.neonjnx.tar.gz md5: d9e7e4ff4030f7d81f72c353aad6d272  step2: untar the file  tar -zxvf deploy.neonjnx.tar.gz  step3: execute command  sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-neonjnx-emmc mmcblk0p1 Note: Booard name of different hardware version  HW A3: jetson-neonjnx-a3-emmc HW A2: jetson-xavier-nx-neonjnx-emmc    method 2\n Backup emmc   \ncd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp cd mntTemp sudo tar -jcf ../customerEMMC.tbz2 * sync cd ../ sudo umount mntTemp rmdir mntTemp - Restore emmc \u0026lt;div\u0026gt; \u0026lt;iframe src=\u0026quot;https://sftp.adlinktech.com/image/EOS-JNX/sop/Restore_emmc.mp4\u0026quot; width=\u0026quot;640\u0026quot; height=\u0026quot;385\u0026quot; scrolling=\u0026quot;no\u0026quot; framespacing=\u0026quot;0\u0026quot; webkitallowfullscreen mozallowfullscreen allowfullscreen\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt;  cd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp sudo tar jxf customerEMMC.tbz2 -C mntTemp sync sudo umount mntTemp rmdir mntTemp\n   "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " This page provides detailed instructions on how to flash the image for Neon-RB5 using Windows. 1. Prerequisite 1.0 Download and Install Android Debug Bridge (ADB) and Fastboot Drivers 2. How to Flash Image 2.1 Updation of Board Support Package(BSP) Image  1 Prerequisite\n A Windows PC(Win10/Win11) is required to flash the image on the Neon-RB5 Module BSP image md5: e7b7d72383bf80fd2b1e768a29519d96  1.0 Download and install ADB and Fastboot drivers ​ Please download the ADB and Fastboot drivers and install them using the link provided below.\n​ [Android USB Driver] : https://adb.clockworkmod.com\n​ [ADB Fastboot Tool] : https://xiaomifirmware.com/download-link/?dlm-dp-dl=2696\n​ Note: If you encounter “0 Files Copied Error”, download and install the “15_Second_ADB_Installer_v1.3.0” version as the latest version might not be compatible with the Windows version of your computer.\n2.1 Updation of BSP Image (Kernel and Filesystem) \nUpdate the kernel and filesystem and not the NON-HLOS firmware or bootloader, follow the steps below:\n Image needed qti-ubuntu-robotics-image-qrb5165-rb5-boot.img, qti-ubuntu-robotics-image-qrb5165-rb5-sysfs.ext4 and abl.elf\n Image can found on path :\n  #windows_Drive: /LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19\n Connect the USB OTG between Target and Host PC\n Power on the Target board.\n Open command prompt\n #windows_Drive: /LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19\n Check Target board (Neon-RB5) got detected using below command. It will take minutes to show the device name after power up the target device.\n#cd LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19 #adb devices -l   Note: In case the \u0026ldquo;#adb devices” command shows no device , please check your Device Manager for any yellow bang in the Qualcomm USB driver as shown in the image below:  The driver may have to be manually selected and updated as detailed in the screenshots below:   The driver is already present in the system at this stage and should be available in the list as ‘Android ADB Interface’\nAfter Updating Driver, please follow below to enter into ADB mode on Host PC and check Neon-RB5 detected as fastboot device\nYour target device should have been listed in your command prompt terminal. If not, please check and update the driver in the device manager on your development host.\n# adb root # adb reboot bootloader  The target device will be rebooted. It will take minutes to show the device name after power up the target device.\n#fastboot devices  ​ In case of device not listed , please check device manager for yellow bang and update driver\nEnter into Fastboot mode\n#fastboot flash abl_a abl.elf #fastboot flash abl_b abl.elf #fastboot flash system qti-ubuntu-robotics-image-qrb5165-rb5-sysfs.ext4 #fastboot flash boot_a qti-ubuntu-robotics-image-qrb5165-rb5-boot.img #fastboot flash boot_b qti-ubuntu-robotics-image-qrb5165-rb5-boot.img #fastboot reboot  The image has been successfully flashed on targeted device and it will be rebooted by itself. The device is now ready to be used.\n Attention: Don\u0026rsquo;t turn off (or) Reset the target device manually. It causes the image packages to break, resulting in the inability to boot. \n Please take a coffee, It will take around 30 minutes to install all the packages and desktop image. \n Once, everything is installed you could see the Ubuntu Login page in the monitor.  Log in as a Ubuntu On Wayland. Username: adlink Password: adlink   "
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/flashimg/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " This page cover the process to flash a new operating system onto a AI Camera Dev Kit device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a dev kit you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu AI Camera Dev Kit  microSD image  Jetpack 4.6.1 backup link MD5:C417A00B3BD500A617D201BC9B9D4421   Checksum  Check the md5 checksum to make sure image file is correct\n Linux $ md5sum [file]  $ md5sum AI_Vison_DEV_kit.zip  Windows 10\n $ certutil -hashfile [file] MD5  $ certutil -hashfile AI_Vison_DEV_kit.zip MD5    Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\n microSD of at least 32GB microSD card reader  Make sure to use a high quality microSD card to prevent corruption\n Unzip the microSD zip file downloaded in Step 1 to get a .img file\nunzip AI_Vison_DEV_kit.zip  Clone image file to microSD card using one of the following methods\n Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card   Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher  Insert SD card into the NEON camera\n  "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/samples/",
	"title": "Sample program",
	"tags": [],
	"description": "",
	"content": " Sample program of Neon-RB5 - Capture and Inference - Model converter\nCapture and Inference  The example program uses the built-in camera of the Neon-RB5 for image capture and employs the YOLOv6 model for object detection. The SNPE version used is 1.68.0.3932. Download link (md5: 253998b5f8b389914924e70b0545d705)\n Installation guild Install compile environment sudo apt-get update sudo apt install --fix-broken sudo apt-get install cmake  Untar the file tar -zxvf Neon-RB5_sample.tar.gz -C /home/adlink/Desktop cd /home/adlink/Desktop/Neon-RB5_sample cp /home/adlink/Desktop/Neon-RB5_sample/yolov6n_base_quantized.dlc /home/adlink tar -zxvf snpe-1.68.0.3932.tar.gz -C /home/adlink  modidy .bashrc and add content below export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/adlink/snpe-1.68.0.3932/lib/aarch64-ubuntu-gcc7.5:/usr/local/lib/ export ADSP_LIBRARY_PATH=\u0026quot;/home/adlink/snpe-1.68.0.3932/lib/dsp;/usr/lib/rfsa/adsp/;/vendor/lib/rfsa/dsp/testsig;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp\u0026quot; export PATH=$PATH:/home/adlink/snpe-1.68.0.3932/bin/aarch64-ubuntu-gcc7.5  Reload the .bashrc setting source ~/.bashrc  Compile the sample cd /home/adlink/Desktop/Neon-RB5_sample/samples mkdir build \u0026amp;\u0026amp; cd build cmake .. \u0026amp;\u0026amp; make  Execute the inference sample cd /home/adlink/Desktop/Neon-RB5_sample/samples/build cp ../../yolo.txt . ./yolov6_snpe   Procedure of Model Conversion Required hardware - x86 Host PC with [ubuntu18.04](https://releases.ubuntu.com/18.04) - Neon-RB5  The AI model used in the example is Meituan YOLOv6-N Below, we will describe how to convert the downloaded yolov6n.pt file into a dlc file for use with Neon-RB5.\nThe model conversion process primarily consists of four steps and requires a host PC with Ubuntu 18.04. Steps 1~5 are executed on the host.\nStep 1: Environment Setup sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade sudo apt install python3-pip -y pip3 install --upgrade pip sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 1 update-alternatives --list python  Step 2: Download conversion tools wget https://sftp.adlinktech.com/image/Neon-RB5/snpe-1.68.0.zip -O ~/snpe-1.68.0.zip wget https://sftp.adlinktech.com/image/Neon-RB5/YOLOv6.tar.gz ~/YOLOv6.tar.gz  Step 3: Setup conversion environment cd ~ unzip -X snpe-1.68.0.zip -d ~ export SNPE_ROOT=/home/adlink/snpe-1.68.0.3932 export PYTHONPATH=$PYTHONPATH:$SNPE_ROOT/lib/python source $SNPE_ROOT/bin/dependencies.sh source $SNPE_ROOT/bin/check_python_depends.sh pip3 install numpy==1.16.5 sphinx==2.2.1 scipy==1.3.1 matplotlib==3.0.3 scikit-image==0.15.0 protobuf==3.6.0 pyyaml==5.1 source snpe-1.68.0.3932/bin/check_python_depends.sh pip3 install onnx pip3 install torch pip3 install onnxsim  Step 4: Convert *.pt to ONNX cd ~ tar -zxvf YOLOv6.tar.gzcd ~/YOLOv6/ python3 ./deploy/ONNX/export_onnx.py \\ --weights ./deploy/ONNX/yolov6n.pt \\ --img 288 \\ --batch 1  Step 5: Convert *.ONNX to *.dlc cd ~/snpe-1.68.0.3932/bin/x86_64-linux-clang ./snpe-onnx-to-dlc --input_network ~/YOLOv6/deploy/ONNX/yolov6n.onnx --output_path yolov6n_base_quantized.dlc  Step 6: Replace model in Neon-RB5 Replace *.dlc in /home/adlink/Desktop/Neon-RB5_sample/samples/build\n cd /home/adlink/Desktop/Neon-RB5_sample/samples/build ./yolov6_snpe  "
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/",
	"title": "AI Camera Dev Kit",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Edge Vision AI Camera Dev Kit Discover how to use the AI Camera Dev Kit and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/",
	"title": "EOS-JNX",
	"tags": [],
	"description": "",
	"content": " ADLINK IST Connected Factory EOS-JNX Discover how to use the EOS-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": " EOS-JNX emmc image and microSD image are one-by-one mapping. That is, you have to flash emmc and microSD image with same version. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nDownload eMMC image  [jetpack5.1.3 md5: 1203689de27fde2bc7151f27ff5b9e6e]((https://sftp.adlinktech.com/image/EOS-JNX/EOS-JNX_JP513_emmc.tar.gz) jetpack5.0.2 md5: 588e91411bb61f5b95b190bebb8373de jetpack4.6.1 md5: e9978a1f7d981b11fd9f323a6ca73aa7 jetpack4.6 jetpack4.5  How to reflash eMMC image  To perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine. microUSB cable  For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass    On the Host PC unzip the file downloaded.\ntar -zxvf EOS-JNX_JP502_emmc.tar.gz  Set EOS-JNX as recovery mode\n Boot EOS-JNX Hold RECOVERY button on front panel Push RESET button on front panel Release RECOVERY button  Connect the microUSB cable to the EOS-JNX and the Host PC\n Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.  Go to folder unzipped in step 1\ncd EOS-JNX_JP502_emmc  Flash the EOS-JNX\n Jetpack before 5.0.2\nsudo ./nvmflash.sh  For Jetpack 5.0.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 20   Once the flash script is complete and shows Flash complete (SUCCESS) reboot the EOS-JNX\n  How to restore eMMC image  method 1\n step1: download environment on Linux x86 ubuntu PC  https://sftp.adlinktech.com/image/EOS-JNX/deploy.eosjnx.tar.gz md5: 0893c0969c1000ab98b1dc60e645d1cc  step2: untar the file  tar -zxvf deploy.eosjnx.tar.gz  step3: execute command  sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-eosjnx-emmc mmcblk0p1   method 2\n Backup emmc   \ncd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp cd mntTemp sudo tar -jcf ../customerEMMC.tbz2 * sync cd ../ sudo umount mntTemp rmdir mntTemp - Restore emmc \u0026lt;div\u0026gt; \u0026lt;iframe src=\u0026quot;https://sftp.adlinktech.com/image/EOS-JNX/sop/Restore_emmc.mp4\u0026quot; width=\u0026quot;640\u0026quot; height=\u0026quot;385\u0026quot; scrolling=\u0026quot;no\u0026quot; framespacing=\u0026quot;0\u0026quot; webkitallowfullscreen mozallowfullscreen allowfullscreen\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt;  cd ~/ mkdir mntTemp sudo mount /dev/mmcblk0p1 mntTemp sudo tar jxf customerEMMC.tbz2 -C mntTemp sync sudo umount mntTemp rmdir mntTemp\nDownload microSD image  jetpack5.1.3 md5: 8869748a7f6736861d963afdfcf3b414\n jetpack5.0.2 md5: dda62d5d21f84d25368b5080eaed80e0\n jetpack4.6.1 md5: d8006c99a4c3fc4ee7d5ba1639e8f57e\n jetpack4.6.1 with EVA md5: b58ea12bc4c8f5a4a5ffe173bc056f5c\n jetpack4.6\n jetpack4.5\n SOP\n Windows  video tool  Linux  video    Check sum $ md5sum EOS-JNX_JP502_emmc.tar.gz 588e91411bb61f5b95b190bebb8373de EOS-JNX_JP502_emmc.tar.gz $ md5sum EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz dda62d5d21f84d25368b5080eaed80e0 EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz   check md5 check sum to make sure image file is correct\n Linux $ md5sum [file]\n $ md5sum files.tar.gz\n Windows 10\n certutil -hashfile [file] MD5\n certutil -hashfile files.tar.gz MD5\n   "
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtobootfromssd/",
	"title": "How to boot from m.2 SSD?",
	"tags": [],
	"description": "",
	"content": "  Internal Storage Installation Clone sd card data to m2 SSD  Internal Storage Installation Refer to manual chapter 2.3 internal storage installation of EOS-JNX.\nClone sd card data to m2 SSD  Refer to SOP video. Take 256Gb m.2 SSD for example.\n format your storage as ext4\n copy the files\nsudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd  Modify /boot/extlinux/extlinux.conf in emmc, emmc, emmc !!!\nAPPEND ${cbootargs} quiet root=/dev/nvme0n1p1 rw rootwait rootfstype=ext4  Reboot\n  "
},
{
	"uri": "https://aiot-ist.github.io/",
	"title": "ADLINK-IST Connected Factories",
	"tags": [],
	"description": "",
	"content": " ADLINK-IST Edge Vision Discover the sharing of technical documents and the common questions. You could navigate from the menu or simply type the keyword to search!\n "
},
{
	"uri": "https://aiot-ist.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/neon/",
	"title": "NEONs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]