[
{
	"uri": "https://aiot-ist.github.io/mcm-204/",
	"title": "MCM-204",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory MCM-204 Discover how to use the MCM-204 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/u300/",
	"title": "PCIe-U300 Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s PCIe-U300 Series is a PCI Express x4 Gen2 USB3 Vision frame grabber supporting 4/8/12 USB 3.1 Gen 1 ports for multiple USB3 Vision device connections with data transfer up to 5 Gb/s per port.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/",
	"title": "USB3 Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about NEON-Series\nModel and Sensor Specifications Hardware Specifications Software Environment Software Version check Trigger input and Strobe output General NEON-Series Model and Sensor Specifications Model Name Image Sensor Specification Image Sensor Module NEON-201B-JT2 1/ 3\u0026quot;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS NEON-202B-JT2 1/1.8\u0026quot;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS NEON-203B-JT2 1/3.7\u0026quot;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS NEON-204B-JT2 1/2.5\u0026quot;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS NEON-201B-JNX 1/ 3\u0026quot;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS NEON-202B-JNX 1/1.8\u0026quot;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS NEON-203B-JNX 1/3.7\u0026quot;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS NEON-204B-JNX 1/2.5\u0026quot;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS NEON-201A-JNX 1/2.6\u0026quot;, 2 M, Global Shutter , 1920x1080, 60fps, COLOR Appropho, On Semi AR0234 NEON-202A-JNX 1/1.8\u0026quot;, 8 M, Rolling Shutter, 3840x2160, 30fps, COLOR Appropho, Sony IMX334 Hardware Specification Neon-iSeries Digital Input Digital Output UART Dimensions Weight Neon-Series 4x DI, includes 1x sensor trigger 4x DO, includes 1x strobe out TXD, RXD, GND 123.3 x 66.81 x 77.5 mm 700g Software environment Neon-Series JetPack L4T Ubuntu CUDA cuDNN TensorRT Python2 Python3 Pylon Version 4.2.1 32.2 18.04.2LTS 10.0 7.5.0.56 5.1.6.1 2.7.15 3.6.9 5.0.9 Version 4.3 32.3.1 18.04.2LTS 10.0.326 7.6.3.28 6.0.1.10 2.7.17 3.6.9 5.2.0 Version 4.4 32.4.3 18.04.2LTS 10.2 8.0.0.180 7.1.3.0 2.7.17 3.6.9 5.2.0 Version 4.6.1 32.7.1 18.04.2LTS 10.2 8.2.1 8.2.1 2.7.17 3.6.9 5.2.0 Version 5.0.2 35.4.1 20.04 11.4 8.4.1 8.4.1 2.7.18 3.8.10 5.2.0 Version 5.1.2 35.4.1 20.04 11.4 8.6.0.166 8.5.2 2.7.18 3.8.10 5.2.0 Version check Jetpack version sudo apt-cache show nvidia-jetpack L4Tversion head -n1 /etc/nv_tegra_release TensorRT dpkg -l|grep nvinfer CUDA nvcc --version cuDNN dpkg -l|grep cudnn Python python --version Trigger in and Strobe out NEON-201B-JNX NEON-202B-JNX NEON-203B-JNX NEON-204B-JNX NEON-201A-JNX NEON-202A-JNX Support V V V V V - How to set the NEON camera to trigger mode. NEON-20xA Enable the trigger mode. l2-ctl --set-ctrl=trigger_mode=1 l2-ctl --set-ctrl=low_latency_mode=1 Reference sample code NEON-20xB Open Pylon viewer. \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Mode\u0026rdquo;-\u0026gt; \u0026ldquo;On\u0026rdquo; \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Source\u0026rdquo;-\u0026gt; \u0026ldquo;Line1\u0026rdquo; Reference sample code How to wire. "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/wd--fov/",
	"title": "Neon &amp; Lens selector",
	"tags": [],
	"description": "",
	"content": "Neon Model and Sensor Specifications Model Name NEON-201B NEON-202B NEON-203B NEON-204B NEON-201A NEON-202A Image Sensor spec. USB3 USB3 USB3 USB3 MIPI CSI MIPI CSI Resolution (HxV) 1280 x 960 1600 x 1200 1920 x 1080 2592 x 1944 1920 x 1200 3840 x 2160 Resolution 1.2M 1.9M 2M 5M 2M 8M Frame Rate (fps) 54 60 30 14 60 30 Color/Mono Color Color Color Color Color Color Shutter Global Global Rolling Rolling Global Rolling Sensor Size 1/3\u0026quot; 1/(1.8\u0026quot;) 1/(3.7\u0026quot;) 1/(2.5\u0026quot;) 1/(2.6\u0026quot;) 1/(1.8\u0026quot;) Pixel Size (µm) 3.75 x 3.75 4.5 x 4.5 2.2 x 2.2 2.2 x 2.2 3 x 3 2.0 x 2.0 Sensor Vendor ON Semiconductor e2v ON Semiconductor ON Semiconductor ON Semiconductor SONY Sensor Model AR0134 EV76C570 MT9P031 MT9P031 AR0234 IMX334 Image sensor SDK Basler pylon Basler pylon Basler pylon Basler pylon V4L2 \u0026amp; Gstreamer V4L2 \u0026amp; Gstreamer Getting the FOV and Resolution Please select the Model Name: None\rNEON-201B\rNEON-202B\rNEON-203B\rNEON-204B\rNEON-201A\rNEON-202A\rPlease select the Lens: None\r3.5mm\r8mm\r12mm\r16mm\rPlease enter WD (mm): VFOV (mm):\rHFOV (mm):\rHFOV(mm/pixel):\rVFOV(mm/pixel):\rImage(H):\rImage(V):\rH_Angle:\rV_Angle:\rPixel Size (µm):\rGet Your NEON AI Smart Camera Now!\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtochooselenses/",
	"title": "How to choose suitable lens for Neon?",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s the lens selector tool from Basler website. Please enter your criteria and we\u0026rsquo;ll show you suitable lens models.\nStep 1: Select camera series and model Takes Neon-203B for example\nStep 2: Please enter as many values as possible. Missing values will be calculated automatically. Take example for real case:\nAt 75ft(22860 mm) needs to support a field of view that is 406 inches (10312 mm) X 406 inches (10312 mm) And supports a 10ft depth of field Step 3: Display suitable lenses If you want to use the lens with a CS-mount camera, a distance ring (5 mm) must be attached to the lens.\nStep 4: You can contact with your vendor and find the similar spec of lenses. Kindly remind you that the smaller focal length may cause “fisheye” effect.\nThe document from Basler website explains what you should know when selecting a lens for your camera.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtouseneontocontroldio/",
	"title": "How to use NEON to control DI/O?",
	"tags": [],
	"description": "",
	"content": "Here is the list for our demo devices. Different devices might have different default settings. Please adjust the procedures based on your corresponding devices.\nEasy Intermediate Easy Devices List: NEON-2000-JNX Series Starter Kit ADLINK DIN-37D-01 PATLITE MES-_02A 24V Power Supply Architecture diagram Overview Wiring Step 1. Connect the Neon with the din board through a connector. Step 2. Connect the Red LED\u0026rsquo;s wire in pin 4 for digital output (we take DO1 as an example). Step 3. Plug the 24V power supply\u0026rsquo;s negative wire in Din board\u0026rsquo;s pin 2 for grounding. Step 4. Connect the positive of power supply with the positive of LED. It shorts the two wires at pin20 which is reserved as terminal. Step 5. Use Neon to control the LED. Use the command cd /usr/src/Neon/Sample/Neon_Setting to adjust Neon setting. Use sudo ./NeonSet DO 1 1 to turn on the LED, use sudo ./NeonSet DO 1 0 to turn off the LED conversely. Note Due to dual function of DO0 and DI0, the default function of DO0 is strobe out and DI0 is trigger in. If you want to set them as general DO and DI. Please configure it as following:\nDIO0Config 1: Set DIO0 as GPDI and GPDO0 DIO0Config 0: Set DIO0 as Trigger in and Strobe out (default) Set DIO0 as GPDI and GPDO0 sudo /usr/src/Neon/Sample/Neon_Setting/NeonSet DIO0Config 1 Check the information of DIO. sudo /usr/src/Neon/Sample/Neon_Information/NeonInformation Intermediate Devices List: ADLINK NEON-1000-MDX ADLINK DIN-37D-01 High Bright Tech PC-24V24W-2-S 5V Power Supply Step 1. Connect the light with the light controller at channel 1. Step 2. Connect the light controller with din board at strobe channel 1. Plug the positive wire in pin 3 for device output, and plug the negative wire in the pin 10 for grounding.\nStep 3. Connect the Neon with the din board through a connector. Step 4. Connect a trigger device on the din board. Insert the positive wire at pin 11 for trigger in, insert the negative wire at pin 10 for grounding. Here we used a 5V power supply for demonstration, you could connect your own device for your own purposes.\nStep 5. Adjust the default settings from Neon. Please enter the specific file path to adjust the settings. We modified the strobe-out polarity to set turning off the light as default, this step might be different due to your controller devices. Once the Neon reboots, the setting would be restored.\nUse command sudo -i to get in root mode. Use the command cd /sys/class/neon_camctrl to change directory. Use cat command to check current status. To change the default setting of strobe-out polarity, use the command echo 1 \u0026gt;StrobeOutPolarity. Step 6. To extend or narrow the device strobe out time, please follow the steps\nUse the command cd /usr/src/Neon/Sample/Neon_Setting to change the current working directory. To adjust the strobe out time, use the command sudo ./NeonSet StrobeOutPulseWidth N. The parameter N is the time of the width, the unit is us(10^-6 second). For checking current infos of Neon, please change the directory with the command cd /usr/src/Neon/Sample/Neon_Information. Use sudo ./NeonInformation to get current status. "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtosetupneon/",
	"title": "How to set up Neon camera?",
	"tags": [],
	"description": "",
	"content": "This section describes the mounting accessories provided with the Neon camera and includes actual installation images for user reference during on-site setup.\nAccessories Mount in the factory Accessories VESA mount The Neon camera provides standard VESA mount installation(10x10cm) and can be referenced with accessory 91-95321-0010. Tripod You can also refer to our designed tripod adapter plate to mount the Neon camera on a photography tripod, for example Velbon EX-Macro. Mount in the factory "
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtobackupandextendsdcard/",
	"title": "How to backup and extend microSD card?",
	"tags": [],
	"description": "",
	"content": "According to application requirements, when the micro SD card installed in the Neon camera or EOS-JNX doesn\u0026rsquo;t meet the capacity needs, you can follow the steps below to back up and restore the card\u0026rsquo;s contents to a larger storage device.\nBack up micro SD card Restore image Clone micro SD card by rsync command Extend the partition of SD card Neon camera and EOS-JNX supports addressing larger capacity SD 3.0 or SD-XC cards up to 2 TB. We have tested the following cards:\nSanDisk ExtremePRO microSDXC 512GB SanDisk ExtremePRO microSDXC 128GB SanDisk ExtremePRO microSDXC 64GB Transcend TS128GUSD430T 128GB Transcend TS64GUSD430T 64GB Back up micro SD card Please prepare the Linux x86 machine and use the \u0026ldquo;disks\u0026rdquo; tool preinstalled in ubuntu.\nRestore image Restore the backup image made from previous section into your another SD card.\nClone image file to microSD card using one of the following methods Ubuntu Disk Manager Video of process to clone image to microSD card Steps: Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher Clone micro SD card by rsync command format your target micro SD card as ext4 format rsync [OPTION] SOURCE [SOURCE]... DEST For example sudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/test \u0026amp;\u0026amp; sync Extend the partition of SD card If your memory card space is larger than the backup image, you need to extend your memory card partition.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/faq/howtoupdatecamfw/",
	"title": "How to update Camera Firmware of Neon-20XA-Series?",
	"tags": [],
	"description": "",
	"content": "Keeping your Neon Camera updated with the latest firmware ensures more stable performance for your application.\nNeon-Series\rNeon-201A\rNeon-202A\rLatest FW Version\r23414\r33413\rStep 1: Check camera FW version v4l2-ctl -C version\nStep 2: Update camera FW //Here is an example where the FW file has been pre-downloaded and extracted to /home/adlink.\rcd FW-24314\rsudo dd if=RS_M12MO_AR0234C70E.bin of=/sys/kernel/debug/m12mo_a/fwupdate obs=2093056\r//The default password of Neon is adlink Step 3: Unplug Neon power and power on after flash procedure Step 4: Check the FW version again v4l2-ctl -C version\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/",
	"title": "EOS-iSeries",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory EOS-iSeries Discover how to use the EOS-iSeries and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/",
	"title": "GigE Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/",
	"title": "PCIe-GIE7x Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s ADLINK\u0026rsquo;s PCIe-GIE7x series PCI Express® PoE+ frame grabber supports 2/4-CH independent Gigabit Ethernet ports for multiple GigE Vision connections transferring up to 1 Gb/s per port. PoE+ provides up to 30W power and automatic detection for stable, reliable connections, reducing costs, simplifying installation, and easing maintenance burdens.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/gettingstart/",
	"title": "Getting start with Yolov3",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with EOS-iSeries from un-boxing.\nStep 1: Hardware wiring Connect the peripherals, keyboard, mouse, monitor and cameras. Connect 110V AC power source to the terminal block Power on Step 2: Run inference with different source The path of inference sample locates on the C:\\Users\\user\\Desktop\\ADLINK. You can select image, video, webcam or Basler GigE camera as inference source.\nOpen Terminal and paste commands below:\nImage cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64\ryolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights C:\\Users\\user\\Desktop\\ADLINK\\darknet-For_basler_camera\\data\\dog.jpg Terminate program by close the Terminal or ctrl+c.\nVideo\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64\ryolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4 Webcam\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64\ryolo_console_dll.exe web_camera Basler GigE camera\nOpen pylon Viewer and make sure GigE camera works. Key in commands cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64\ryolo_console_dll.exe basler_camera or Click Run_basler_camera_with_Object_Detection.bat in folder "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-1000-mdx/",
	"title": "Neon-1000-MDX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision Neon-1000-MDX Discover how to use the Neon-1000-MDX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about Pylon\nHow to grab the Image from the GigE Basler. Step1 : Set the Lan port IP address ? Assign the TCP/IP4 IP to be 192.168.11.1 Step2: How to set the Camera IP by Pylon? Set the Ip 192.168.11.4 by Pylon IP configurator. Video "
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/powerbudget/",
	"title": "What is PCIe-GIE74P PoE Budget Mode？",
	"tags": [],
	"description": "",
	"content": "The PoE budget mode is to ensure product power supply security, PoE automatically cuts off when PoE power is exceeded. PCIe-GIE74P PoE Power budget 1、Max 20.0W w/ PCIe slot only. 2、Max 61.6W w/ PCIe slot and 4-pin Molex connector. How does PCIe-GIE74P reserve PoE Power Budget？ ADLINK provides two power budget modes, customers can configure settings according to user scenarios. PoE Power Consumption Mode for variable power budget PCIe-GIE74P will detect the camera’s real power consumption to reserve PoE power budget. PoE Class Mode for fixed power budget PCIe-GIE74P will detect the camera’s PoE class to reserve PoE power budget. How to setting Power Budget Mode？ 1、Using API of AVS_PoESetPowConsumCalcModel. Customer can configure settings according to user scenarios. 2、Execute POE Budget Mode Setting Tool. This tool can set multi-card to the power consumption mode at one time. Step1、Download SDK from WebLink Step2、 Install AVS SDK Step3、 Open Folder \u0026ldquo;POE_tool\u0026rdquo;. Step4、 Execute “Sample.exe”. Example、User’s Situation. If I want to connect 3 GIgE cameras, and I didn’t connect PCIe-GIE74P’s 4-pin Molex connector\u0026hellip; By PoE Power Consumption Mode If this camera’s real power consumption is 5 watts, PCIe-GIE74P will reserve 6 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 5 (watts) = 15 (watts)\nPCIe-GIE74P can power 3 cameras. By PoE Class Mode If this camera’s PoE class is “2”, according to the PoE class of the PoE standard, PCIe-GIE74P will reserve 7 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 7 (watts) = 21 (watts)\nPCIe-GIE74P can only power 2 cameras, cannot power the third camera. If you want to provide more than 20 watts of power, you need to connect the 4-pin Molex connector of PCIe-GIE74P.\nTable PoE Classes Class No. Type Maximum power available at the Power Sourcing Equipment (PSE) Power required by PoE class at the Powered Device (PD) 0 802.3af 15.4 W 0.44 – 12.95 W 1 802.3af 4.0 W 0.44 – 3.84 W 2 802.3af 7.0 W 3.84 – 6.49 W 3 802.3af 15.4 W 6.49 – 12.95 W 4 802.3at(PoE+) 30 W 12.95 – 25.5 W "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/datasheet/",
	"title": "00-Datasheet",
	"tags": [],
	"description": "",
	"content": "All accessory datasheets for KVM local and remote packages kit are available. Accessory list HDMI to VGA adapter VGA to HDMI Converter VGA Splitter 1 to 2 KVM DX-131RX VX-131TX "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/newloadproject/",
	"title": "00-New project &amp; Run Script &amp; combine Script",
	"tags": [],
	"description": "",
	"content": " How to create a new project Reload a project(blf) when remodifying How to combine a script before running Run the script (.SPT) What is the difference between with SPT VS BLF How to create a new project Step: 1 Click New 2 Click Select 3 Choice the folder for save the script data 4 Click Ok Reload a project when remodifying Step 1 Click load 2 Choice the .blf list file How to combine a script before running Step 1 Click mouse button of right in any one getting.bmp 2 Combine Script -\u0026gt; General (It\u0026rsquo;s ok for choicing any one bmp) 3 Inupt the name and click ok Run the script Step 1 Click load 2 Choice the .spt list file 3 select the file 4 Click open Click Run Scripts What is the difference between with SPT VS BLF Pleas see below list\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/quicklystart/",
	"title": "00-Quckly start (Video)",
	"tags": [],
	"description": "",
	"content": "DEX-100 Quckly start (video) Follow those videos you can quickly finish one project by yourself. You can check other links for getting more tips, samples, and notes.\n-製作圖層篇,How to make the layer\n-設定Page ID篇,How to set the PageID\n-編輯腳本篇,How to edit the instruction\n-OCR使用REST上拋資料篇,How to get the OCR by restful\n-設定OP Screen篇,How to set the OP Screen\n-自動執行腳本篇,How to set for autorun the script\n-OCR設定篇,How to add the OCR\n-OCR 字庫訓練工具篇,How to train OCR to improve accuracy\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/goto/",
	"title": "01-Jump",
	"tags": [],
	"description": "",
	"content": " Use GoTo fulfill JUMP function Use both element of GoTo with Lebel fulfill JUMP function Use PageID fulfill JUMP function Use both element of PageID with Lebel fulfill JUMP function Use Table Compare fulfill JUMP function How to design \u0026ldquo;Jump function\u0026rdquo; by GOTO \u0026amp; PageIdentify \u0026amp; label \u0026amp; Table compare. There are three function can fulfill JUMP\nGOTO : Directly jump to other command line. PageID : It is like the pattern match and jump to other command line. Compare Table : Compare the OCR Table and jump to other command line. Use GoTo fulfill JUMP function Discription Use both element of GoTo with Lebel fulfill JUMP function Discription Use PageID fulfill JUMP function Discription Use both element of PageID with Lebel fulfill JUMP function Discription Use Table Compare fulfill JUMP function Discription "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/layer/",
	"title": "01-Layer",
	"tags": [],
	"description": "",
	"content": "How to add the bmp(layer) and select the folder for project You can flow the below steps or watch the video link.\nQuickly guide Video SOP Select one folder to save the script data. Click Connect Select the browser of Remote Control Click Control Move mouse or key the keyboard to you want page(bmp)\nClick snapshot\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/unboxing/",
	"title": "01-Unboxing",
	"tags": [],
	"description": "",
	"content": "DEX-100 Unboxing Equipment wiring map DEX-100 wiring for VGA DEX-100 wiring for DVI DEX-100 wiring for USB DEX-100 wiring for PS2 DEX-100 for editing environment What is the different between (DP to VAG) and (VGA out) ? Equipment wiring map The basic wiring is below the figure. DEX-100 wiring for VGA This is PC connected to screen by VGA cable. Separate the VGA cable that the VGA-INPUT connects to dex-100 then VGA-OUTPUT connects to screen. DEX-100 wiring for DVI This is PC connected to screen by DVI cable. Separate the DVI cable that the DVI -INPUT connects to dex-100 then DVI -OUTPUT connects to screen. 1.DVI-D is dual-link type please referring.\n2.DVI Output shall display when DVI input and DVI output are connected then restart the dex-100.\nDEX-100-wiring-usb These are USB of keyboard and mouse how to wire with DEX. DEX-100-wiring-ps2 These are PS2 of keyboard and mouse how to wire with DEX. DEX-100 for editing environment The dex-100 has one GUI that is dex-pro, the dex-pro can design a script for auto-running and show the exclusive screen by DP. Due to editing the script that requires ones pair keyboard and mouse then show in the exclusive screen. What is the different between (DP to VAG) and (VGA out) ? Describe the different functions between (PD to VAG) and (VGA OUT), Please referring the below figure. e "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/script/",
	"title": "01-Unboxing the KVM",
	"tags": [],
	"description": "",
	"content": "How to unbox and wire for both KVM local and KVM remote solutions. Overview Full view of KVM local accessories Full view of KVM remote accessories Unboxing KVM Local accessories KVM remote accessories Full view of KVM local accessories Full view of KVM remote accessories KVM Local accessories VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\nMachine VGA wiring to KVM\nKeyboard \u0026amp; Mouse wiring from KVM to Machine\nKVM remote accessories VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\nWiring both TX and RX\nWiring the VGA from Machine to TX\n"
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/unboxing/",
	"title": "01-Unboxing the KVM",
	"tags": [],
	"description": "",
	"content": "How to unbox and wire for both KVM local and KVM remote solutions. Overview Full view of KVM local accessories Full view of KVM remote accessories Unboxing KVM Local accessories KVM remote accessories Full view of KVM local accessories Full view of KVM remote accessories KVM Local accessories VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\nMachine VGA wiring to KVM\nKeyboard \u0026amp; Mouse wiring from KVM to Machine\nKVM remote accessories VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\nWiring both TX and RX\nWiring the VGA from Machine to TX\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/addscript/",
	"title": "02-Add script",
	"tags": [],
	"description": "",
	"content": " Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction How to add script Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/mouse/",
	"title": "02-Mouse",
	"tags": [],
	"description": "",
	"content": " Click Dclick Up Down Coordinate refer from OCR Table How to Click、Dclick the mouse in the script Instruction can simulat those command for mouse button of left,middle and right.\nFollow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse click the bmp in right side 4. Show the coordinate X and Y 5. Can add bye hotkey or add-Script 6. You can see the click/Dclick instruction in the script How to shift the popup form in instruction Follow the steps:\n1. Click the Starting poistion 2. Recorder the coordinate X \u0026amp; Y. 3. Choice the instruction of \u0026ldquo;left click down\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 4. Click the second poistion 5. Recorder the coordinate X \u0026amp; Y. 6. Choice the instruction of \u0026ldquo;move\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 7. Click the ended poistion 8. Recorder the coordinate X \u0026amp; Y. 9. hoice the instruction of \u0026ldquo;left click up\u0026rdquo; and keyin the coordinate X \u0026amp; Y. How the position coordinate refer the dynamic OCR Table sample data\nScript.7z restful.py SOP.zip "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/keyin/",
	"title": "03-Keyin",
	"tags": [],
	"description": "",
	"content": " From Text From OCR Table From external txt Choice Number key From Text The instrction can key in the statice string.\nFollow the steps:\nChoice the getting_xxx.bmp Screen capture mode Add script and select instruction of key-in Key-in from Text x = 1 from text Text = \u0026ldquo;ADLINK GOGO\u0026rdquo; From OCR Table Choice the getting_xxx.bmp\nScreen capture mode\nAdd script and select instruction of key-in Key-in from Text\nx = 3 from text Y = 1 OCR Table PS: Key-in value from the \u0026ldquo;OCR Table 1 = ABCD1234 \u0026quot;\nFrom external txt Choice the getting_xxx.bmp\nScreen capture mode\nAdd script and select instruction of key-in Key-in from external text Choice Number key Keyboard\u0026rsquo;s 0 to 9 there are two types for sent the different ASCII, so one parameter can switch it. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/pageid/",
	"title": "03-PageID",
	"tags": [],
	"description": "",
	"content": "How to identify the feature for each bmp(layer) You can follow the below steps or watch the link below Video.\nQuickly guide Video SOP Open your design\u0026rsquo;s [.blf] file. Select the page of Page Identify Define and draw the region(ROI) Example For the ROI, all parameter values of pixel size must be multiples 4.\nSave config the region of the feature(ROI) after adjusting the threshold for each BMP(layer). A larger threshold can tolerate more noise, but it will lose accuracy.\nClick ok Load config for checking PageIdentify enabled Click the button to connect again then seeing the light blue bar. It is successful. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/",
	"title": "04-Instruction",
	"tags": [],
	"description": "",
	"content": "Overview Video Video Funtions list How to design \u0026ldquo;Jump function\u0026rdquo; jump to flag or next step by goto Jump to flag when this page is right. Jump to next or previous step when comparing the OCR table. How to design \u0026ldquo;click function\u0026rdquo; Left-button,right-button and midel-button all can single-click and double-click. Left-button,right-button and midel-button all can click-down or click-up. The mouse can click multi-point by dynamically update the OCR table. How to design \u0026ldquo;key-in function\u0026rdquo; key-in string is fixed. key-in string is from OCR Table. key-in string is from text. Add OCR and save the log How add OCR in script How to add the Exist ROI setting How to save the OCR table in CSV How to design a for loop Use the single loop. use the dual loop. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ocrandlog/",
	"title": "04-OCR and Log",
	"tags": [],
	"description": "",
	"content": " How to add the ROI for OCR in script How to set the \u0026ldquo;Exsit Setting\u0026rdquo; Gaussian blur Threashold Super resolution Interpolation Confidence How to save the ROI result in csv Enable the sharing OCR Table items Add instruction for recoding the organizations in CSV Save all OCR or part of OCR How to add the ROI for OCR in script Follow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse drawing the ROI in the layer before click add Draw the ROI which you want to monitor. (1~4) Click the Add for increasing the OCR instruciton in the script.(5) 4. Select the \u0026ldquo;Exist Settings\u0026rdquo; with each ROI Base on the amount of your designed ROI and select the \u0026ldquo;Exist ROI Setting\u0026rdquo; until lastly ROI. There are 4 pair instructions in the script after adding OCR. (For this demo )\nIf your \u0026ldquo;Exist Setting\u0026rdquo; is such as the left side of the below figure , pls view the link\nHow to set the Exist Setting Introduc the parameters in Exist Setting.\nAdd new one \u0026ldquo;Exist Settings\u0026rdquo; recipe Gaussian blur Functional difference for real case\nThreashold Super resolution Interpolation White and Black List Traineddata and Configuration What is the \u0026ldquo;Traineddata\u0026rdquo;?\nDue to character have many font,size and color, We usually separate the same feature in the same trainedata folder. If you don\u0026rsquo;t have any ideal,Please referring the Link\nWhat is the \u0026ldquo;Configuration\u0026rdquo;?\nChoose the configuration (single selection) selection). Press the Advanced Setting button for Tesseract advanced settings if required as follow. For the more parameters information in detail , you have to refer the tesseract from google. config.cfg is the golden and config1.cfg is up to user defined)\nConfidence How to save the ROI result in csv The script can log that you want to monitor OCR in CSV after enable sharing the OCR Table.\nEnable the sharing OCR Table items: Open the configuration managment Click the DDS \u0026amp; REST (OCR configuration) Select Reest Modify 0 to 20 and check yes then click \u0026ldquo;Save to items\u0026rdquo; (for this sample) Check thes OCR item to be Ture that you want to monitor. Stop REST Run REST (Reset the REST module) It is successful when you resee the restful module popup again. Exit Add instruction for recoding the organizations in CSV Open you desing\u0026rsquo;s [.blf] file. Choice the bmp which you want to design Click mouse button of left side then Add the Script Save all OCR or part of OCR Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (save all OCR) Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (part of OCR) where is the CSV ? There is a file per day.\nPath C:\\Users\\USER\\Documents\\RCM_TABLE_LOG The system default keeps the files for 30 days however you can modify the parameter.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/backup/",
	"title": "05-Backup/Restore",
	"tags": [],
	"description": "",
	"content": " DEX Pro Backup DEX Pro Restore Backup Config Replace Config Factory Restore OCR Restore When you backup/restore the machine ? You want to exchange to another DEX-100 and going to run that you design script and parameter. DEX Pro Backup Content OCR data base Schedule file Program DEX Pro Restore Content OCR data base Schedule file Program Backup Config Content DDS/LOG Config Modbus Config Machineini Page Definition The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config.\nReplace Config Content DDS/LOG Config Modbus Config Machineini Page Definition The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config\nFactory Restore Content Machineini OCR Restore SOP Download the OCRClean.bat Run the OCRClean.bat Download the OCRDB.zip and unzip it. Using the DEX Pro Restore and choice the OCRDB folder. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/loop/",
	"title": "05-LOOP",
	"tags": [],
	"description": "",
	"content": " How to use the loop How to use both loop2 with loop How to the loop X: Loop in execution.\n0: go to previous steps; =0: go to the last instruction \u0026lt;0: go to next steps\nY: Loop is ended.\n0: go to next steps =0: go to the last instruction \u0026lt;0: go to previous steps;\nW: This value denotes the iteration times of the loop\nPleas referring the sample: How to use both loop2 with loop They are a pair both loop2 and loop when you use the double loop.\nPleas referring the sample: The loop2\u0026rsquo;s start instruction must be the first loop. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/pageidetify/",
	"title": "06-PageIdentify",
	"tags": [],
	"description": "",
	"content": " How to use the PageIdentify How to the use the PAgeIdentify Introduction the parameter X: number of instructions to jump X\u0026gt;0: go to previous steps X=0: go to the last instruction X\u0026lt;0: go to the next steps Y: jump mode. 0: jump by instruction number 1: jump by label W: target page id Referring the number of bmp Pleas referring the sample 1: The pattern matches(Yes) then going to the \u0026ldquo;Jump\u0026rdquo; label.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\nPleas referring the sample 2: The pattern matches(Yes), then going to the following four steps.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/autorun/",
	"title": "06-Script Setting &amp; AutoRun",
	"tags": [],
	"description": "",
	"content": " Script Setting Scripts Management AutoRun Script Setting Time Stop: The waiting time after executing each instruction. Spt Step Speed: The minimum waiting time between instructions. Example: Time Stop Use both settings.\nObserve the figure below, where the second instruction takes an additional 3s compared to the first instruction.\nHow to get the 3s?\n\u0026ldquo;Time stop\u0026rdquo; is 1000ms \u0026ldquo;Sleep\u0026rdquo; is 3 3s = Time stop * Sleep Example: Spt Step Speed Set \u0026ldquo;Spt Step Speed\u0026rdquo; to 1000ms.\nNotice that the second instruction runs with a 1s delay after the first instruction.\nScripts Management Provides Multi-Script Management to define the trigger command table.\nIn other words, you can use RESTFUL or MSMQ to trigger the required script.\nOpen the MSMQ Execute Scripts Management Path: Tools \u0026gt; MSMQ Execute Scripts Management 1. Choose the MSMQ Cmd 2. Select the Scripts file 3. Click Change. Attachments MSMQ \u0026amp; RESTFUL sample MSMQ C# Sample (Download) Python Sample Download and install python from python.org Run pip3 install requests Sample code import requests import json #trigger script url3 = \u0026#34;http://127.0.0.1:5555/executescript/2\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} response3 = requests.post(url3, headers=headers) print(response3.text) #stop url4 = \u0026#34;http://127.0.0.1:5555/stopscript\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} response4 = requests.post(url4,headers=headers) print(response4.text) AutoRun Set up the Background Schedule.\nPath: Tool \u0026gt; Config \u0026gt; Background Schedule Setting\nSelect the \u0026ldquo;Script File\u0026rdquo; you want to run.\nNote: Enable the \u0026ldquo;Run script if HID keeps idling over timeout.\u0026rdquo;\nYou can also click Start(Auto) without selecting the \u0026ldquo;.spt\u0026rdquo; file again once you have selected the script file.\nHID idle time: The script will AutoRun when both keyboard \u0026amp; mouse have been idle for the set time.\nAutorun screensaver timeout: A 10-second countdown will begin before automatic execution.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/restful/",
	"title": "07-Restful",
	"tags": [],
	"description": "",
	"content": " How to add the instruction for RESTFUL API Upload all items once Uplaod one by one RESTFUL function list RESTFUL sample code and Postman application How to add the instruction for RESTFUL API Sop 0. Please enable the OCR Table you can refer the Enable the sharing OCR Table items,if you don\u0026rsquo;t how to set. 1. Add the script 2. Select the MSG_SAVE_TABLE_TO_LOG Upload all items once Q : 0,Upload all once through RESTFUL(Submachinestatus) X : 0,Save all items Text : Bypass the parameter when X is 0 and recode the ROI Table referring to the items of True status Get the all items by Submachinestatus of RESTFUL API.\nThe items auto upload once based on your design OCR amount when executing this instruciton. There are 5 ROI case.\nThere are 9 ROI case\nUpload one by one Q : 1,Upload one bye one through RESTFUL X : 1,Save selected items Text : 1~4;7 (This case is only allowed those 1 to 4 and 7 of OCR table Set or Get by RESTFUL API.) You can see the relationship in the below image.\nWhat is one by one The RESTFUL\u0026rsquo;s Get function can get one data once for you select OCR table items and without parsing anything metadata.\nThe RESTFUL\u0026rsquo;s Set function can set one data to OCR table for handshaking with other app.\n1.Set the data to OCR table by anytime.\n2.Get the current data from OCR table after executing this command.\n3.It wait more time by more you select OCR items.\nRESTFUL function list They support two format both XML and Json. Doc\nThe port is 5555 for RESTFUL. (Exapmle for local : 127.0.0.1:5555)\n1. SubmachineStatuse You can get all the OCR tables that you design for sharing,but mabye you shall analyze it by yourself. 2. Get You can get one metadata from OCR Table that you design for sharing. Those OCR TABLE are update after executing this MSG_SAVE_TABLE_TO_LOG instrution. The 0 in the OCR table cannot be obtained\n3. Set You can set any data to OCR Table. 4. Executescript It can trigg the script that you want to autorun. 5. Stopscript It can stop the current script. RESTFUL Sample code and Postman applictation C# sample\nPython sample\nPostman Application\n3.1 SubmachineStatuse For Command content: http://x.x.x.x:5555/submachinestatus (exchange the ip by yourself)\nFor Body content: {\u0026ldquo;DES\u0026rdquo;:{\u0026ldquo;MachineStatus\u0026rdquo;: {\u0026ldquo;SubDataType\u0026rdquo;: \u0026ldquo;RAW_DATA\u0026rdquo;}}}\n3.2 Set value For Command content: http://x.x.x.x:5555/setocrdata (exchange the ip by yourself )\nFor Body content : OCRID : 22 (OCR Table 22),OCRValue : 7878\n3.3 Get value For Command content: http://x.x.x.x:5555/getocrdata/3 (exchange the ip and 3 ,it is ocr table address, by yourself )\n3.4 Executescript For Command content: http://x.x.x.x:5555/executescript/1 (exchange the ip and 1 ,MSMQ cmd number, by yourself )\n3.5 Stopscript For Command content: http://x.x.x.x:5555/stopscript (exchange the ip by yourself )\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ledcontrol/",
	"title": "08-LED control",
	"tags": [],
	"description": "",
	"content": "How to control the LED You can control the LED ON/OFF by instruction.\nExample : There two LED turn on \u0026amp; ture off by instruction. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/printscreen/",
	"title": "09-Printscreen",
	"tags": [],
	"description": "",
	"content": " How to Print Screen Print Screen for tracing with OCR How to choose PNG or JPEG for storing image How to Print Screen Sometimes you need to store the current screen for analysis by another tool without using the OCR.\n1. Add one OCR of instruction Please feel free to add a position of OCR (this OCR is meaningless and only for creating instruction) in .blf mode and combine it after saving the Script.\n2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo; Load the .spt of your design before modifying the parameter. Select the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path. The storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\nPrint Screen for tracing with OCR To verify the results of OCR with the current screen, you can refer to the below steps.\n1. Add the OCR of instruction This sample has 4 OCR and one instruction of “MSG_SAVE_TABLE_LOG” in .blf mode and combines it after saving the Script.\nNote: To log the results of OCR in CSV, that needs the “MSG_SAVE_TABLE_LOG.”\n2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo; Select the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path.\nThe storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\nSample sharing How to choose PNG or JPEG for storing image Mabey, you have different considerations about your tools to choose the saving type of images.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-source/",
	"title": "1 - Choice the Dex-100 Video Source",
	"tags": [],
	"description": "",
	"content": "Choice the Dex-100 Video Source Click the Frame Grabber Setting and choice the Source input. Dex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting\rUse the Diag.exe for checking the current video information. Path -\u0026gt; -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe\rClear horizontal and vertical offset values Click the button of \u0026quot;Frame Adjust\u0026quot;\rPlease set 0 for all the offset\nSelect source and resolution Select source from VGA or DVI Select resolution \u0026ldquo;Auto\u0026rdquo; Get the Video informations Check the both result for Resolution and Signal, if the connection is successful.\nIf doesn\u0026rsquo;t detected the signal, follow the steps for troubleshooting. Check the Local machine setting for display resolution. Reconnect the local machine VGA cable to monitor. Reboot the local machine. Check the resolution from the monitor. Reconnect the VGA to Dex-100. Check Frame Grabber Setting whether the auto select for unexpected resolution. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/opscreen/",
	"title": "10-OPScreen",
	"tags": [],
	"description": "",
	"content": " How to use the OPSCreen Print Screen for tracing with OCR When to use the OPscreen The operator or engineer wants to view the specified page when DEX-100 is running. At this moment this function will be very suitable. The OPScreen supports a max of 5 pages of all.\nThose 5 pages are not the current frame. However, dex-100 can autosave the image when the script is running to the required page.\nThe Operator or Engineer only can select any pages of 5, and keyboard \u0026amp; mouse are disabled to control the machine. (This moment Dex-100 is working)\nNeed to know OPScreen is a special mode that screen is from VGA out and the mouse only can control by Mouse Input. How to use the OPSCreen -VIDEO,How to set the OP Screen\nSOP 1. Open the OP Screen setting 2. Select the Host view 3. Click the Done when you finish the setting. If you saw \u0026ldquo;Fail to load image: page X is undefined.\u0026rdquo;, Please recheck the Pageidentify setting.\nHow to stop by the trigger of external or internal? 1. Trigger by internal (software) Use the mouse click the button of Full Operation. 2. Trigger by external (hardware) The DI2 ON can alse stop the OPScreen mode.\nkill switch is enable.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-quality/",
	"title": "2 - Video quality adjustment",
	"tags": [],
	"description": "",
	"content": "How to adjust quality when inputting the bad video 3. Using the Diag.exe 4. Select source \u0026amp; resolution 5. Draw the image by clicking the \u0026ldquo;Start\u0026rdquo; 6. Confirm the current image that is right 7. Using the \u0026ldquo;Video Quality Analysis\u0026rdquo; 8. Choose the test interval and start the testing 9. Cancel the testing log or save it in storage 10. System auto full in the result in the parameter of the sampling phas 11. Trobleshooting Steps 1 Boot up the dex-100 Power on the DEX-100\n2 Input the Video source (VGA or DVI) Plug-in the video source cable.\n3 Using the Diag Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe\r4 Select source and resolution Select source Select resolution 5 Draw the image by clicking the button of Start You shall see the result when grabbing is successful.\nIt is the current source It is the current resolution It is succeful or filed If the resolution can\u0026rsquo;t match your choose resolution or the signal can\u0026rsquo;t be detected, please follow the below steps.\nDex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting 6 Confirm the current image that is right It is not right, please retrying Step 2~5 again.\nDex-100 can\u0026rsquo;t process some Intel chips that can\u0026rsquo;t output your required resolution because the resolution always keeps in one resolution.\n7 Using the Video Quality Analysis Test VGA video quality for a long time and give suggested parameters\n(Ensure that the video quality of the machine is almost the same throughout the day.)\n8 Choose the test interval and start the testing Choose the test interval There are several parameters for your requirement.\nClick the \u0026ldquo;start\u0026rdquo; button. 9 Cancel the testing log or save it in storage You shall see the report after automatically test the video quality.\nClick the ok button. 10 System auto full in the result in the parameter of the sampling phas It is successful when you see the value be set in the \u0026ldquo;Sampling Phase\u0026rdquo; from the testing result.\nThe system only brings into the testing result of the sampling phase in the correct parameter.\n11 Trobleshooting If you get a large value from \u0026ldquo;Average Dynamic Noise,\u0026rdquo; we suggest replacing the VGA cable with anti-noise (EMI CORES) when getting the unideal value after using Video Quality Analysis.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/resolution/",
	"title": "3 - Create new resolution",
	"tags": [],
	"description": "",
	"content": "How to increase a new resolution Frame Grabber Setting Attachments Frame Grabber Setting Path -\u0026gt; DEX-Pro -\u0026gt; Tool -\u0026gt; Config -\u0026gt; Frame Grabber Setting Select an Auto mode in \u0026ldquo;Frame Grabbe Configuration.\u0026rdquo;\nSteps () 1 Get the new resolution by Screen. 2 Run the ADLINK_SFDT Run the ADLINK_SFDT.exe follow the below path.\nPath -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\ADLINK_SFDT\r3 Detect the parameters from the current video You can get the parameters from the current resolution after clicking the “Sense ”button.\nIf no parameters are displayed, please refer to the below link (troubleshooting)\nvideo-source\n4 Backup the dex100_rgb.txt Clone the dex100.rgb to desktop and copy one more to be a backup file.\nPath -\u0026gt; C:\\Windows\\dex100_rgb.txt\r5 Increase a new resolution table Please search for a set of resolutions closest to your desire and copy it to the top of the \u0026ldquo;dex100_rgb.txt\u0026rdquo;.\n6 Edit the resolution name [Width Height fps Interlace] Name the new title and modify the parameters (Width、Height 、FPS) to quickly selected the resolution list.\n7 Fill in the parameters [HF VF VTotal Vsw] Fill in the parameters from the ADLINK_SFDT tool in “dex100_rgb.txt.”\n8 Get the unknown parameters Get the unknown parameters by filling in the new resolution((Width、Height 、FPS) in the first sheet “CVT” of VesaCVT.xlsx\n9 Fill in the parameters to Way_2 Fill in the parameters from the ADLINK_SFD in the sheet of Way_2.\n10 Fill in the parameters [Hf HTotal Hsv Hbp Vf VTotal Vsw Vbp PClock Cr] Please key-in each color-bound box parameter to dex100_rgb.\n11 Fill in the parameters in content of the new resolution set Please key-in each color-bound box parameter to dex100_rgb.\n12 Save the txt 13 Replace the dex100_rgb.txt Replace the new dex100_rgb.txt to this C:\\Windows\\dex100_rgb.txt.\n14 Reboot the Dex-100 15 Reuse the Diag.exe Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe\r15 Checking the resolution table can be choiced. 16 Choice the new list and click the button on start 17 Align the screen edge by button of Frame Adjust Attachments VesaCVT.xlsx\ndex100_rgb.txt\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/coaxlink-quad/",
	"title": "Coaxlink Quad",
	"tags": [],
	"description": "",
	"content": " How to install the firmware for one camera or multi camera How to show the video by eGrabber Studio How to install the firmware for one camera or multi camera You can use 1~4 cameras by installing the different firmware from Euresys Coaxlink Firmware management.\nSOP steps : Open this Coaxlink Firmware management Choice the avaliable card. Select your required variant. Click the button of proceed Click the ok. Wait to download finished. Shart down and replug-in the power, then power on again. How to show the video by eGrabber Studio SOP steps: Open this eGrabber Studio Select the Coaxlink and the system will auto show how many cameras can be used. Choose each CCD and click the button to open it. You shall see a new browser and click the play. "
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/ps2metadata/",
	"title": "Detect the PS2 metadata",
	"tags": [],
	"description": "",
	"content": "How to acquit the ps2 metadata for KB or Mouse. Step1: Power off both Dex-100 \u0026amp; host-machine Step2: Boot up the Dex-100 and run the Dbgview as administrator Step3: Enable the function to follow the below image Step4: Setting the diffrent filter-string Step5: Power on the host-machine Step6: Record the logs by Dbgview Step7: Save the log Troubleshooting for DebugView DebugView download Power off both of Dex-100 and host-machine The Dex-100 choices the USB or PS2 protocol by both keyboard and mouse successful connection with host-machine, but the handshake is beginning when power on for PS2 mode.\nBoot up the Dex-100 and run the Dbgview as administrator Enable the function to follow the below image DEX-100-wiring-usb Setting the diffrent filter-string Key-in the different filter-string for collecting the data of the mouse or keyboard and click ok.\nFor log keyboard matamata : KBD\nFor log Mouse matamata : KVM2\nYou can click the button and choose each string for collecting the data of the mouse or keyboard.\nPower on the host-machine Dex-100 choices the PS2 mode when hand-shake is successful both with the PS2 mouse and host-machine after power on the host-machine.\nDex-100 has only one rule for choosing PS2 or USB protocol by detecting the ps2 mouse.\nRecord the logs by Dbgview You can see some metadata to show on the Debug Print , if it is successful.\nIt recordes logs when moving mouse or key-in any keyboard.\nPlease record both of Bypass mode \u0026amp; control mode.\nSave the log Save the log and sent back for Adlink contact window.\nTroubleshooting for DebugView Please follow the troubleshooting when you see the warning message\nRemove the Dbgv.sys as administrator or renmae it. Path : C:\\Windows\\System32\\drivers\nRetry step1 to step7. Attachment DebugView(save as to your computer)\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/disablemouseaccelerator/",
	"title": "Disable Mouse accelerator",
	"tags": [],
	"description": "",
	"content": "How to turn off the Mouse accelerator? Step0: Disable Mouse accelerator by UI Step1: Run the Registry Editor Step2: Change the Mouse speed setting How to turn off the Mouse accelerator for Linux ? Disable the mouse acceleartor in Terminal Disable Mouse accelerator by UI in local machine Wake up the Mouse Properties and disable the \u0026ldquo;Enhance pointer precision\u0026rdquo; in the local machine\nPath:Control Panel\\All Control Panel Items\\Mouse\nPlease follow the next step if you can\u0026rsquo;t find the UI of Mouse properties.\nRun the registry editor in local machine Using the hot-ky \u0026ldquo;Win+R\u0026rdquo;\nWake up the Registry Editor\nChange the mouse speed setting in local machine Edite the Mouse speed setting from 1 to 0. Define 1 = Enable 0 = Disabl\nPath:HKEY_CURRENT_USER\\Control Panel\\Mouse\\MouseSpeed\nDisable the mouse acceleration by command line in terminal for Linux Step 1 : Call the Terminal by the hotkey \u0026ldquo;Alt + Ctrl + T\u0026rdquo;. Step 2 : Key-in the command line \u0026ldquo;xset m 1 1\u0026rdquo; Sometimes your terminal is not the admin, maybe you can change the command line, \u0026ldquo;Sudo xset m 1 1\u0026rdquo; , if you know the admin\u0026rsquo;s password.\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/",
	"title": "Euresys",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Euresys Tutorials about Easy Tool series\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/",
	"title": "Euresys Grab card",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Euresys Grab Card Tutorials about the Grablink for Cameralink\nTutorials about the Coaxlink for CoaXPress\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/cxp-for-camera-triggering-with-encoder/",
	"title": "Using Euresys Coaxlink with Terminal Board  for Camera Triggering",
	"tags": [],
	"description": "",
	"content": "\nTerminal Board Signal Wiring and Frame Grabber System Architecture Overview Wiring External Input Signals to the Terminal Board Wiring Strob Output from the Terminal Board Trigger Camera Control Method Configuration in the Device Tab (Step 1) Setting the TriggerSource (Step 2) Setting the InputTool (Step 3) Verifying Operation with EventCount (Step 4) Setting the Strobe Output (Step 5) Setting the Cycle Trigger Source(Step 6) Setting Divider with External Signals Terminal Board Signal Wiring and Frame Grabber System Architecture Overview This section will explain how to connect the Terminal Board to the Coaxlink\u0026rsquo;s I/O board and verify signal reception. CXP Series \u0026amp; Grablink Duo I/O Connector (Pin Define) | Terminal Board (Pin Define)|Terminal Board Datasheet\nYou can find more information in the official Euresys documentation.\nCXP Series \u0026amp; Grablink Duo I/O Connector Pin Signal Usage 1 GND Ground 2 DIN12+ High-speed differential input #12 – Positive pole 3 IIN11+ Isolated input #11 – Positive pole 4 IIN13- Isolated input #13 – Negative pole 5 IIN14- Isolated input #14 – Negative pole 6 IOUT12- Isolated contact output #12 – Negative pole 7 GND Ground 8 Not connected 9 GND Ground 10 GND Ground 11 DIN12- High-speed differential input #12 – Negative pole 12 IIN11- Isolated input #11 – Negative pole 13 IIN12+ Isolated input #12 – Positive pole 14 IIN13+ Isolated input #13 – Positive pole 15 IIN14+ Isolated input #14 – Positive pole 16 IOUT12+ Isolated contact output #12 – Positive pole 17 TTLIO12 TTL input/output #12 18 GND Ground 19 DIN11- High-speed differential input #11 – Negative pole 20 DIN11+ High-speed differential input #11 – Positive pole 21 IIN12- Isolated input #12 – Negative pole 22 IOUT11- Isolated contact output #11 – Negative pole 23 IOUT11+ Isolated contact output #11 – Positive pole 24 GND Ground 25 TTLIO11 TTL input/output #11 26 +12V +12 V Power output Back to Wiring Map Back to Top\nTerminal Board Pin Define ( CXP Series \u0026amp; Grablink Duo) Pin Signal Usage Pin Signal Usage 1 +12V +12 V Power output 14 GND Ground 2 DIN11+ High-speed differential input #11 – Positive pole 15 DIN11- High-speed differential input #11 – Negative pole 3 DIN12+ High-speed differential input #12 – Positive pole 16 DIN12- High-speed differential input #12 – Negative pole 4 GND Ground 17 GND Ground 5 IIN11+ Isolated input #11 – Positive pole 18 IIN11- Isolated input #11 – Negative pole 6 IIN12+ Isolated input #12 – Positive pole 19 IIN12- Isolated input #12 – Negative pole 7 IIN13+ Isolated input #13 – Positive pole 20 IIN13- Isolated input #13 – Negative pole 8 IIN14+ Isolated input #14 – Positive pole 21 IIN14- Isolated input #14– Negative pole 9 Not connected 22 GND Ground 10 IOUT11+ Isolated contact output #11 – Positive pole 23 IOUT11- Isolated contact output #11 – Negative pole 11 IOUT12+ Isolated contact output #12 – Positive pole 24 IOUT12- Isolated contact output #12 – Negative pole 12 TTLIO11 TTL input/output #11 25 GND Ground 13 TTLIO12 TTL input/output #12 26 GND Ground Back to Wiring Map Back to Top\nWiring External Input Signals to the Terminal Board In this case, as encoder signals are being used, the High-speed differential input #11 has been selected as the connection point for its capability to handle high-speed signals. For signals that are not high-speed, standard input ports can be utilized. It\u0026rsquo;s important to select an appropriate input port based on the signal speed to ensure optimal performance and accuracy of the system.\nBack to Top\nWiring Strob Output from the Terminal Board Trigger Camera Control Method The Coaxlink card supports two modes for camera triggering:\rNC Mode (Freerun):\rThe camera captures images continuously at its own pace, independent of external signals.\rRG Mode (Asynchronous): The frame grabber dictates the camera's cycle rate and exposure, synchronizing it with external triggers like an encoder.\rRG Mode is ideal for situations requiring precise timing control, such as high-speed imaging or synchronized event capture. Select the mode that best fits your application\u0026rsquo;s requirements.\nConfiguration in the Device Tab (Step 1) Adjust the following parameters in the Device Tab to configure the RG mode:\rSet the Camera Control Method to RG.\rEnable Exposure Readout Overlap to allow the exposure and readout processes to overlap, optimizing the camera cycle time.\rSet the Cycle Trigger Source to Immediate, which triggers the camera cycle without waiting for an external trigger.\rThe Cycle Trigger Source is a critical setting. Choose LIN1 (see Step 3 for detailed settings) or Divider as the source. It's essential to set this parameter correctly for proper system operation.\rCalculating the Cycle Minimum Period To ensure the proper camera cycle rate, calculate the Cycle Minimum Period with this formula:\rFor example, if your camera's frame rate (FPS) is 40, the calculation would be:\rCycle Minimum Period = 1 / 40 FPS = 0.025 seconds = 25 milliseconds (ms)\rGiven the 25 ms Cycle Minimum Period, set both the Exposure Time and the Strobe Duration to 20 milliseconds (20000 microseconds) to ensure they remain within this operational limit. This keeps the camera's exposure and illumination well-coordinated and prevents exceeding the maximum allowed cycle time.\rExposure Time = 20000 microseconds (20 ms), which is less than the 25 ms limit.\rStrobe Duration = 20000 microseconds (20 ms), which also does not exceed the 25 ms limit.\rBack to Top\nSetting the TriggerSource(Step 2) Different camera manufactures might use different names for their parameters.Always check with the camera manufacturer on parameters to set.\nBack to Top\nSetting the InputTool (Step 3) When configuring the InputTool in your system, it\u0026rsquo;s important to understand the function of each parameter:\nLineInputToolSelector: This parameter serves as a central reference for the InputTool configuration, encompassing the selected source and activation mode for the input. Once set, LineInputToolSelector ensures consistency across the system; any changes made to this parameter will automatically update all related settings that reference it. This feature streamlines the process of modifying the input source or trigger method, as adjustments need only be made once at this central point.\rLineInputToolSource: This parameter defines the source of the input signal. It can be used to specify where the signal is coming from, such as a specific hardware port or an internal source.\rLineInputToolActivation: This parameter sets the activation mode of the input tool. It determines how the tool reacts to the input signal, such as triggering on a rising edge, falling edge, or other condition.\rProperly configuring these parameters ensures that your system accurately responds to external signals and triggers. Back to Top\nVerifying Operation with EventCount (Step 4) EventSelector:\rThis parameter allows you to select the source for event monitoring. You can set it to LIN1 (as described in Step 3) or choose Divider as the source. Selecting the correct EventSelector enables you to track specific events relevant to your input configuration.\r\u0026lt;li\u0026gt;\u0026lt;span style=\u0026quot;background-color: yellow;\u0026quot;\u0026gt;EventCount:\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt;\rThis parameter displays the count of the events detected. A value in EventCount indicates successful operation. For consistent movements over fixed distances, you will observe an equivalent accumulation in the EventCount value.\r\u0026lt;li\u0026gt;\u0026lt;span style=\u0026quot;background-color: yellow;\u0026quot;\u0026gt;EventCountReset:\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt;\rThis parameter can be used to reset the EventCount value. It is useful for clearing the count to facilitate easier tracking and calculations of new events.\rClick the button shown in the figure below to retrieve the current counter value.\nBack to Top\nSetting the Strobe Output (Step 5) LineSelector [Selector]:\rThis parameter is set to TTLIO11 to select the corresponding line for strobe output configuration.\r\u0026lt;li\u0026gt;\u0026lt;span style=\u0026quot;background-color: yellow;\u0026quot;\u0026gt;LineMode:\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt;\rThe line mode is set to Output, which configures the selected line to act as an output, driving the strobe signal.\r\u0026lt;li\u0026gt;\u0026lt;span style=\u0026quot;background-color: yellow;\u0026quot;\u0026gt;LineSource:\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt;\rThe source for the line is set to Device0Strobe\rSetting Divider with External Signals Most encoders emit high-frequency signals, which are often too rapid for direct camera triggering. Using a divider, these signals can be adjusted to lower frequencies, suitable for stable camera activation, ensuring accurate and consistent image capturing.\nDividerToolSelector:\rThis parameter acts as the central identifier for the Divider Tool configuration, integrating the selected source (specified by DividerToolSource), the activation control (determined by DividerToolEnableControl), and the division factor (set by DividerToolDivisionfactor) for the input. Once DividerToolSelector is configured, it guarantees consistency across the system; any subsequent modifications to the source, activation, or division factor are centralized through this single parameter. This consolidation simplifies the adjustment process, enabling changes to be made in one place that then propagate to all related configurations.\rDividerToolSource: Select LIN1 for the source, as previously described in Step 3.\rDividerToolEnableControl: Set this to 'Enable' to activate the divider tool.\rDividerToolDivisionfactor: Enter the division factor value you wish to use for frequency division.\rOnce you have configured the Divider Tool, remember to revisit Step 1 to adjust the \u0026lsquo;Cycle Trigger Source\u0026rsquo; accordingly. Additionally, you can verify the functionality by modifying the \u0026lsquo;EventSelector\u0026rsquo; in Step 4. These steps ensure that your settings are consistent and functional across the system.\nBack to Top\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/deep-learning-studio/",
	"title": "Deep Learning Studio",
	"tags": [],
	"description": "",
	"content": "Software version WHITE PAPER STUDIO DATASHEET Deep Learning Studio AT A GLANCE Ease the evaluation of Open eVision\u0026rsquo;s Deep Learning tools Dataset creation and image annotation for classification, segmentation, and object localization Create and configure dataset splits to decide how your images are used Manage the data augmentation transformations Train your tools in succession thanks to the training queue Validation and analysis of the results of the trained tools Available on Windows and Linux Free of charge Deep Learning Studio is a Visualization Tool Deep Learning Studio has a complete visualization training tool that can be used for proof of concept (POC) without requiring a license. The general workflow is as follows:\n[Project] (#1) Labeling Splitting Augmentation Training Tool Validation Project In the project management of Deep Learning Studio, it is possible to create an AI model from five different models and individually set project names based on requirements. Labeling You can import your collected dataset and freely create the number of labels in Deep Learning Studio. Additionally, the software automatically compresses the images without the need for third-party tools to perform resizing operations. Splitting Deep Learning Studio allows you to split your dataset into training, validation, and test sets. You can create multiple dataset splits to experiment and check the performance of tools trained with different set of images. Augmentation Deep Learning Studio integrates annotation tools adapted to each library. For classification and unsupervised segmentation, you can quickly assign label to each image. For supervised segmentation, the segmentation editor allows you to draw the ground truth segmentation. For localisation, the object editor allows you to quickly draw the bounding box around each of your objects. Training Tool The Tools tab allows you to configure and train your tools. Operating on CPU or GPU, the training can be stopped and restarted at any time. You can launch as many training as you want thanks to the processing queue. The training and inference operations will be queued and processed one after the other. Validation The validation process is customized for each library to allow you to get the most out of your data. A comprehensive set of metrics, tables, and/or graphs is available to analyze and explore the results of the training process.\nTables and confusion matrixes allow you to filter your results to understand the strengths and weaknesses of the trained models. Score histograms and ROI curves are useful to select a threshold and adapt the trained models to your needs. "
},
{
	"uri": "https://aiot-ist.github.io/euresys/dongle/",
	"title": "Dongle &amp; License ID",
	"tags": [],
	"description": "",
	"content": " Dongle rule License ID Alternative solutions for PAR EOL Neo Licensing System Neo is the new Licensing System of Euresys. It is reliable, state-of-the-art, and is now available to store Open eVision and eGrabber licenses. Neo allows you to choose where to activate your licenses, either on a Neo Dongle or in a Neo Software Container. You buy a license, you decide later. Neo Dongles offer a sturdy hardware and provide the flexibility to be transferred from a computer to another. Neo Software Containers do not need any dedicated hardware, and instead are linked to the computer on which they have been activated. Neo ships with its own, dedicated, Neo License Manager, which comes in two flavours: an intuitive, easy to use, Graphical User Interface and a Command Line Interface that allows for easy automation of Neo licensing procedures. Dongle-rule ![Normal machine](/Euresys/Dongle/images/Dongle rule.png)\nLicense ID (NEO、Open eVision 、eVision) Dongle \u0026 License ID\rLibrary\rOpen eVision Licenses\reVision Licenses\rNeo Dongle\n6514\rUSB Dongle\n6512\rPAR Dongle\n6513\rHost(Software)\n(EOL)\rUSB Dongle\n6512\rPAR Dongle\n6513\rBoard\rInspection Bundle (EasyImage,EasyGauge,EasyMatch,EasyObject,EasyColor)\r4314\r4164\r4214\r4264\r4014\r4064\r4114\rEasyMatrixCode\r4307\r4157\r4207\r4257\r4007\r4057\r4107\rLegacy Mark Inspection Bundle\r4165\r4215\r4265\r4015\r4065\r4115\rEasyFind\r4308\r4158\r4208\r4258\r4008\r4058\r4108\rEasyMatch\r4303\r4153\r4203\r4253\r4003\r4053\r4103\rEasyGauge\r4309\r4159\r4209\r4259\r4009\r4059\r4109\rEasyImage\r4301\r4151\r4201\r4251\r4001\r4051\r4101\rEasyOCR\r4305\r4155\r4205\r4255\r4005\r4055\r4105\rLegacy ID Bundle 1.2\r4176\r4226\r4276\rEasyQRCode\r4325\r4175\r4225\r4275\rFull Bundle\r4327\r4177\r4227\r4277\r4017\r4067\r4117\rEasy3D\r4331\r4181\r4231\r4281\rEasy3DObject\r4333\r4183\r4233\r4283\rEasy3DMatch\r4334\r4184\r4234\r4284\rEasy3DLaserLine\r4336\r4186\r4236\r4286\rEasyObject\r4302\r4152\r4202\r4252\r4002\r4052\r4102\rEasyColor\r4304\r4154\r4204\r4254\r4004\r4054\r4104\rEasyOCR2\r4329\r4179\r4229\r4279\rEasyBarCode\r4310\r4160\r4210\r4260\r4010\r4060\r4110\rEasyOCV\r4306\r4156\r4206\r4256\r4006\r4056\r4106\rDeep Learning Bundle\r4332\r4182\r4232\r4282\r3D Bundle\r4335\r4185\r4235\r4285\rID Bundle\r4330\r4180\r4230\r4280\rLegacy Inspection Bundle\r4016\r4066\r4116\rLegacy EasyMeasure\r4011\r4061\r4111\rLegacy SDK\r4178\reGrabber Gigelink\r4400\reGrabber Recorder\r4401\rEasySpotDetector\r4340\r4190\rEasyDeepOCR\r4341\r4191\rAlternative solutions for PAR EOL Due to the official announcement from Euresys that PAR Dongle is being phased out, USB Dongle will be recommended as a replacement in the future. Below are two suggested solutions:\n1. Please use USB Dongle with the dedicated part number for Open eVision Licenses. For example: Original solution: Inspection Bundle + PAR Dongle, 4214 + 6513\nAlternative solution: Inspection Bundle + USB Dongle, 4164 + 6512\n2. Please use USB Dongle with the dedicated part number for eVision Licenses. For example: Original solution: Inspection Bundle + PAR Dongle, 4064 + 6513\nAlternative solution: Inspection Bundle + USB Dongle, 4014 + 6512\nBoundle detail list Number ID Function Items 4332 Deep Learning Bundle EasyClassify, EasySegment(u/s), EasyLocate 4337 Full Bundle EasyImage, EasyGauge, EasyFind, EasyMatch, EasyObject, EasyColor, EasyOCR, EasyOCR2, EasyBarCode, EasyMatrixCode, EasyQRCode 4335 3D Bundle Easy3D, Easy3DLaserLine, Easy3DObject, Easy3DMatch 4314 Inspection-Bundle EasyImage, EasyGauge, EasyMatch, EasyObject, EasyColor 4330 ID Bundle EasyImage, EasyOCR, EasyOCR2, EasyBarCode, EasyMatrixCode, EasyQRCode "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easycolor/",
	"title": "EasyColor",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyColor by Euresys\nPerforming Thresholding on Color Images\nPerforming Color Segmentation\nPerforming Thresholding on Color Images Following this tutorial, you will learn how to use EasyColor to segment a color source image, by setting a threshold value for each color component of the current color system. For example, to retrieve the solder pads on a PCB, you\u0026rsquo;ll perform a color segmentation based on the golden pixels (H), with a loose discrimination on the brightness (L) and saturation (S), to eliminate surface and lighting effects.\nLoad the source image Create a destination image Create a color lookup table Perform the color segmentation Link on line Doc Performing Color Segmentation Following this tutorial, you will learn how to use EasyImage to perform color segmentation. Load the source image Create a color lookup table Perform the color segmentation Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/",
	"title": "EasyDeepLearning",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start DeepLearning by Euresys\nEasyClassify\nEasySegment\nEasyLocate\nThere are 3 functions for deep learning studio EasyClassifier EasySegmentation 2.1 Unsupervised 2.2 Supervised EasyLocate 3.1 Bounding Box 3.2 Interest Point ![Normal machine](/Euresys/EasyDeepLearning/images/DeepLearning overall.png) EasyClassify Link on line Doc\nEasysegment Link on line Doc\nSupervised Unsupervised EasyLocate "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/",
	"title": "EasyFind",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\nDetecting Highly-Degraded Occurrences of a Reference Model in Multiple Files\nImproving the Score of Found Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\nDetecting Highly-Degraded Occurrences of a Reference Model in Multiple Files Following this tutorial, you will learn how to use EasyFind to detect in multiple images highly-degraded occurrences of a reference model. The degradation can be due to noise, blur, occlusion, missing parts or unstable illumination conditions.\nLoad the reference image Create an ROI to define the reference model on the reference image Learn the reference model Set rotation and scaling tolerances Select multiple images Browse multiple images Link on line Doc Improving the Score of Found Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyFind to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; in geometric pattern matching. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image, such as text and numbers. Loading the reference image Creating an ROI to define the reference model on the reference image Learning the reference model Setting a rotation tolerance Detecting instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Defining the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detecting instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/qa/",
	"title": "EasyFind (Q&amp;A)",
	"tags": [],
	"description": "",
	"content": "Q\u0026amp;A list How to reduce the working time for EPatternFinder.Find processing time\nHow to reduce the working time for EPatternFinder.Find processing time Description the scenario: Users would like to reduce the working time or give up the execution when having a high processing time.\nFundamental for EasyFinder EasyFind works internally in 2 stages.\nIt selects reasonable candidates for the pattern It makes a finer analysis/positioning of the candidates. Suggestion way There is a new parameter that may help by using the number of selected candidates has a direct impact on the processing time. Link on line Doc\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easygauge/",
	"title": "EasyGauge",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\nMeasuring the Rotation Angle of an Object\nMeasuring the Diameter of a Circle\nMeasuring a Distorted Rectangle\nLocating Points Regarding to a Coordinate System\nUnwarping a Distorted Image\nMeasuring the Rotation Angle of an Object Following this tutorial, you will learn how to use EasyGauge to measure the rotation angle of a CCD sensor package. As we only need to retrieve an angle value, it\u0026rsquo;s not required to work in a calibrated field of view. All geometrical parameters and results will be express as numbers of pixels.\nLoad the source image Attach a line gauge to the image Perform the inspection Link on line Doc Measuring the Diameter of a Circle Following this tutorial, you will learn how to use EasyGauge to measure the diameter of a circle in an image. Load the calibration image Calibrate the field of view Load the source image Attach a circle gauge to the image Perform the inspection Link on line Doc Measuring a Distorted Rectangle Following this tutorial, you will learn how to use EasyGauge to perform measurements on a distorted rectangle component.. Load the calibration image Calibrate the field of view Load the distorted image Attach a rectangle gauge to the image Perform the inspection Link on line Doc Locating Points Regarding to a Coordinate System Following this tutorial, you will learn how to use EasyGauge to perform lead frames inspection. This operation determines the dimension, position, curvature, size, angle or diameter of the lead frames with an excellent accuracy. Robustness is ensured by powerful edge-point selection mechanisms that are intuitive and easy to tune, allowing measurement in cluttered images. Load the calibration image Calibrate the field of view Loading a lead frame image Setting a coordinate system Attaching a point gauge to the frame shape Attaching other point gauges to the frame shape Loading another lead frame image Tuning the coordinate system Performing the inspection Link on line Doc Unwarping a Distorted Image Following this tutorial, you will learn how to use EasyGauge to perform grid calibration, and unwarp a distorted image. Load the calibration image Calibrate the field of view Load the distorted image Unwarp the distorted image Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyimage/",
	"title": "EasyImage",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyImage by Euresys\nTransforming a Gray-Level image into its Black and White Edges Extracting an Object Contour Detecting the Corners of an Object Using Harris Corner Detector Detecting a Horizontal or Vertical Line Using Projection Creating a Flexible Mask \u0026amp; Computing Gray-Level Statistics Using a Flexible Mask Detecting the Corners of an Object Using Hit-and-Miss Transform Extracting a Vector Using Profile Function Enhancing an X-ray image Correcting Non-Uniform Illumination Correcting Shear Effect Correcting Skew Effect Video Function_1 Thresholding Following this tutorial, you will learn how to use EasyImage to convert a gray-level source image into a binary destination image. Thresholding an image transforms all the gray pixels into black or white pixels, depending on whether they are below or above a specified threshold. Thresholding an image makes further analysis easier. How to use the singal threshold How to use the dual-threshold How to use the Adaptive Threshold Link on line Doc Function_2 Extracting an Object Contour Following this tutorial, you will learn how to use EasyImage to trace an object outline in a gray-level image. The contour extraction allows you to get in a path vector all the points that constitute an object contour, just by clicking an edge of this object. Load the source image Set the destination vector Extract the contour Link on line Doc Function_3 Detecting the Corners by Using Harris Corner Detector Following this tutorial, you will learn how to use EasyImage to detect the corners of an object. The detection uses the Harris corner detector algorithm. Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transform Link on line Doc Function_4 Detecting a Horizontal or Vertical Line Using Projection Following this tutorial, you will learn how to use EasyImage to detect defects (horizontal/vertical line) in a gray-level image. Load the source image Set the destination vector Detect the defects Link on line Doc Function_5 Computing Gray-Level Statistics Using a Flexible Mask Following this tutorial, you will learn how to compute gray-level statistics on an arbitrary-shaped area only. Load the source image Invert the image Threshold the image Save the flexible maskLink on line Doc Load the flexible mask image Apply the flexible mask on the source image Compute the gray-level statisticsLink on line Doc Function_6 Detecting the Corners of an Object Using Hit-and-Miss Transform Following this tutorial, you will learn how to use EasyImage to detect top corners in an image, using the hit-and-miss transform. Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transformLink on line Doc Function_7 Extracting a Vector Using Profile Function Following this tutorial, you will learn how to use EasyImage to detect scratches. Load the source image Set the destination vector and detecting the scratchesLink on line Doc Enhancing an X-ray image Following this tutorial, you will learn how to use EasyImage to enhance an X-ray image. Load the source image Set the convolution parameterLink on line Doc Correcting Non-Uniform Illumination Following this tutorial, you will learn how to use EasyImage to correct non-uniform illumination in an image. Load the source image Load the reference image Perform the correctionLink on line Doc Correcting Shear Effect Following this tutorial, you will learn how to use EasyImage to correct a shear effect in an image. The following image is taken by a line-scan camera. The camera sensor was misaligned, resulting in a so-called shear effect. Load the source image Create a destination image Set the pivots parameters Link on line Doc Correcting Skew Effect Following this tutorial, you will learn how to use EasyImage to correct skew effect in an image. Load the source image Creating a destination image Setting the correction angle Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easymatch/",
	"title": "EasyMatch",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyMatch by Euresys\nLearning a Pattern According to an ROI\nImproving the Score of Matching Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\nLearning a Pattern According to an ROI Following this tutorial, you will learn how to use EasyMatch to learn a model from an ROI in a source image, and to perform pattern matching on the same image.\nLoad the source image Define an ROI Learn a model from the ROI Match the pattern Link on line Doc Improving the Score of Matching Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyMatch to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo;. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image. Load the source image Learn the reference model Detect instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Define the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detect instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyobject/",
	"title": "EasyObject",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyObject by Euresys\nRemoving Non-Significant Objects After Image Segmentation\nDetecting Differences Between Images Using Min-Max References\nDetecting Printing Errors Using a Flexible Mask\nRemoving Non-Significant Objects After Image Segmentation Following this tutorial, you will learn how to use EasyObject to detect bad rice grains (largely dark) among many normal rice grains (largely light).\nLoad the source image Perform image segmentation Remove the smallest objects Remove the smallest objects Link on line Doc Detecting Differences Between Images Using Min-Max References Following this tutorial, you will learn how to use EasyObject to compare images. In this example, we will check the quality of a PCB film. Load the source image Build min and max reference images Load an image to be inspected Compare the image with the reference images Link on line Doc Detecting Printing Errors Using a Flexible Mask Following this tutorial, you will learn how to use a flexible mask to target and search specific areas in the image. Load the source image Load the flexible mask image Inspect the image Link on line Doc "
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/bayer2rgb/",
	"title": "Euresys Bayer to RGB by FPGA",
	"tags": [],
	"description": "",
	"content": "Euresys Bayer to RGB Conversion by FPGA Peak Pixel Processing Rate Firmware Version Applies to Frame-Grabber Cards Peak Pixel Processing Rate 1 i. 3620 Coaxlink Quad CXP-12 JPEG (4-camera) ii. 3620-4 Coaxlink Quad CXP-12 JPEG (4-camera) 500,000,000 pixels/sec 2 i. 1633 Coaxlink Quad G3(2-camera), (2-camera, bayer)\nii. 1633-LH Coaxlink Quad G3 LH(2-camera), (2-camera, bayer) 1,000,000,000 pixels/sec 3 i. 1633 Coaxlink Quad G3(1-camera) ii. 1633-LH Coaxlink Quad G3 LH (1-camera) iii. 1635 Coaxlink Quad G3 DF(1-camera), (1-df-camera) iv. 3602 Coaxlink Octo (2-camera) v. 3622 Coaxlink Duo CXP-12(1-camera) vi. 3622-LH Coaxlink Duo CXP-12 LH (1-camera) 1,108,000,000 pixels/sec 4 i. 3602 Coaxlink Octo (1-camera) ii. 3603 Coaxlink Quad CXP-12 (1-camera) . 3603-4 Coaxlink Quad CXP-12 (1-camera) iii. 3625 Coaxlink QSFP+ (1-camera) 2,216,000,000 pixels/sec Calculate Maximum Frame Rate Please select the Firmware Version:\nFirmware Version: Please select\r1\r2\r3\r4\rCamera Resolution (MPixel): MB\nMaximum Frame Rate:\rFPS\r"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/ffc/",
	"title": "Euresys Grabber Card with FFC Support (FFC)",
	"tags": [],
	"description": "",
	"content": "FPS Calculation with FFC Enabled\nIntroduction to Flat Field Correction (FFC) The Flat-field correction (Wikipedia: FFC) is a method used to correct:\nThe differences of light sensitivity between the pixel sensors of a camera some artifacts related to the optical system (e.g., non-uniform lighting and vignetting) The goal is to correct the pixels of the captured (raw) images in such a way that when a uniform background is captured by the system (camera \u0026amp; lens), the resulting output image is uniform.\nBelow link shows the cards and variants that supports FFC Link\nMaximum image size for all supported pixel foramts when FFC enabled Please verify if the camera output, which is either monochrome or RGB interface, is within the specified limits as shown in the diagram.\rSustainable Relative Data Rate A 4-lane CXP-6 maximum data rate is 2500MB/s\rCalculate For a Quad CXP-12, the sustainable relative data rate 123.2% which is 2500MB x 1.232 = 3080 MB/s If your resolution is 7K x 9K, Then the max fps will be about 3080MB/7K x 9k = 48 fps\nFrame grabber card\rNone\rCXP-6\rCXP-12\rBit depth\rNone\r8位元\r10位元\r12位元\r14位元\r16位元\rPixel format (Camera to Grabber) None\rBayer\rRGB\rImage(V):\rImage(H):\r2500MB\r*\rRatio\r=\rsustainable data rate (MB/s)\rSustainable data rate (MB/s):\r/\rImage(V):\r/\rImage(H):\r=\rFPS\rFFC Wizard Sample Program (Quickly start) Euresys provides the source code of a sample application, called ffc-wizard, that computes the coefficients and packs them in a binary file targeting the frame grabber.\nThe purpose of this sample code is threefold:\nguide the user through the calibration procedure; provide a technical and practical translation of what\u0026rsquo;s described in this document; provide building blocks for developing custom applications. Link\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/dongle/how2activate/",
	"title": "How to activate the license",
	"tags": [],
	"description": "",
	"content": " How to activate the Neo-license in NEO Dongle How to activate the Open-Evision-license in USB dongle How to activate the Evision-license in USB dongle How to activate the Neo-license in Neo Dongle Your browser does not support the video tag.\rHow to activate the Open-Evision-license in USB dongle Your browser does not support the video tag.\rHow to activate the Evision-license in USB dongle Evision (Download) "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/",
	"title": "Neon-2000-JT2",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision NEON-2000-JT2 Discover how to use the NEON-2000-JT2 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/parameters/",
	"title": "Parameters",
	"tags": [],
	"description": "",
	"content": " Color Lookup Color Lookup IndexBits, the number of table entries and the corresponding table size are given below: Here are grouped images of those full RGB palettes by 4 bits \u0026amp; 5 bits \u0026amp; 6 bits.\n4 bits = 12-bit RGB\r5 bits = 15-bit RGB\r6 bits = 18-bit RGB\rReference https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats#4-bit_grayscale_2\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/gettingstart/",
	"title": "Getting start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with Neon-iSeries from un-boxing.\nHardware wiring Inference by NEON Run DI/O Sample Hardware wiring The NEON-2000-JT2 DC power source can be either from a USB Type-C adaptor or DC jack. The USB Type-C connector also supports a DisplayPort video signal and USB3, which can be used to connect a keyboard and mouse. The following figures show examples of possible power and peripheral connection configurations.\nSeparate Power and Peripheral Connections This configuration requires an ADLINK AC/DC power adapter (P/N 31-62156-1000-A0). Combined Power and Peripheral Connections This configuration requires an ADLINK USB Type-C hub/adapter (P/N 92-99090-1010). Inference by NEON The inference sample, Capture_and_inference_Sample, is built on the Desktop of NEON-2000-JT2.\ninference by image\n./imagenet-console [input_image][output_image] cd ~/Desktop/Neon_Samples/Neon-20xB/C++/Capture_and_Inference/build/aarch64/bin\r./imagenet-console ./aarch64/bin/jellyfish.jpg ./aarch64/bin/output.jpg inference by camera\n./imagenet-camera [input_width] [input_height] cd ~/Desktop/Neon_Samples/Neon-20xB/C++/Capture_and_Inference/build/aarch64/bin\r./imagenet-camera 1920 1080 Run DIO Sample Get hardware and DI/O information cd /usr/src/Neon/Sample/Neon_Information\rsudo ./Neon_Information Get hardware and DI/O information by python cd /usr/src/Neon/Sample/Neon_Python\rsudo python3 Neon.py Note Due to dual function of DO0 and DI0, the default function of DO0 is strobe out and DI0 is trigger in. If you want to set them as general DO and DI. Please configure it as following: DIO0Config 1: Set DIO0 as GPDI and GPDO0 DIO0Config 0: Set DIO0 as Trigger in and Strobe out (default) sudo /usr/src/Neon/Sample/Neon_Setting/NeonSet DIO0Config 1\nCheck the information of DIO.\nsudo /usr/src/Neon/Sample/Neon_Information/NeonInformation\nThe way that using the DIO functionality of the Neon camera without sudo privileges. cd /sys/class/neon_dio\rsudo chown adlink:adlink DO_0\rsudo chown adlink:adlink DO_1\rsudo chown adlink:adlink DO_2\rsudo chown adlink:adlink DO_3 "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page cover the process to flash a new operating system onto a NEON device.\nThere are 2 methods to flash the NEON-2000-JT2 and NEON-2000-JT2-X:\nFlash over USB Clone image to microSD card Other useful information about flashing a Jetson based device can be found here.\nUSB Method This method uses a host machine to flash the internal eMMC storage over USB.\nTo perform this method the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers Step 1: Download image to your host pc with Ubuntu Neon-2000-JT2 Jetpack 4.2.1 emmc image Jetpack 4.3 v1.0.0 emmc image Jetpack 4.3 v1.0.2 emmc image MD5:e70d52d564e09b11b76fa74314c96c79 Jetpack 4.4 v1.0.3 emmc image MD5:19ee6e9bed2247d5894c3e9066d20b2b Jetpack 4.6.1 v1.0.6 emmc image MD5:31a7ddc595ac588633f1ea64275c33b9 Jetpack 4.6.1 v1.0.7 emmc image(with EVA3.8.5 + EVA IDE) MD5:b3485b4cdcb1aa1de57b27be8efc6146 Neon-2000-JT2-X Jetpack 4.3 v1.0.2 MD5:ddf504b2c600175eacd61777f528b435 Jetpack 4.4 v1.0.2 MD5:00126a7170cbb82a38707da5ca3e25e3 Jetpack 4.4 v1.0.2 with EVA MD5:18e818021339e3204065d50b54e91dbd Jetpack 4.6.1 v1.0.6(with EVA3.8.5 + EVA IDE) MD5:1b3f971e278371ee3d8daa9b6fccd339 Step 2: Checksum Check md5 checksum to make sure image file is correct\nLinux\n$ md5sum [file] $ md5sum JT2X_JP461_emmc_v1.0.6.tar.gz Windows 10\n$ certutil -hashfile [file] MD5 $ certutil -hashfile JT2X_JP461_emmc_v1.0.6.tar.gz MD5 Step 3: Flash the image to NEON This step involves connecting the NEON camera to the Host machine and flashing the image.\nThere is also a video and PDF showing the process\nOn the Host PC unzip the file downloaded in Step 1\n$ tar -zxvf JT2X_JP461_emmc_v1.0.6.tar.gz or\n$ tar -jxvf A4_Linux_for_Tegra.20200528_JT2_JP43_v1.0.2.tbz2 Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode. Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again Connect the microUSB cable to the NEON and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd JT2X_JP461_emmc_v1.0.6 Flash the Neon-2000-JT2/Neon-JT2X sudo ./flash.sh -r jetson-tx2 mmcblk0p1 To flash the Neon-JT2(jetpack 4.6.1), Neon-JT2X(jetpack 4.6.1), Neon-2000-JNX sudo ./flash.sh Below is a video of the process to flash a NEON-2000-JT2 microSD Card Method This method involves cloning the operating system image for the NEON camera to a microSD, booting the NEON camera from this image and then optionally copying the image from the microSD card to the internal emmc storage on the NEON camera.\nRequired tools:\nmicroSD of at least 32GB microSD card reader If you plan on running the operating system from the microSD permanently, make sure to use a high quality microSD card to prevent corruption\nDownload image file: Jetpack 4.4 v1.0.3 microSD image MD5:9ccc55b9dec65b15eefee866e6a1fc85 Jetpack 4.4 v1.0.3 microSD image with EVA_IDE sample Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum JT2_JP44_microSD_v1.0.3.tar.gz Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile JT2_JP44_microSD_v1.0.3.tar.gz Unzip the file downloaded in Step 1 to get a .img file $ tar -zxvf JT2_JP44_microSD_v1.0.3.tar.gz Clone image file to microSD card using one of the following methods Ubuntu Disk Manager Video of process to clone image to microSD card Steps: Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher Once cloned, check the boot config file is set to boot from the SD card Insert SD card into a machine Open file /\u0026lt;path to sd card\u0026gt;/boot/extlinux/extlinux.conf Make sure the boot line is set to /dev/mmcblk2p1 APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait Insert SD card into the NEON camera Boot NEON Optional - Clone image from microSD card to internal eMMC in the NEON: Mount the internal eMMC lsblk sudo mkfs -t ext4 /dev/mmcblk0p1 lsblk -f sudo mkdir /media/adlink/ssd sudo mount /dev/mmcblk0p1 /media/adlink/ssd Clone image from microSD card to internal eMMC using rsync sudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd Change boot config on the internal eMMC to boot from the internal eMMC mmcblk0p1 Open file /media/adlink/ssd/boot/extlinux/extlinux.conf Replace the following # Replace this line\r# APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait\r# With this line\rAPPEND ${cbootargs} quiet Remove the SD card and reboot the NEON "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jt2/howtobackupneonibymicrosd/",
	"title": "How to backup Neon by microSD card?",
	"tags": [],
	"description": "",
	"content": " Clone Neon data to microSD card Backup microSD card as image Recovery microSD card by image According to the official document, the boot sequence is decided by U-boot. U-Boot functionality includes a default booting scan sequence. It scans bootable devices in the following order:\nExternal SD card Internal eMMC (Jetson TX2 series devices only) USB device (Jetson TX2 series devices only) NFS device U-Boot boots up the kernel by /boot/extlinux/extlinux.conf in the sequence of bootable device. That is to say, if NEON-2000-JT2 equits microSD card with OS, U-Boot boots up accord to /boot/extlinux/extlinux.conf in microSD card instead of extlinux.conf in eMMC.\nRequirements: 32G up microSD card NEON-2000-JT2 Clone Neon data to microSD card Step 1: Format your microSD card as ext4 The total size of image with Jetpack4.4 is 16G. It suggests cloning the image using a 32G microSD card.\nInsert microSD card in Neon Right click on microSD card folder and click Format\u0026hellip; Fill Volume Name:JP44 for example. Choose Type as Internal disk for use with Linux systems only (Ext4) Confirm details and click Format Search applications: Disks Select SD Card Reader -\u0026gt; Mount selected partition Step 2: Clone eMMC data to microSD Make sure NEON-2000-JT2 mounts your microSD card /dev/mmcblk2p1.\ndf -h Clone eMMC data to microSD\nsudo cp -ax / \u0026#39;/media/adlink/yourSDcard\u0026#39; \u0026amp;\u0026amp; sync Step 3: Modify extlinux.conf in microSD card for bootup sequence Edit /boot/extlinux/extlinux.conf in microSD card\nAPPEND ${cbootargs} quiet\nAPPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait\nStep 4: Reboot NEON-2000-JT2 It will boot from microSD card. Reference\nOptional Backup microSD card as image Step 1. Use command and check the disk of microSD in PC df -h Step 2. Image a card with dd if=path_of_your_image.img of=/dev/disk\nsudo dd bs=4M if=/dev/sdd status=progress | zip Neon-2000-JT2-JP43.zip - Recovery microSD card by image image for microSD, Jetpack 4.3 v1.0.2 MD5: ae005ea9624999fb0922c487322680d1s Step 1. Unzip the image unzip Neon-2000-JT2-JP43.zip Step 2. Rename it mv - unzip Neon-2000-JT2-JP43.image Step 3. Insert microSD and flash it by Etcher "
},
{
	"uri": "https://aiot-ist.github.io/dex/video/",
	"title": "02-Video Source",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Video Source There are some know how for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/",
	"title": "03-Mouse",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Mouse There are several note for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/",
	"title": "04-Dex-pro",
	"tags": [],
	"description": "",
	"content": "How to add the bmp(layer) and select the folder for project. How to use classic functions in the script.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/faq/",
	"title": "10-FAQ",
	"tags": [],
	"description": "",
	"content": "FAQ List Specifications of DI1 DI2 LED Batch kill dex-pro The mouse follows offset How to restart the MSMQ How to check why the dexpro can\u0026rsquo;t grab the video How to check the exception-event How to backup or restore the traineddata to other machines How to quckly select TesseractDB for mutil-ROI Specifications of DI1 DI2 Hardware Figure DI connector pin assignment DI1/2 function LED There are 3 LEDs in the hardware that can light by script command. kill the dex-pro by batch Please run the cmd as administrator\ninput the command taskkill /f /im DEX-Pro.exe\nThe mouse follows offset Check the machine PC’s mouse setting, Keep middle speed and disable “Enhance pointer precision”. If the offset distance is not fixed, adjust mouse speed of the machine PC How to restart the MSMQ If the Dex-por can\u0026rsquo;t work by MSMQ, Please follow the below steps for Troubleshooting\nGo to \u0026ldquo;control panel\u0026rdquo; Go to \u0026ldquo;Program\u0026rdquo; Go to \u0026ldquo;Turn Windows Features on or off\u0026rdquo; Disable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo; Reboot the DEX-100 Enable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo; How to check the setting when the dexpro can not grab the video If the Dex-pro can\u0026rsquo;t grab the video, We have several suggestions please follow checking the below steps.\nPlease checking the setting is Auto by the dex-pro tool of Fram Grabber Configuration. Please open the Diagnostic tool. Tool-Path \u0026#34;C:\\Program Files\\ADLINK\\DEX-100\\utility\u0026#34; You shall see both the result for Resolution and Signal if the connection is successful. If is no signal , please use below steps to help you troubleshooting. A. Check the Local machine setting for display resolution. B. Reconnect the local machine VGA cable to monitor. C. Reboot the local machine. D. Check the resolution from the monitor. E. Reconnect the VGA to Dex-100 F. Check Frame Grabber Setting whether the auto mode is selected for any resolution. How to check the exception-event You can track the Event Viewer if dex-pro encounters the Unexpected crashing.\n1. Enter CMD (Command Prompt)\r2. Key-in \u0026#34;eventvwr\u0026#34; Path : \u0026#34;Event Viewer ==\u0026gt; Windows Logs ==\u0026gt; System\u0026#34; How to backup or restore the traineddata to other machines A. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management B. Bakcup Select all Export Click Yes Select the folder ==\u0026gt; OK C. Restore Select all import (select the Folder name) Select the folder ==\u0026gt; Ok How to quckly select TesseractDB for mutil-ROI A. In the blf mode B. Select an image layer that has many ROI for OCR. C. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management D. Can be selected in batches and edit the ROI setting. E. Click the Replace F. Exit without change G. Save Script H. Recombine the script "
},
{
	"uri": "https://aiot-ist.github.io/dex/sdkfwversion/",
	"title": "11-SDK &amp; FW Version &amp; Manual",
	"tags": [],
	"description": "",
	"content": "SDK Internal version : 1.2.7 2022/03/28\nOffical version : V7\nSDK V7 (Download)\nRelease note\nAutoinstall Download\nPls unzip and double-click this .bat file.\nHow to install SOP\n1. Uninstall 2. Reboot 3. Install SDK 4. Reboot FW FW-C5 (Download) FW-C7 (Download)\nVision_history\nHow to intsll SOP\nUse this tool C:\\Program Files\\ADLINK\\DEX-100\\utility\\BurnDex100_x64 Check the version that is C5 or C7\nUpload the right bit\nNote: The FPGA shall be null when you install the error bit.\nCold boot Utility_Manual Utility_Manual_20210426 (Download)\nModules\u0026amp;EdgeServe DEX-100_SW_User_Manual (Download)\nHardware Hardware-Manual(Download)\nCE DEX-100 CE EMC TEST REPORT(Download)\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/",
	"title": "Vision Card",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory ADLINK Vision Card Find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with MCM-204 from un-boxing\nStep 1: Connect Power. Connect the positive and negative wires from a 9V to 30V DC power source to the terminal block. Step 2: Connect MCM-204 to PC or laptop. Connect MCM-204 to the PC or laptop by ethernet cable. Please make sure the PC/laptop network mode is under DHCP or static IP set at 169.254.x.x network segment.\nStep 3: Open the web browser to access the build-in web console. Open the web browser (chrome is recommanded) to access with the default IP http://169.254.1.1\nOnce acess MCM-204 web console sucessfully, the page will displayed like below. The default username is administrator and password is Adlink6166, after fill in these required information then click LOGIN to login to the web console. Step 4: Device Setting Click menu Device Setting to enter the device setting page. You could control the analog input setting and customize what kind of data you want to acquire. You could scroll the page to the bottom and simply click Apply to apply the default setting at this tutorial.\nAfter done, there will pop up a successful message to notify.\nStep 5: Data Capture Click menu Data Capture to enter the data display page and all the data set at last step will be displayed at this page.\nIf the data include the Voltage data type, you could click Draw to draw the voltage chart. "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/",
	"title": "Neon-2000-JNX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision Neon-2000-JNX Discover how to use the NEON-2000-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithm/",
	"title": "Custom Algorithm Deploy Sample",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to build your own algorithm and deploy to MCM-204 You could deploy your own algorithm at MCM-204.\nPrerequisites Operating System : Ubuntu 18.04 installed and up-to date Step 1: Download ARM gcc tool chain and examples. Please click below link to download zip file compressed of ARM gcc tool chain and examples at Ubuntu.\nDownload link: ARM tool chain with examples\nAfter downloaded, please extract it, then there will be a ARM tool chain folder and a sample folder called CalRMS.\nStep 2: Compile sample. Extract the downloaded file at Step 1.\n$ unzip CustomAlgorithm.zip Intall make, it\u0026rsquo;s not installed by default.\n$ sudo apt install make Change directory to the CalRMS\n$ cd CalRMS Build .so file from sample.\n$ make The customAlgo.so is build at current directory, you could use this file to upload to MCM-204.\n$ ls\rcustomAlgo.c customAlgo.h customAlgo.o customAlgo.so Makefile Step 3: Deploy the .so file to MCM-204 Once you completed above steps, you will get customAlgo.so. What we need to do next is deploy it to the MCM-204. Log in to MCM-204 web portal, if you don\u0026rsquo;t know how to login to web portal, you could click thie link to getting start page.\nAfter you login to web portal, then click System Setting menu. Scroll down to the bottom of page, then find the Customization library Upload section then click Choose File.\nAfter choose the customAlgo.so, simply click UPLOAD to upload it to the MCM-204 The upload process will finished at about 30 seconds, there will pop up a re login dialog until done. Simply click re-login link to re login to web portal. Step 4 Apply custom algorithm to device setting. Click Device Setting at MCM-204 web portal, then scroll down to Channel Config section At this tutorial we use the default setting of AI0, simply click ADD DATATYPE, then choose Data Type to Customization and input rms at Customization Parameter. Scroll down to the bottom of page then click Apply\n)\nStep 5 Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. You could enter to next tutorial to learn how to develope your own algorithm.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithmdev/",
	"title": "Custom Algorithm Development",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to develop your own algorithm In the previous tutorial, we can get the RMS or Mean value through the sample code CustomAlgorithm.zip. This tutorial will modify the sample code to get the RMS and Mean value at the same time.\nPrerequisites Operating System : Ubuntu 18.04 installed and up-to date Step 1: Change the output data from 1 to 2. Find the following code from customAlgo.c.\ndouble* CustomAlgo(uint16_t chIndex, void *rawData, DeviceInfo devInfo, char* customParams, uint32_t* outCount)\r{\ruint32_t* raw = (uint32_t*)rawData;\rdouble inputRange = devInfo.inputRange == B10 ? 10.0 : 1.25;\rdouble scalingFactor = inputRange/8388607.0*1000.0/devInfo.sensor.sensitivity;//for convert rawData to g\rconst uint32_t COUNT = 1;\rdouble* data = (double*)malloc(sizeof(double) * COUNT);\rdouble* gArray = (double*)malloc(sizeof(double) * devInfo.dataCount);\rraw = raw + chIndex*devInfo.dataCount;\r*outCount = COUNT;\rfor(int i = 0; i \u0026lt; devInfo.dataCount; i++)\rgArray[i] = scalingFactor*(((raw[i] \u0026amp; 0x00800000) == 0x00800000) ? (int32_t)(raw[i] | 0xFF000000) : (int32_t)raw[i]);\rif(strcmp(customParams, \u0026#34;rms\u0026#34;)==0)\rdata[0] = GetRms(gArray, devInfo.dataCount); else\rdata[0] = GetMean(gArray, devInfo.dataCount); free(gArray);\rreturn data;\r} Modify COUNT to 2 in line 34.\nconst uint32_t COUNT = 2; Step 2: Change the output data. Modify conditions, then save and deploy the code.\nif(strcmp(customParams, \u0026#34;rms\u0026#34;)==0)\r{\rdata[0] = GetRms(gArray, devInfo.dataCount);\r} else if(strcmp(customParams,\u0026#34;mean\u0026#34;)==0)\r{\rdata[0] = GetMean(gArray, devInfo.dataCount);\r}\relse if(strcmp(customParams,\u0026#34;both\u0026#34;)==0)\r{\rdata[0]=GetRms(gArray,devInfo.dataCount);\rdata[1]=GetMean(gArray,devInfo.dataCount);\r} Step 3: Apply custom algorithm to device setting. Please refer to the previous tutorial.\nStep 4: Modify Customization Parameter. Enter both in customization parameter. Step 5: Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. In this way you can get the RMS and the Mean value at the same time.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about MCM-204\nGeneral What is the default IP address? The default IP is the fixed link local IP address 169.254.1.1 How to connect by hostname? The connection via hostname relies on DNS service, please make sure the DNS service is functional. Programming What kinds of API supported? RESTFul API : You could get most everything via this. C/C++ API : Used for getting rawdata more effiency. "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page cover the process to flash a new operating system onto a NEON-2000-JNX and EOS-JNX device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a JNX device you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu If upgrading the Jetpack version make sure to also download the eMMC image. This is because the internal eMMC and microSD card must be flashed with same version of Jetpack. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nNeon-2000-JNX microSD image\nJetpack 5.1.2 v1.0.9 microSD image (with pylon5.2 + without EVA) MD5:81992a6ada7f01ee30206680bdc4e878 Jetpack 5.0.2 v1.0.8 microSD image (with pylon7.2.1 + EVA4.0.2) MD5:79d87d4dc0ef2440845dccb1d0a33d4b Jetpack 5.0.2 v1.0.7 microSD image (with pylon5.2) MD5:c60395f01cd76131c25f3bf96ed53543 Jetpack 4.6.1 v1.0.5 microSD image MD5:564951bdd6074db6e853037b1338e905 Jetpack 4.6.1 v1.0.6 microSD image (with EVA 3.8.3) MD5:8b4e3d4ce7bf7e69bc90b04e98e14851 Jetpack 4.5 v1.0.3 microSD image MD5:243b98f10a873f2830e4f635eab7c80d Jetpack 4.5 v1.0.4 microSD image (with EVA 3.5) MD5:43b5931b625e3b598423e0bb4a131d6e eMMC image - required if changing Jetpack version\nJetpack 5.1.2 emmc image MD5:dc01fad05c3ca7df2b5096bc27877b95 Jetpack 5.0.2 emmc image MD5:73758afcdf0119a98b27c9d1630add6a Jetpack 4.6.1 emmc image MD5:f08d89a8aa4927b7df7af449c2f8edd9 Jetpack 4.5 emmc image MD5:e603db76e8ab1bbe5596760d40adb90c Checksum Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz MD5 Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\nmicroSD of at least 32GB microSD card reader Make sure to use a high quality microSD card to prevent corruption\nUnzip the microSD zip file downloaded in Step 1 to get a .img file tar -zxvf NeonJNX_A3_JP512_microSD_v1.0.9_woEVA.img.tar.gz Clone image file to microSD card using one of the following methods Ubuntu Disk Manager Video of process to clone image to microSD card Steps: Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher Insert SD card into the NEON camera Step 3: Flash eMMC image The internal eMMC must be flashed if the Jetpack version on the microSD card has changed, so the Jetpack version on the eMMC and microSD card match.\nThis step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers There is also a video and PDF showing the process\nOn the Host PC unzip the file downloaded in Step 1\ntar -zxvf NeonJNX_A3_JP512_emmc_v1.0.9.tar.gz Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode. Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again Connect the microUSB cable to the NEON and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd NeonJNX_A3_JP512_emmc_v1.0.9 Flash the Neon-2000-JNX For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass Jetpack before 5.0.2\nsudo ./flash.sh For Jetpack 5.0.2 ~ 5.1.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 1 Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON Below is a video of the process to flash a NEON-2000-JNX Appendix: How to restore eMMC image method 1\nstep1: download environment on Linux x86 ubuntu PC https://sftp.adlinktech.com/image/Neon-JNX/deploy.neonjnx.tar.gz md5: d9e7e4ff4030f7d81f72c353aad6d272 step2: untar the file tar -zxvf deploy.neonjnx.tar.gz step3: execute command sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-neonjnx-emmc mmcblk0p1 Note: Booard name of different hardware version HW A3: jetson-neonjnx-a3-emmc HW A2: jetson-xavier-nx-neonjnx-emmc method 2\nBackup emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rcd mntTemp\rsudo tar -jcf ../customerEMMC.tbz2 *\rsync\rcd ../\rsudo umount mntTemp\rrmdir mntTemp Restore emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rsudo tar jxf customerEMMC.tbz2 -C mntTemp\rsync\rsudo umount mntTemp\rrmdir mntTemp "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-jnx/demo/",
	"title": "People count application With Deepstream SDK and Transfer Learning Toolkit",
	"tags": [],
	"description": "",
	"content": "People count application With Deepstream SDK and Transfer Learning Toolkit\nGit repo: https://github.com/AIoT-IST/deepstream-occupancy-analytics/tree/master\nDescription Prerequisites Getting Started Build Run Output References Description This is a sample application for counting people entering/leaving in a building using NVIDIA Deepstream SDK, Transfer Learning Toolkit (TLT) and pre-trained models. This application can be used to build real-time occupancy analytics application for smart buildings, hospitals, retail, etc. The application is based on deepstream-test5 sample application.\nIt can take streaming video or Neon camera as input, counts the number of people crossing a tripwire. In this application, you will learn:\nHow to use PeopleNet model from NGC How to use NvDsAnalytics plugin to draw line and count people crossing the line You can extend this application to change region of interest, use cloud-to-edge messaging to trigger record in the DeepStream application or build analytic dashboard or database to store the metadata.\nTo learn how to build this demo step-by-step, check out the on-demand webinar on Creating Intelligent places using DeepStream SDK.\nPrerequisites Neon-201A-JNX or Neon-202A-JNX with jetpack 5.1.2\nDownload PeopleNet model: [https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet/files]\nThis application is based on deepstream-test5 application. More about test5 application: [https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_ref_app_test5.html]\nGetting Started Preferably clone the repo in /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y\rsudo apt-get install libjson-glib-dev libgstrtspserver-1.0-dev -y\rsudo chmod 777 -R /opt/nvidia/deepstream/deepstream-6.3/\rgit clone https://github.com/AIoT-IST/deepstream-occupancy-analytics.git /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/ Download peoplnet model:\ncd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/config \u0026amp;\u0026amp; ./model.sh For Jetson use: bin/jetson/libnvds_msgconv.so Build and Configure Set CUDA_VER in the MakeFile as per platform.\nFor Jetson, CUDA_VER=11.4\ncd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics \u0026amp;\u0026amp; make Run cd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/\r./deepstream-test5-analytics -c config/dstest_occupancy_analytics.txt Modify the boarder cd /opt/nvidia/deepstream/deepstream-6.3/sources/apps/sample_apps/deepstream-occupancy-analytics/tool\rpython3 preview.py Output The output will look like this:\nWhere you can see the kafka messages for entry and exit count.\nReferences CREATE INTELLIGENT PLACES USING NVIDIA PRE-TRAINED VISION MODELS AND DEEPSTREAM SDK: [https://info.nvidia.com/iva-occupancy-webinar-reg-page.html?ondemandrgt=yes] Deepstream SDK: [https://developer.nvidia.com/deepstream-sdk] Deepstream Quick Start Guide: [https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html#page/DeepStream_Development_Guide/deepstream_quick_start.html#] Transfer Learning Toolkit: [https://developer.nvidia.com/transfer-learning-toolkit] forked from https://github.com/NVIDIA-AI-IOT/deepstream-occupancy-analytics "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-ono/",
	"title": "Neon-2000-ONO",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision Neon-2000-ONO Discover how to use the NEON-2000-ONO and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-ono/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "Step 1: Download image to your host pc with Ubuntu Neon-2000-ONO eMMC image - required if changing Jetpack version Jetpack 5.1.3 emmc image MD5:1863fa4d85d1624f9f2901886ee19d60 Checksum Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2 Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2 MD5 Step 2: Flash eMMC image This step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers There is also a video and PDF showing the process\nOn the Host PC unzip the file downloaded in Step 1\nsudo tar -I lbzip2 -xf mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0.tbz2 Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode. Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again Connect the microUSB cable to the NEON and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd mfi_jetson-orin-nano-neon2000-ono_4-35.4.1-rel.1v1.0 Flash the Neon-2000-ONO\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 15 --network usb0 Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\nBelow is a video of the process to flash a NEON "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-onx/",
	"title": "Neon-2000-ONX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision Neon-2000-ONX Discover how to use the NEON-2000-ONX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-2000-onx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "Step 1: Download image to your host pc with Ubuntu Neon-2000-ONX eMMC image - required if changing Jetpack version Jetpack 5.1.3 emmc image MD5:8af092371359db985ce30da934a9eed7 Checksum Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2 Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2 MD5 Step 2: Flash eMMC image This step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers There is also a video and PDF showing the process\nOn the Host PC unzip the file downloaded in Step 1\nsudo tar -I lbzip2 -xf mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0.tbz2 Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode. Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again Connect the microUSB cable to the NEON and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd mfi_jetson-orin-nano-neon2000-onx_8-35.4.1-rel.1v1.0 Flash the Neon-2000-ONO\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 15 --network usb0 Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON\nBelow is a video of the process to flash a NEON "
},
{
	"uri": "https://aiot-ist.github.io/dex/",
	"title": "DEX-100",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory DEX-100 Discover how to use the DEX series and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/",
	"title": "DEX-100 with KVM",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory DEX-100 with KVM package solution Tutor you on how to use KVM with DEX-100.\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/1_quick_start/",
	"title": "1 Quick start",
	"tags": [],
	"description": "",
	"content": "EVA SDK is edge computing visual analysis software supporting single source AI machine vision project deployment across different hardware platforms. EVA SDK IDE provides a visual AI machine vision deployment interface to quickly verify AI ​​Inferences. Watch the video to learn more about the intuitive GUI for fast AI inference validation. EVA IDE is built into ADLINK\u0026rsquo;s NEON series AI smart camera and EOS series AI vision system to help with AI machine vision project development.\nEVA IDE introduction Chinese EVA Hands-on webinal Chinese EVA manual Download link\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/2_showroom/",
	"title": "2 EVA Showroom",
	"tags": [],
	"description": "",
	"content": "Apply Now to experience the EVA Online Showroom Apply Link Having trouble choosing a topic for your factory AI application? EVA Online Showroom helps you with quick topic selection, online evaluation, and the ability to immediately launch AI vision applications.tely launch AI vision applications EVA Online Showroom is a utility that allows you to experience how EVA SDK simplifies AI Vision deployment at the edge, enabling you to deliver a successful proof of concept within two weeks. Through this digital Online Showroom, you will experience the performance and effects of AI Inference on different ADLINK AI machine vision hardware devices in a variety of selected application scenarios in order to help you choose the most appropriate AI application topics.\nEVA Online Showroom Experience steps: "
},
{
	"uri": "https://aiot-ist.github.io/eva/3_showcase/",
	"title": "3 EVA Cases",
	"tags": [],
	"description": "",
	"content": "Welcome to ADLINK EVA Showcase! In this git repository, you can download the source code of the plug-ins we designed for the showcases. Each showcase demonstrates the pipeline elements you will need. EVASDK can help you to quickly set up and build the applications for your specific implementation.\nRepo Link Showcases currently available: Case Description Categories Case 1 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area. Factories commonly have restricted areas to keep workers safe from moving automation or robots. In this showcase you will see how a corridor is designated as a restricted area to prevent people from entering it. If someone enters the restricted area, the region will turn red if the display is set to true, and an alert event will be recorded as metadata for a downstream element to activate. Geofencing Case 2 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area and checking if the person is wearing a clean room suit. For certain manufacturing processes, a clean room suit is required when entering an environmentally controlled space. In this showcase, geofencing is also used to detect when someone crosses the entrance. The downstream element for clean room suit detection will then be activated to detect if the person is wearing the appropriate clothes before entering the FAB. Geofencing/Wear Detection Case 3 In this showcase you will see how EVA is used to prepare the pipeline for detecting abnormal alignment of cookies on the production line. We can use Custom Vision to test the recognition effect of AI. In this demonstration, Custom Vision can be exported into a universal ONNX model, and the EVA can be used to quickly verify the video or the actual shooting results. Product Inspection Case 4 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the parts preparation and its order in the parts container. This monitoring is one of the assemble standard operation to prevent missing to assemble parts like screws in the next assemble steps. In additional to this purpose, the preparation time is also the critical fact for the analyst for formulating the optimal assembly standard. Collecting this information extremely help to reflect the true preparation time instead of coming from few simulating samples. Product Inspection/Preparation Case 5 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the product assembly based on the standard operation procedure. Each parts is required assembled orderly in time. Otherwise the whole procedure will exceed the standard assemble time designed. Each part will calculate its own consuming time and then summed for the analyst to dynamically adjust or designed the whole product assembly standard operation procedure. Ultimately, the convenient combination with showcase 4 for illustrating the two SOP algorithm plug and play in EVA. Product Inspection/Assembly "
},
{
	"uri": "https://aiot-ist.github.io/eva/4_template/",
	"title": "4 Template",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s common command line for each case.\nGet camera/video Source video\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! xvimagesink sync=true Basler gst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! videoconvert ! xvimagesink sync=false Appropho gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false Pre-processing / Post-process AI inferece Classification googlenet gst-launch-1.0 pylonsrc camera=0 fps=20 ! videoconvert ! adrt model=/home/adlink/Downloads/model/googlenet.engine batch=1 ! adtrans_classifier label=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/googlenet-v2_RT_labels.txt\u0026quot; ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true Object detection yolov3-tiny\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! adrt rgbconv=true model=yolov3-tiny-288-1012.engine device=0 scale=0.0039 mean=\u0026quot;0 0 0\u0026quot; batch=1 ! adtrans_yolotrt label-file=label.txt blob-size=\u0026quot;9,18\u0026quot; mask=\u0026quot;(3,4,5),(0,1,2)\u0026quot; anchor=\u0026quot;(10,14),(23,27),(37,58),(81,82),(135,169),(344,319)\u0026quot; input_width=288 class-num=80 input_height=288 threshold=0.4 ! admetadrawer ! videoconvert ! xvimagesink sync=true yolov3 gst-launch-1.0 pylonsrc camera=0 fps=6 ! videoconvert ! adrt model=yolov3-416_test.engine scale=0.004 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_yolo label=label.txt class-num=2 ! admetadrawer ! videoconvert ! xvimagesink sync=false ssd inception gst-launch-1.0 pylonsrc camera=0 fps=15 ! videoscale ! video/x-raw, width=800, height=600 ! videoconvert ! adrt model=/home/adlink/Downloads/model/ssdv2.engine batch=1 device=0 scale=0.0078 mean=\u0026quot;0 0 0\u0026quot; norm=false ! adtrans_ssd label=/home/adlink/Downloads/model/ssd_coco_labels.txt ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true Pose detection gst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! bayer2rgb ! videoconvert ! adrt model=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/pose-b1.engine\u0026quot; scale=0.0039 rgbconv=true ! adtrans_openpose_py ! admetadrawer ! videoconvert ! xvimagesink sync=false Segmentation fcn gst-launch-1.0 videotestsrc ! adrt model=road.engine scale=1.0 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_segment class-num=4 blob-height=512 blob-width=896 ! xvimagesink sync=false RTSP Launch RTSP server before every operation $ /opt/adlink/eva/bin/rtsp-simple-server\rVideo file streaming out gst-launch-1.0 videotestsrc ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test Appropho camera streaming out gst-launch-1.0 v4l2src io-mode=4 ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test gst-launch-1.0 v4l2src io-mode=0 ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h265enc bitrate=4000000 ! rtspclientsink location=rtsp://localhost:8554/test Receive RTSP streaming gst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test ! rtph265depay ! h264parse ! avdec_h264 ! xvimagesink gst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test user-id=admin user-pw=admin ! rtph264depay ! h264parse ! nvv4l2decoder ! nvvideoconvert ! \u0026quot;video/x-raw(memory:NVMM),format=RGBA\u0026quot; ! nvegltransform ! nveglglessink sync=false Save file Save as mkv file gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv Preview gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false Preview \u0026amp; save gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! videoconvert ! tee name=t ! queue ! 'video/x-raw, format=(string)UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv sync=false t. ! queue ! xvimagesink sync=false Inference and save video gst-launch-1.0 filesrc location=pallet.mp4 ! decodebin ! nvvidconv ! videoconvert ! adrt model=pallet_yolov4_416.engine scale=0.004 mean=\u0026quot;0 0 0\u0026quot; rgbconv=True ! adtrans_yolo class-num=1 anchor=anchor=\u0026quot;(12,16),(19,36),(40,28),(36,75),(76,55),(72,146),(142,110),(192,243),(459,401)\u0026quot; threshold=0.3 label=pallet_yolov4_v1.names use-sigmoid=True ! admetadrawer ! videoconvert ! omxh264enc bitrate=4000000 ! h264parse ! qtmux ! filesink location=filename.h264 "
},
{
	"uri": "https://aiot-ist.github.io/eva/5_qa/",
	"title": "5 Q&amp;A",
	"tags": [],
	"description": "",
	"content": " Installation path of EVA SDK /opt/adlink/eva/ How to get inference result from pipeline through plugin refer to /opt/adlink/eva/samples/plugin_sample.py through pipeline refer to /opt/adlink/eva/samples/pipeline_app.py How to clean gstreamer cache rm -rf /home/adlink/.cache/gstreamer-1.0 "
},
{
	"uri": "https://aiot-ist.github.io/eva/6_performance/",
	"title": "6 Measure performance",
	"tags": [],
	"description": "",
	"content": "Ways to calculate execute time of each element in EVA pipeline.\nUsing debug mode in EVA IDE and see the exec time of each element. Third party tool gst-shark\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/",
	"title": "EVA",
	"tags": [],
	"description": "",
	"content": "EVA (Edge Vision Analytics) EVA SDK is a unified edge vision analytics service-ready software platform that enables ADLINK AI hardware, making it easier for users to develop optimized edge AI vision applications by simplifying integration and focusing on essential functionality. Users can leverage readyto- use open-source plugins to facilitate each stage the AI vision project lifecycle, including image capture and processing, AI inference, postprocessing, and analytics. This \u0026ldquo;One API\u0026rdquo; framework allows users successfully to build a proof-of-concept in two weeks and speed up mass deployment time.\nIntuitive GUI for Fast and Easy AI Inference Pipeline Development Key functions Support hardwares AI Smart Camera\nNEON-2000-JNX AI Vision System\nEOS-i6000-P Series EOS-JNX-I / EOS-JNX-G Contact us IST_EVA_Support@adlinktech.com\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/",
	"title": "Neon-3000-RB5",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision Neon-3000-RB5 Discover how to use the Neon-3000-RB5 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": " Introduction User Interface UnBoxing Introduction ADLINK Launches Image Sensor Integrated NVIDIA Jetson Nano AI Camera Dev Kit for Easy, Rapid AI Vision Prototyping Summary: A vision development kit with the NVIDIA® Jetson Nano module and image sensor, industrial DI/O, issue-free integration with affordable price Get started quickly with add-on AI vision applications and sample code for rapid AI vision prototyping Easy leverage to vast NVIDIA, OpenCV and G-streamer resources through V4L2 video interface ADLINK exclusive EVA provides selected AI models, tools of training \u0026amp; labeling and SOP, easy to build a PoC without issues. High extensibility: seamlessly transfer the AI Camera Dev Kit to ADLINK industrial vision devices and systems ADLINK Technology Inc., a global leader in edge computing, launched its NVIDIA Jetson Nano based vision developer kit — AI Camera Dev Kit. ADLINK is applying its 25-year machine vision design experience in an effort to meet the needs of AI vision developers for rapid concept validation and viability testing and has launched a pocket-sized developer kit with image sensor, lens, vertical I/O, varied peripherals, and exclusive ADLINK edge vision analytics software – EVA. Accordingly, the AI Camera Dev Kit delivers low cost, simple integration, fast development, and easy access to open resources and high extensibility to accelerate AI vision prototyping while seamlessly enabling users to convert the kit to ADLINK edge AI vision devices.\nADLINK’s AI Camera Dev Kit includes an 8MP color MIPI camera module; NVIDIA® Jetson Nano™ SOC; LAN; industrial DI/O interface; USB Type-C hub for easy powering, display, keyboard/mouse connection and data transmission; SD memory card with Linux OS; NVIDIA JetPack SDK; OpenCV and ADLINK’s exclusive EVA software—everything you need to start AI Vision prototyping.\nTo help streamline software development, the kit includes two pre-installed ready-to-use AI vision applications, Pose Detection and Object Classification, with open-source training, a labeling tool and SOP that are easy to learn and modify. These two applications can be applied on the scenarios like the Robot Hazard Prevention and Fab Clear SOP or similar applications such as virtual fences or other SOP compliance. ADLINK EVA is our exclusive no-code/low-code GUI-based edge vision analytics software. The intuitive GUI pipeline development environment provides fast and flexible image capturing and AI inference to logical judgement configuration. Follow the EVA SOP, users can train their own AI model on the powerful computers and migrate it to the AI Camera Dev Kit. The ADLINK EVA Portal also provides tutorials, application templates and resources for effortless PoC development.\nToday’s market trend is to start project development from a developer kit that can quickly help developers test and demonstrate their concept. Targeted for the booming AI vision market, ADLINK provides an image sensor integrated AI camera developer kit based on our popular NEON series AI smart camera. The AI Camera Dev Kit eliminates tedious integration effort with its built-in no-code GUI software (EVA) and two AI applications with sample code. This kit enables users to start their AI vision PoC in just five minutes,” said Kevin Hsu, ADLINK´s senior product manager for edge vision products. “Based on the V4L2 interface, users can easily access all NVIDIA Tao and NGC catalog resources that they are familiar with. The AI camera dev kit leverage the experience and advantage of our most popular product NEON series AI Smart Camera with affordable price, is ideal for development of a wide range of edge vision and AI.\nThe ADLINK AI Camera Dev Kit can accelerate AI vision prototyping and seamlessly enable users to convert the kit to ADLINK edge AI vision devices. The AI Camera Dev Kit is available now. Follow the links for further information about ADLINK’s AI Camera Dev Kit and EVA software.\nUser Interface Front view Back view Purchase optional accessories for your custom use case: To extend monitor, keyboard/mouse and power supply capabilities To connect to digital I/O devices: How to wiring D-sub I/O Connector Unboxing Following the video to unbox your AI camera dev kit. Learn to impletement AI application: Geo-fence and SOP compliance. Note: Open English Caption if needed. Please refer to What is Adlink AI Suite? to know more info about the applications.\nOptional Accessories:\n① USB Type-C hub/adapter/30cm USB Type-C cable (92-99090-1010)\n② 1.8m USB Type-C cable with screw lock (30-01284-0030-A0)\n③ I/O cable with DB-15 connector (30-21621-0000-A0)\n④ 3m DB-15 to DB-37 I/O extension cable (30-01332-0010-A0)\n⑤ DIN-37D-01 IO extension board (91-14025-1020)\n⑥ AI camera kit tripod bracket (91-95340-000E)\n⑦ AI camera kit acrylic plate (91-95341-000E) This page cover the process to flash a new operating system onto a NEON-2000-JNX and EOS-JNX device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a JNX device you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu If upgrading the Jetpack version make sure to also download the eMMC image. This is because the internal eMMC and microSD card must be flashed with same version of Jetpack. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nNeon-2000-JNX microSD image\nJetpack 5.0.2 v1.0.8 microSD image (with pylon6.2.1 + EVA4.0.2) MD5:79d87d4dc0ef2440845dccb1d0a33d4b Jetpack 5.0.2 v1.0.7 microSD image (with pylon5.2) MD5:c60395f01cd76131c25f3bf96ed53543 Jetpack 4.6.1 v1.0.5 microSD image Jetpack 4.6.1 v1.0.6 microSD image (with EVA 3.8.3) MD5:8b4e3d4ce7bf7e69bc90b04e98e14851 Jetpack 4.5 v1.0.3 microSD image MD5:243b98f10a873f2830e4f635eab7c80d Jetpack 4.5 v1.0.4 microSD image (with EVA 3.5) MD5:43b5931b625e3b598423e0bb4a131d6e eMMC image - required if changing Jetpack version\nJetpack 5.0.2 emmc image MD5:73758afcdf0119a98b27c9d1630add6a Jetpack 4.6.1 emmc image MD5:1442d6597b6c93e8fff2543a808652ce Jetpack 4.5 emmc image MD5:e603db76e8ab1bbe5596760d40adb90c Checksum Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz MD5 Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\nmicroSD of at least 32GB microSD card reader Make sure to use a high quality microSD card to prevent corruption\nUnzip the microSD zip file downloaded in Step 1 to get a .img file tar -zxvf NeonJNX_A3_JP502_microSD_v1.0.7_woEVA.img.tar.gz Clone image file to microSD card using one of the following methods Ubuntu Disk Manager Video of process to clone image to microSD card Steps: Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher Insert SD card into the NEON camera Step 3: Flash eMMC image The internal eMMC must be flashed if the Jetpack version on the microSD card has changed, so the Jetpack version on the eMMC and microSD card match.\nThis step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers There is also a video and PDF showing the process\nOn the Host PC unzip the file downloaded in Step 1\ntar -zxvf NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode. Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again Connect the microUSB cable to the NEON and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd NeonJNX_A3_JP502_emmc_v1.0.7 Flash the Neon-2000-JNX For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass Jetpack before 5.0.2\nsudo ./flash.sh For Jetpack 5.0.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 1 Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON Below is a video of the process to flash a NEON-2000-JNX Appendix: How to restore eMMC image method 1\nstep1: download environment on Linux x86 ubuntu PC https://sftp.adlinktech.com/image/Neon-JNX/deploy.neonjnx.tar.gz md5: d9e7e4ff4030f7d81f72c353aad6d272 step2: untar the file tar -zxvf deploy.neonjnx.tar.gz step3: execute command sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-neonjnx-emmc mmcblk0p1 Note: Booard name of different hardware version HW A3: jetson-neonjnx-a3-emmc HW A2: jetson-xavier-nx-neonjnx-emmc method 2\nBackup emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rcd mntTemp\rsudo tar -jcf ../customerEMMC.tbz2 *\rsync\rcd ../\rsudo umount mntTemp\rrmdir mntTemp Restore emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rsudo tar jxf customerEMMC.tbz2 -C mntTemp\rsync\rsudo umount mntTemp\rrmdir mntTemp "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page provides detailed instructions on how to flash the image for Neon-RB5 using Windows. 1. Prerequisite 1.0 Download and Install Android Debug Bridge (ADB) and Fastboot Drivers 2. How to Flash Image 2.1 Updation of Board Support Package(BSP) Image 1 Prerequisite\nA Windows PC(Win10/Win11) is required to flash the image on the Neon-RB5 Module BSP image md5: e7b7d72383bf80fd2b1e768a29519d96 1.0 Download and install ADB and Fastboot drivers ​\tPlease download the ADB and Fastboot drivers and install them using the link provided below.\n​\t[Android USB Driver] : https://adb.clockworkmod.com\n​\t[ADB Fastboot Tool] : https://xiaomifirmware.com/download-link/?dlm-dp-dl=2696\n​\tNote: If you encounter “0 Files Copied Error”, download and install the “15_Second_ADB_Installer_v1.3.0” version as the latest version might not be compatible with the Windows version of your computer.\n2.1 Updation of BSP Image (Kernel and Filesystem) Update the kernel and filesystem and not the NON-HLOS firmware or bootloader, follow the steps below:\nImage needed qti-ubuntu-robotics-image-qrb5165-rb5-boot.img, qti-ubuntu-robotics-image-qrb5165-rb5-sysfs.ext4 and abl.elf\nImage can found on path :\n#windows_Drive: /LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19\nConnect the USB OTG between Target and Host PC\nPower on the Target board.\nOpen command prompt\n#windows_Drive: /LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19\nCheck Target board (Neon-RB5) got detected using below command. It will take minutes to show the device name after power up the target device.\n#cd LEC-RB5-NEON-8G-UBUNTU_20_04-HLOS_2v1.0.10_24_02_19 #adb devices -l Note: In case the \u0026ldquo;#adb devices” command shows no device , please check your Device Manager for any yellow bang in the Qualcomm USB driver as shown in the image below: The driver may have to be manually selected and updated as detailed in the screenshots below: The driver is already present in the system at this stage and should be available in the list as ‘Android ADB Interface’\nAfter Updating Driver, please follow below to enter into ADB mode on Host PC and check Neon-RB5 detected as fastboot device\nYour target device should have been listed in your command prompt terminal. If not, please check and update the driver in the device manager on your development host.\n# adb root # adb reboot bootloader The target device will be rebooted. It will take minutes to show the device name after power up the target device.\n#fastboot devices ​ In case of device not listed , please check device manager for yellow bang and update driver\nEnter into Fastboot mode\n#fastboot flash abl_a abl.elf #fastboot flash abl_b abl.elf #fastboot flash system qti-ubuntu-robotics-image-qrb5165-rb5-sysfs.ext4 #fastboot flash boot_a qti-ubuntu-robotics-image-qrb5165-rb5-boot.img #fastboot flash boot_b qti-ubuntu-robotics-image-qrb5165-rb5-boot.img #fastboot reboot The image has been successfully flashed on targeted device and it will be rebooted by itself. The device is now ready to be used.\nAttention: Don\u0026rsquo;t turn off (or) Reset the target device manually. It causes the image packages to break, resulting in the inability to boot. Please take a coffee, It will take around 30 minutes to install all the packages and desktop image. Once, everything is installed you could see the Ubuntu Login page in the monitor. Log in as a Ubuntu On Wayland. Username: adlink Password: adlink "
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/flashimg/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page cover the process to flash a new operating system onto a AI Camera Dev Kit device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a dev kit you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu AI Camera Dev Kit microSD image Jetpack 4.6.1 backup link MD5:C417A00B3BD500A617D201BC9B9D4421 Checksum Check the md5 checksum to make sure image file is correct Linux $ md5sum [file] $ md5sum AI_Vison_DEV_kit.zip Windows 10 $ certutil -hashfile [file] MD5 $ certutil -hashfile AI_Vison_DEV_kit.zip MD5 Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\nmicroSD of at least 32GB microSD card reader Make sure to use a high quality microSD card to prevent corruption\nUnzip the microSD zip file downloaded in Step 1 to get a .img file unzip AI_Vison_DEV_kit.zip Clone image file to microSD card using one of the following methods Ubuntu Disk Manager Video of process to clone image to microSD card Steps: Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher Insert SD card into the NEON camera "
},
{
	"uri": "https://aiot-ist.github.io/neon/neon-rb5/samples/",
	"title": "Sample program",
	"tags": [],
	"description": "",
	"content": "Sample program of Neon-RB5\nCapture and Inference Model converter Capture and Inference The example program uses the built-in camera of the Neon-RB5 for image capture and employs the YOLOv6 model for object detection. The SNPE version used is 1.68.0.3932.\nDownload link (md5: 253998b5f8b389914924e70b0545d705)\nInstallation guild Install compile environment sudo apt-get update\rsudo apt install --fix-broken sudo apt-get install cmake - #### Untar the file tar -zxvf Neon-RB5_sample.tar.gz -C /home/adlink/Desktop\rcd /home/adlink/Desktop/Neon-RB5_sample\rcp /home/adlink/Desktop/Neon-RB5_sample/yolov6n_base_quantized.dlc /home/adlink\rtar -zxvf snpe-1.68.0.3932.tar.gz -C /home/adlink\r- #### modidy .bashrc and add content below export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/adlink/snpe-1.68.0.3932/lib/aarch64-ubuntu-gcc7.5:/usr/local/lib/\rexport ADSP_LIBRARY_PATH=\u0026quot;/home/adlink/snpe-1.68.0.3932/lib/dsp;/usr/lib/rfsa/adsp/;/vendor/lib/rfsa/dsp/testsig;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp\u0026quot;\rexport PATH=$PATH:/home/adlink/snpe-1.68.0.3932/bin/aarch64-ubuntu-gcc7.5\r- #### Reload the .bashrc setting source ~/.bashrc\r- #### Compile the sample cd /home/adlink/Desktop/Neon-RB5_sample/samples\rmkdir build \u0026amp;\u0026amp; cd build\rcmake .. \u0026amp;\u0026amp; make\r- #### Execute the inference sample cd /home/adlink/Desktop/Neon-RB5_sample/samples/build\rcp ../../yolo.txt .\r./yolov6_snpe\r## Procedure of Model Conversion\r### Required hardware\r- x86 Host PC with [ubuntu18.04](https://releases.ubuntu.com/18.04)\r- Neon-RB5\rThe AI model used in the example is [Meituan YOLOv6-N](https://github.com/meituan/YOLOv6) Below, we will describe how to convert the downloaded [yolov6n.pt](https://github.com/meituan/YOLOv6/releases/download/0.4.0/yolov6n.pt) file into a dlc file for use with Neon-RB5.\rThe model conversion process primarily consists of four steps and requires a host PC with Ubuntu 18.04. Steps 1~5 are executed on the host.\r#### Step 1: Environment Setup sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade sudo apt install python3-pip -y pip3 install \u0026ndash;upgrade pip sudo update-alternatives \u0026ndash;install /usr/bin/python python /usr/bin/python3.6 1 update-alternatives \u0026ndash;list python\n#### Step 2: Download conversion tools wget https://sftp.adlinktech.com/image/Neon-RB5/snpe-1.68.0.zip -O ~/snpe-1.68.0.zip wget https://sftp.adlinktech.com/image/Neon-RB5/YOLOv6.tar.gz ~/YOLOv6.tar.gz\n#### Step 3: Setup conversion environment cd ~ unzip -X snpe-1.68.0.zip -d ~ export SNPE_ROOT=/home/adlink/snpe-1.68.0.3932 export PYTHONPATH=$PYTHONPATH:$SNPE_ROOT/lib/python source $SNPE_ROOT/bin/dependencies.sh source $SNPE_ROOT/bin/check_python_depends.sh pip3 install numpy==1.16.5 sphinx==2.2.1 scipy==1.3.1 matplotlib==3.0.3 scikit-image==0.15.0 protobuf==3.6.0 pyyaml==5.1 source snpe-1.68.0.3932/bin/check_python_depends.sh pip3 install onnx pip3 install torch pip3 install onnxsim\n#### Step 4: Convert *.pt to ONNX cd ~ tar -zxvf YOLOv6.tar.gzcd ~/YOLOv6/ python3 ./deploy/ONNX/export_onnx.py \u0026ndash;weights ./deploy/ONNX/yolov6n.pt \u0026ndash;img 288 \u0026ndash;batch 1\n#### Step 5: Convert *.ONNX to *.dlc cd ~/snpe-1.68.0.3932/bin/x86_64-linux-clang ./snpe-onnx-to-dlc \u0026ndash;input_network ~/YOLOv6/deploy/ONNX/yolov6n.onnx \u0026ndash;output_path yolov6n_base_quantized.dlc\n#### Step 6: Replace model in Neon-RB5\rReplace *.dlc in /home/adlink/Desktop/Neon-RB5_sample/samples/build cd /home/adlink/Desktop/Neon-RB5_sample/samples/build\r./yolov6_snpe\r"
},
{
	"uri": "https://aiot-ist.github.io/neon/devkit/",
	"title": "AI Camera Dev Kit",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Edge Vision AI Camera Dev Kit Discover how to use the AI Camera Dev Kit and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/",
	"title": "EOS-JNX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory EOS-JNX Discover how to use the EOS-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "EOS-JNX emmc image and microSD image are one-by-one mapping. That is, you have to flash emmc and microSD image with same version. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nDownload eMMC image [jetpack5.1.3 md5: 1203689de27fde2bc7151f27ff5b9e6e]((https://sftp.adlinktech.com/image/EOS-JNX/EOS-JNX_JP513_emmc.tar.gz) jetpack5.0.2 md5: 588e91411bb61f5b95b190bebb8373de jetpack4.6.1 md5: e9978a1f7d981b11fd9f323a6ca73aa7 jetpack4.6 jetpack4.5 How to reflash eMMC image To perform this step the following equipment is required:\nA bare metal machine running Ubuntu. Must not be a virtual machine. microUSB cable For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass On the Host PC unzip the file downloaded.\ntar -zxvf EOS-JNX_JP502_emmc.tar.gz Set EOS-JNX as recovery mode\nBoot EOS-JNX Hold RECOVERY button on front panel Push RESET button on front panel Release RECOVERY button Connect the microUSB cable to the EOS-JNX and the Host PC\nOpen a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode. Go to folder unzipped in step 1\ncd EOS-JNX_JP502_emmc Flash the EOS-JNX Jetpack before 5.0.2 sudo ./nvmflash.sh For Jetpack 5.0.2 sudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 20 Once the flash script is complete and shows Flash complete (SUCCESS) reboot the EOS-JNX How to restore eMMC image method 1\nstep1: download environment on Linux x86 ubuntu PC https://sftp.adlinktech.com/image/EOS-JNX/deploy.eosjnx.tar.gz md5: 0893c0969c1000ab98b1dc60e645d1cc step2: untar the file tar -zxvf deploy.eosjnx.tar.gz step3: execute command sudo ./flash.sh -r -k APP -G backup.img jetson-xavier-nx-eosjnx-emmc mmcblk0p1 method 2\nBackup emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rcd mntTemp\rsudo tar -jcf ../customerEMMC.tbz2 *\rsync\rcd ../\rsudo umount mntTemp\rrmdir mntTemp Restore emmc\ncd ~/\rmkdir mntTemp\rsudo mount /dev/mmcblk0p1 mntTemp\rsudo tar jxf customerEMMC.tbz2 -C mntTemp\rsync\rsudo umount mntTemp\rrmdir mntTemp Download microSD image jetpack5.1.3 md5: 8869748a7f6736861d963afdfcf3b414 jetpack5.0.2 md5: dda62d5d21f84d25368b5080eaed80e0 jetpack4.6.1 md5: d8006c99a4c3fc4ee7d5ba1639e8f57e jetpack4.6.1 with EVA md5: b58ea12bc4c8f5a4a5ffe173bc056f5c jetpack4.6 jetpack4.5 SOP Windows video tool Linux video Check sum $ md5sum EOS-JNX_JP502_emmc.tar.gz\r588e91411bb61f5b95b190bebb8373de EOS-JNX_JP502_emmc.tar.gz\r$ md5sum EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz\rdda62d5d21f84d25368b5080eaed80e0 EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz check md5 check sum to make sure image file is correct Linux $ md5sum [file]\n$ md5sum files.tar.gz\nWindows 10 certutil -hashfile [file] MD5\ncertutil -hashfile files.tar.gz MD5\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtobootfromssd/",
	"title": "How to boot from m.2 SSD?",
	"tags": [],
	"description": "",
	"content": " Internal Storage Installation Clone sd card data to m2 SSD Internal Storage Installation Refer to manual chapter 2.3 internal storage installation of EOS-JNX.\nClone sd card data to m2 SSD Refer to SOP video. Take 256Gb m.2 SSD for example.\nformat your storage as ext4\ncopy the files\nsudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd Modify /boot/extlinux/extlinux.conf in emmc, emmc, emmc !!!\nAPPEND ${cbootargs} quiet root=/dev/nvme0n1p1 rw rootwait rootfstype=ext4 Reboot\n"
},
{
	"uri": "https://aiot-ist.github.io/",
	"title": "ADLINK-IST Connected Factories",
	"tags": [],
	"description": "",
	"content": "ADLINK-IST Edge Vision Discover the sharing of technical documents and the common questions. You could navigate from the menu or simply type the keyword to search!\n"
},
{
	"uri": "https://aiot-ist.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/neon/",
	"title": "NEONs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]